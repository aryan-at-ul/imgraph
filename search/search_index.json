{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"imgraph: Graph Neural Networks for Image Processing","text":"<p><code>imgraph</code> is a Python library for converting images to graph representations and applying Graph Neural Networks (GNNs) to image analysis tasks. Built on top of PyTorch and PyTorch Geometric, it provides an easy-to-use interface for a variety of image-to-graph conversion methods and GNN architectures. The library supports multiple methods for node creation, feature extraction, and edge construction to enable the most effective graph representation for your specific computer vision task.</p>"},{"location":"#key-features","title":"\ud83d\udd11 Key Features","text":"<ul> <li>Diverse Graph Representations: Convert images to graphs using multiple methods:</li> <li>SLIC superpixels</li> <li>Felzenszwalb superpixels</li> <li>Regular grid patches</li> <li>Pixel-level nodes</li> <li>Comprehensive Feature Extraction: Extract rich node and edge features:</li> <li>Color features (mean, std, histograms)</li> <li>Texture features (LBP, GLCM, Gabor)</li> <li>Position features</li> <li>CNN-based features</li> <li>Boundary and geometric features</li> <li>Flexible Edge Construction: Connect nodes using various strategies:</li> <li>Region adjacency</li> <li>Distance-based connections</li> <li>Grid-based connections (4-connected, 8-connected)</li> <li>Feature similarity</li> <li>Pre-built GNN Models: Includes implementations of GCN, GAT, GIN, and GraphSAGE</li> <li>Easy Visualization: Visualize graph representations with intuitive plotting functions</li> <li>Ready-to-Use Presets: Common graph creation configurations available as presets</li> <li>Training Pipeline: Complete training and evaluation pipeline for graph-based image classification</li> </ul>"},{"location":"#installation","title":"\ud83d\udd27 Installation","text":"<pre><code># Basic installation\npip install imgraph\n\n# Full installation with all dependencies\npip install imgraph[full]  # testing and under dev\n\n# Developer installation\npip install imgraph[dev]  \n</code></pre>"},{"location":"#quick-start","title":"\ud83d\udcda Quick Start","text":""},{"location":"#basic-graph-creation","title":"Basic Graph Creation","text":"<pre><code>import cv2\nimport matplotlib.pyplot as plt\nfrom imgraph import GraphPresets\n\n# Load an image\nimage = cv2.imread('sample_image.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Create a graph using a preset\ngraph_builder = GraphPresets.slic_mean_color()\ngraph = graph_builder(image)\n\n# Visualize the graph\nfig = graph_builder.visualize_graph(image, graph)\nplt.show()\n\n# Access graph properties\nprint(f\"Number of nodes: {graph.num_nodes}\")\nprint(f\"Number of edges: {graph.edge_index.shape[1]}\")\nprint(f\"Node feature dimensions: {graph.x.shape[1]}\")\n</code></pre>"},{"location":"#creating-custom-graphs","title":"Creating Custom Graphs","text":"<pre><code>from imgraph import GraphBuilder\nfrom imgraph.data.node_creation import slic_superpixel_nodes\nfrom imgraph.data.node_features import mean_std_color_features\nfrom imgraph.data.edge_creation import region_adjacency_edges\nfrom imgraph.data.edge_features import color_difference\n\n# Create a custom graph builder\ngraph_builder = GraphBuilder(\n    node_creation_method=lambda img: slic_superpixel_nodes(img, n_segments=100, compactness=10),\n    node_feature_method=lambda img, nodes: mean_std_color_features(img, nodes, color_space='hsv'),\n    edge_creation_method=lambda nodes, img: region_adjacency_edges(nodes, connectivity=2),\n    edge_feature_method=lambda img, nodes, edges: color_difference(img, nodes, edges)\n)\n\n# Process an image\ngraph = graph_builder(image)\n</code></pre>"},{"location":"#using-the-pipeline-api","title":"Using the Pipeline API","text":"<pre><code>from imgraph import GraphPipeline\n\n# Define a custom configuration\nconfig = {\n    'node_creation': {\n        'method': 'slic_superpixel_nodes',\n        'params': {\n            'n_segments': 100,\n            'compactness': 10,\n            'sigma': 1.0\n        }\n    },\n    'node_features': {\n        'method': 'mean_std_color_features',\n        'params': {\n            'color_space': 'hsv',\n            'normalize': True\n        }\n    },\n    'edge_creation': {\n        'method': 'region_adjacency_edges',\n        'params': {\n            'connectivity': 2\n        }\n    },\n    'edge_features': {\n        'method': 'color_difference',\n        'params': {\n            'color_space': 'hsv',\n            'normalize': True\n        }\n    }\n}\n\n# Create the custom graph\npipeline = GraphPipeline(config)\ngraph = pipeline.process(image)\n</code></pre>"},{"location":"#training-a-gnn-model","title":"Training a GNN Model","text":"<pre><code>import torch\nfrom torch_geometric.loader import DataLoader\nfrom imgraph.datasets import ImageFolderGraphDataset\nfrom imgraph.models import GCN\nfrom imgraph.training import Trainer, EarlyStopping\n\n# Create dataset from a folder of images\ndataset = ImageFolderGraphDataset(\n    root='dataset_directory',\n    preset='slic_mean_color',\n    force_reload=False\n)\n\n# Split dataset\ntrain_dataset, test_dataset = dataset.get_train_test_split(train_ratio=0.8)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4)\n\n# Create model\nnum_features = train_dataset[0].x.shape[1]\nnum_classes = len(dataset.classes)\nmodel = GCN(num_features, 64, num_classes, num_layers=3)\n\n# Setup training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\nearly_stopping = EarlyStopping(patience=10)\n\n# Train the model\ntrainer = Trainer(model, optimizer, criterion, device, early_stopping)\nhistory = trainer.fit(train_loader, test_loader, epochs=50)\n\n# Evaluate\naccuracy = trainer.evaluate(test_loader)\nprint(f\"Test accuracy: {accuracy:.4f}\")\n</code></pre>"},{"location":"#available-methods","title":"\ud83e\udde9 Available Methods","text":""},{"location":"#node-creation-methods","title":"Node Creation Methods","text":"<pre><code>from imgraph.data.node_creation import (\n    slic_superpixel_nodes,           # SLIC superpixel segmentation\n    felzenszwalb_superpixel_nodes,   # Felzenszwalb's algorithm\n    regular_patch_nodes,             # Regular grid patches\n    pixel_nodes                      # Individual pixels as nodes\n)\n</code></pre>"},{"location":"#node-feature-extraction-methods","title":"Node Feature Extraction Methods","text":"<pre><code>from imgraph.data.node_features import (\n    # Color features\n    mean_color_features,             # Mean color in various color spaces\n    mean_std_color_features,         # Mean and standard deviation of colors\n    histogram_color_features,        # Color histograms\n\n    # Texture features\n    lbp_features,                    # Local Binary Patterns\n    glcm_features,                   # Gray-Level Co-occurrence Matrix\n    gabor_features,                  # Gabor filter responses\n\n    # Position features\n    position_features,               # Raw position coordinates\n    normalized_position_features,    # Normalized position coordinates\n\n    # Deep features\n    pretrained_cnn_features          # Features from pre-trained CNNs\n)\n</code></pre>"},{"location":"#edge-creation-methods","title":"Edge Creation Methods","text":"<pre><code>from imgraph.data.edge_creation import (\n    # Grid-based connections\n    grid_4_edges,                    # 4-connected grid\n    grid_8_edges,                    # 8-connected grid\n\n    # Distance-based connections\n    distance_threshold_edges,        # Edges based on distance threshold\n    k_nearest_edges,                 # K-nearest neighbor connections\n\n    # Region-based connections\n    region_adjacency_edges,          # Connect adjacent regions\n\n    # Feature-based connections\n    feature_similarity_edges         # Connect nodes with similar features\n)\n</code></pre>"},{"location":"#edge-feature-extraction-methods","title":"Edge Feature Extraction Methods","text":"<pre><code>from imgraph.data.edge_features import (\n    # Feature differences\n    feature_difference,              # Difference between node features\n    color_difference,                # Color differences between nodes\n\n    # Geometric features\n    distance_features,               # Distance between nodes\n    angle_features,                  # Angle between nodes\n\n    # Boundary features\n    boundary_strength,               # Strength of boundaries between regions\n    boundary_orientation             # Orientation of boundaries\n)\n</code></pre>"},{"location":"#available-presets","title":"\ud83e\udde9 Available Presets","text":"<p>The library includes several presets for common graph creation configurations:</p> <ul> <li><code>slic_mean_color()</code>: SLIC superpixels with mean color features</li> <li><code>slic_color_position()</code>: SLIC superpixels with color and position features</li> <li><code>patches_color()</code>: Regular grid patches with color features</li> <li><code>tiny_graph()</code>: Small-scale graph with minimal nodes</li> <li><code>superpixel_comprehensive()</code>: Detailed superpixel representation with multiple feature types</li> </ul>"},{"location":"#architecture","title":"\ud83d\udcd0 Architecture","text":"<p>The package is organized into several modules:</p> <ul> <li><code>data</code>: Graph creation and feature extraction</li> <li><code>node_creation</code>: Methods for creating nodes from images</li> <li><code>node_features</code>: Methods for extracting node features</li> <li><code>edge_creation</code>: Methods for creating edges between nodes</li> <li><code>edge_features</code>: Methods for extracting edge features</li> <li><code>models</code>: GNN model implementations (GCN, GAT, GIN, GraphSAGE)</li> <li><code>datasets</code>: Dataset classes for common benchmarks and custom data</li> <li><code>training</code>: Training utilities for model training and evaluation</li> <li><code>visualization</code>: Tools for visualizing graphs and features</li> <li><code>pipeline</code>: End-to-end processing pipelines</li> </ul> <p>The core functionality is built around the <code>GraphBuilder</code> class, which provides a modular way to construct graphs from images by combining node creation, feature extraction, and edge construction methods.</p>"},{"location":"#advanced-usage","title":"\ud83d\ude80 Advanced Usage","text":""},{"location":"#combining-multiple-feature-types","title":"Combining Multiple Feature Types","text":"<pre><code>from imgraph import GraphBuilder\nfrom imgraph.data.node_creation import slic_superpixel_nodes\nfrom imgraph.data.node_features import mean_color_features, normalized_position_features\nfrom imgraph.data.edge_creation import region_adjacency_edges\nfrom imgraph.data.make_graph import combine_features\n\n# Combine multiple feature extraction methods\ndef combined_node_features(image, node_info):\n    return combine_features(\n        [\n            lambda img, nodes: mean_color_features(img, nodes, color_space='rgb'),\n            lambda img, nodes: normalized_position_features(img, nodes)\n        ],\n        image, \n        node_info\n    )\n\n# Create a graph builder with combined features\ngraph_builder = GraphBuilder(\n    node_creation_method=slic_superpixel_nodes,\n    node_feature_method=combined_node_features,\n    edge_creation_method=region_adjacency_edges\n)\n\n# Process an image\ngraph = graph_builder(image)\n</code></pre>"},{"location":"#creating-multiple-graph-representations","title":"Creating Multiple Graph Representations","text":"<pre><code>from imgraph import MultiGraphBuilder, GraphPresets\n\n# Create multiple graph builders with different configurations\nmulti_builder = MultiGraphBuilder([\n    GraphPresets.slic_mean_color(),\n    GraphPresets.patches_color(),\n    GraphPresets.superpixel_comprehensive()\n])\n\n# Process an image to get multiple graph representations\ngraphs = multi_builder(image)\n\n# Access individual graphs\nslic_graph = graphs[0]\npatches_graph = graphs[1]\ncomprehensive_graph = graphs[2]\n</code></pre>"},{"location":"#customgnn-building-flexible-graph-neural-networks","title":"CustomGNN: Building Flexible Graph Neural Networks","text":"<p>ImGraph provides the <code>CustomGNN</code> model, which enables you to create advanced graph neural network architectures with complete flexibility over layers, pooling methods, and feature handling.</p>"},{"location":"#creating-custom-architectures","title":"Creating Custom Architectures","text":"<pre><code>from torch_geometric.nn import GCNConv, GATConv, SAGEConv\nfrom imgraph.models import CustomGNN\n\n# Basic GNN with standard layers\nmodel = CustomGNN(\n    num_features=node_feature_dim,\n    hidden_dim=64,\n    num_classes=num_classes,\n    gnn_layer_cls=GCNConv\n)\n\n# Advanced: Mix different layer types in a single model\nmodel = CustomGNN(\n    num_features=node_feature_dim,\n    hidden_dim=64,\n    num_classes=num_classes,\n    num_layers=3,\n    gnn_layer_cls=[\n        GCNConv,                                                # Layer 1: GCN\n        lambda in_c, out_c: GATConv(in_c, out_c, heads=4, concat=False),  # Layer 2: GAT\n        SAGEConv                                               # Layer 3: SAGE\n    ]\n)\n</code></pre>"},{"location":"#working-with-edge-features","title":"Working with Edge Features","text":"<p>For tasks that benefit from edge information (boundary detection, spatial relationships):</p> <pre><code>from imgraph.models import CustomGNNWithEdgeFeatures\n\nmodel = CustomGNNWithEdgeFeatures(\n    num_features=node_feature_dim,\n    hidden_dim=64,\n    num_classes=num_classes,\n    edge_dim=edge_feature_dim,\n    gnn_layer_cls=GCNConv\n)\n</code></pre>"},{"location":"#custom-pooling-and-hierarchical-architectures","title":"Custom Pooling and Hierarchical Architectures","text":"<p>You can specify different pooling strategies or create hierarchical architectures:</p> <pre><code>from torch_geometric.nn import TopKPooling\nfrom imgraph.models import CustomGNN\n\n# Using different pooling methods\nmodel = CustomGNN(\n    num_features=node_feature_dim,\n    hidden_dim=64,\n    num_classes=num_classes,\n    pooling_method='max',  # Options: 'mean', 'max', 'sum'\n    gnn_layer_cls=GCNConv\n)\n\n# Custom pooling layer\npooling_layer = TopKPooling(64, ratio=0.5)\nmodel = CustomGNN(\n    num_features=node_feature_dim,\n    hidden_dim=64,\n    num_classes=num_classes,\n    pooling_method=pooling_layer,\n    gnn_layer_cls=GCNConv\n)\n</code></pre>"},{"location":"#implementing-domain-specific-layers","title":"Implementing Domain-Specific Layers","text":"<p>You can create specialized GNN layers for image analysis tasks:</p> <pre><code>import torch.nn as nn\nfrom torch_geometric.nn import MessagePassing\nfrom imgraph.models import CustomGNN\n\n# Custom layer for texture-aware graph processing\nclass TextureAwareConv(MessagePassing):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super().__init__(aggr='max')\n        self.texture_lin = nn.Linear(in_channels // 2, out_channels // 2)\n        self.other_lin = nn.Linear(in_channels // 2, out_channels // 2)\n        self.combine_lin = nn.Linear(out_channels, out_channels)\n        self.out_channels = out_channels  # Required for CustomGNN\n\n    def forward(self, x, edge_index):\n        # Split features (assuming first half are texture features)\n        x_texture = x[:, :x.size(1) // 2]\n        x_other = x[:, x.size(1) // 2:]\n\n        # Process separately\n        x_texture = self.texture_lin(x_texture)\n        x_other = self.other_lin(x_other)\n\n        # Combine and apply message passing\n        x = torch.cat([x_texture, x_other], dim=1)\n        x = self.combine_lin(x)\n        return self.propagate(edge_index, x=x)\n\n# Use custom layer with CustomGNN\nmodel = CustomGNN(\n    num_features=node_feature_dim,\n    hidden_dim=64,\n    num_classes=num_classes,\n    gnn_layer_cls=TextureAwareConv\n)\n</code></pre>"},{"location":"#multi-scale-feature-extraction","title":"Multi-Scale Feature Extraction","text":"<p>For complex visual tasks, you can build a model that captures features at multiple scales:</p> <pre><code>from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\nimport torch.nn.functional as F\n\nclass MultiScaleGNN(nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        # Feature extraction at different scales\n        self.conv1 = GCNConv(num_features, 32)\n        self.conv2 = GCNConv(32, 64)\n        self.conv3 = GCNConv(64, 128)\n\n        # Final classifier\n        self.classifier = nn.Linear(32 + 64 + 128, num_classes)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        # Multi-scale feature extraction\n        x1 = F.relu(self.conv1(x, edge_index))\n        x2 = F.relu(self.conv2(x1, edge_index))\n        x3 = F.relu(self.conv3(x2, edge_index))\n\n        # Multi-scale pooling\n        x1_pool = global_mean_pool(x1, batch)\n        x2_pool = global_mean_pool(x2, batch)\n        x3_pool = global_max_pool(x3, batch)\n\n        # Combine features from different scales\n        x_combined = torch.cat([x1_pool, x2_pool, x3_pool], dim=1)\n\n        # Classification\n        out = self.classifier(x_combined)\n        return out\n\n# Wrap in CustomGNN for compatibility with ImGraph pipeline\n# (See the documentation for details on wrapping custom architectures)\n</code></pre>"},{"location":"#best-practices-for-advanced-usage","title":"Best Practices for Advanced Usage","text":"<ol> <li>Layer Selection: Match the GNN layer type to your specific task:</li> <li>GCNConv: Good baseline, captures local structure</li> <li>GATConv: Better for nodes with varying importance</li> <li>SAGEConv: Better for large, heterogeneous graphs</li> <li> <p>Custom layers: For domain-specific feature extraction</p> </li> <li> <p>Edge Features: Always use edge features when working with region adjacency graphs or when boundary information is important</p> </li> <li> <p>Pooling Strategy: Choose based on your task:</p> </li> <li>'mean': Balanced representation of all nodes</li> <li>'max': Captures the most important features</li> <li> <p>'sum': Preserves information about graph size</p> </li> <li> <p>Custom Layers: Create specialized layers when working with multiple feature types (texture, color, position) to process them differently</p> </li> </ol> <p>See the example scripts (<code>training_custom_gnn_example.py</code>) for more detailed implementations and use cases.</p>"},{"location":"#citation","title":"\ud83d\udcdd Citation","text":"<p>If you use <code>imgraph</code> in your research, please cite:</p> <pre><code>@software{imgraph2023,\n  author = {Singh, Aryan},\n  title = {imgraph: Graph Neural Networks for Image Processing},\n  url = {https://github.com/aryan-at-ul/imgraph},\n  version = {0.0.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"imgraph/data/feature_extractor/","title":"Feature Extractor","text":""},{"location":"imgraph/data/feature_extractor/#imgraphdatafeature_extractor","title":"<code>imgraph.data.feature_extractor</code>","text":""},{"location":"imgraph/data/legacy/","title":"Legacy","text":""},{"location":"imgraph/data/legacy/#imgraphdatalegacy","title":"<code>imgraph.data.legacy</code>","text":"<p>Backward compatibility module for imgraph.</p> <p>This module provides the old function names that were used in previous versions.</p>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.graph_generator","title":"<code>graph_generator(image, n_segments=100, compactness=10)</code>","text":"<p>Legacy function for generating a graph from an image.</p>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.graph_generator--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image n_segments : int, optional     Number of segments, by default 100 compactness : int, optional     Compactness parameter, by default 10</p>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.graph_generator--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph data object</p> Source code in <code>imgraph/data/legacy.py</code> <pre><code>def graph_generator(image, n_segments=100, compactness=10):\n    \"\"\"\n    Legacy function for generating a graph from an image.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    n_segments : int, optional\n        Number of segments, by default 100\n    compactness : int, optional\n        Compactness parameter, by default 10\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph data object\n    \"\"\"\n    warnings.warn(\n        \"graph_generator is deprecated. Use GraphBuilder or GraphPresets instead.\",\n        DeprecationWarning, stacklevel=2\n    )\n\n    # Create nodes\n    node_info = slic_superpixel_nodes(image, n_segments=n_segments, compactness=compactness)\n\n    # Extract node features\n    node_features = mean_color_features(image, node_info)\n\n    # Create edges\n    edge_index = region_adjacency_edges(node_info)\n\n    # Create graph data object\n    graph_data = Data(\n        x=node_features,\n        edge_index=edge_index,\n        num_nodes=len(node_features),\n        image_size=torch.tensor(image.shape[:2])\n    )\n\n    # Store node info for visualization\n    graph_data.node_info = node_info\n\n    return graph_data\n</code></pre>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.image_transform_slic","title":"<code>image_transform_slic(image, n_segments=100, compactness=10)</code>","text":"<p>Legacy function for SLIC superpixel segmentation.</p>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.image_transform_slic--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image n_segments : int, optional     Number of segments, by default 100 compactness : int, optional     Compactness parameter, by default 10</p>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.image_transform_slic--returns","title":"Returns","text":"<p>dict     Node information dictionary</p> Source code in <code>imgraph/data/legacy.py</code> <pre><code>def image_transform_slic(image, n_segments=100, compactness=10):\n    \"\"\"\n    Legacy function for SLIC superpixel segmentation.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    n_segments : int, optional\n        Number of segments, by default 100\n    compactness : int, optional\n        Compactness parameter, by default 10\n\n    Returns\n    -------\n    dict\n        Node information dictionary\n    \"\"\"\n    warnings.warn(\n        \"image_transform_slic is deprecated. Use slic_superpixel_nodes instead.\",\n        DeprecationWarning, stacklevel=2\n    )\n    return slic_superpixel_nodes(image, n_segments=n_segments, compactness=compactness)\n</code></pre>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.make_edges","title":"<code>make_edges(node_info, image=None)</code>","text":"<p>Legacy function for creating edges between nodes.</p>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.make_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/legacy/#imgraph.data.legacy.make_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor</p> Source code in <code>imgraph/data/legacy.py</code> <pre><code>def make_edges(node_info, image=None):\n    \"\"\"\n    Legacy function for creating edges between nodes.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor\n    \"\"\"\n    warnings.warn(\n        \"make_edges is deprecated. Use region_adjacency_edges instead.\",\n        DeprecationWarning, stacklevel=2\n    )\n    return region_adjacency_edges(node_info, image)\n</code></pre>"},{"location":"imgraph/data/make_graph/","title":"Make Graph","text":""},{"location":"imgraph/data/make_graph/#imgraphdatamake_graph","title":"<code>imgraph.data.make_graph</code>","text":"<p>Main module for constructing a graph from an image.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder","title":"<code>GraphBuilder</code>","text":"<p>A class for constructing a graph from an image.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder--parameters","title":"Parameters","text":"<p>node_creation_method : callable     Method for creating nodes from an image node_feature_method : callable     Method for extracting node features edge_creation_method : callable     Method for creating edges between nodes edge_feature_method : callable, optional     Method for extracting edge features, by default None</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>class GraphBuilder:\n    \"\"\"\n    A class for constructing a graph from an image.\n\n    Parameters\n    ----------\n    node_creation_method : callable\n        Method for creating nodes from an image\n    node_feature_method : callable\n        Method for extracting node features\n    edge_creation_method : callable\n        Method for creating edges between nodes\n    edge_feature_method : callable, optional\n        Method for extracting edge features, by default None\n    \"\"\"\n\n    def __init__(self, node_creation_method, node_feature_method, edge_creation_method, edge_feature_method=None):\n        \"\"\"Initialize the GraphBuilder.\"\"\"\n        self.node_creation_method = node_creation_method\n        self.node_feature_method = node_feature_method\n        self.edge_creation_method = edge_creation_method\n        self.edge_feature_method = edge_feature_method\n\n    def build_graph(self, image):\n        \"\"\"\n        Builds a graph from an image.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            Input image with shape (H, W, C)\n\n        Returns\n        -------\n        torch_geometric.data.Data\n            Graph representation of the image\n        \"\"\"\n        # Create nodes\n        node_info = self.node_creation_method(image)\n\n        # Extract node features\n        node_features = self.node_feature_method(image, node_info)\n\n        # Create edges\n        edge_index = self.edge_creation_method(node_info, image)\n\n        # Extract edge features (if method provided)\n        if self.edge_feature_method is not None:\n            edge_attr = self.edge_feature_method(image, node_info, edge_index)\n        else:\n            edge_attr = None\n\n        # Create graph data object\n        graph_data = Data(\n            x=node_features,\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            # Add additional metadata\n            num_nodes=len(node_features),\n            image_size=torch.tensor(image.shape[:2])\n        )\n\n        # Store node info for visualization or further processing\n        graph_data.node_info = node_info\n\n        return graph_data\n\n    def __call__(self, image):\n        \"\"\"\n        Alias for build_graph method.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            Input image with shape (H, W, C)\n\n        Returns\n        -------\n        torch_geometric.data.Data\n            Graph representation of the image\n        \"\"\"\n        return self.build_graph(image)\n\n    def visualize_graph(self, image, graph=None):\n        \"\"\"\n        Visualizes the graph on top of the image.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            Input image with shape (H, W, C)\n        graph : torch_geometric.data.Data, optional\n            Graph to visualize, by default None (build a new graph)\n\n        Returns\n        -------\n        matplotlib.figure.Figure\n            Figure with the visualization\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        # Build graph if not provided\n        if graph is None:\n            graph = self.build_graph(image)\n\n        # Get node positions\n        if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n            node_positions = graph.node_info['centroids']\n        else:\n            raise ValueError(\"Graph does not contain node positions\")\n\n        # Get edge indices\n        edge_index = graph.edge_index.cpu().numpy()\n\n        # Create figure\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        # Display image\n        ax.imshow(image)\n\n        # Plot nodes\n        ax.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n        # Plot edges\n        for i in range(edge_index.shape[1]):\n            src_idx = edge_index[0, i]\n            dst_idx = edge_index[1, i]\n\n            src_pos = node_positions[src_idx]\n            dst_pos = node_positions[dst_idx]\n\n            ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n        # Set title\n        ax.set_title(f\"Graph Visualization: {len(node_positions)} nodes, {edge_index.shape[1]} edges\")\n\n        # Turn off axis\n        ax.axis('off')\n\n        return fig\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.__call__","title":"<code>__call__(image)</code>","text":"<p>Alias for build_graph method.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.__call__--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C)</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.__call__--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph representation of the image</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def __call__(self, image):\n    \"\"\"\n    Alias for build_graph method.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph representation of the image\n    \"\"\"\n    return self.build_graph(image)\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.__init__","title":"<code>__init__(node_creation_method, node_feature_method, edge_creation_method, edge_feature_method=None)</code>","text":"<p>Initialize the GraphBuilder.</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def __init__(self, node_creation_method, node_feature_method, edge_creation_method, edge_feature_method=None):\n    \"\"\"Initialize the GraphBuilder.\"\"\"\n    self.node_creation_method = node_creation_method\n    self.node_feature_method = node_feature_method\n    self.edge_creation_method = edge_creation_method\n    self.edge_feature_method = edge_feature_method\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.build_graph","title":"<code>build_graph(image)</code>","text":"<p>Builds a graph from an image.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.build_graph--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C)</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.build_graph--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph representation of the image</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def build_graph(self, image):\n    \"\"\"\n    Builds a graph from an image.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph representation of the image\n    \"\"\"\n    # Create nodes\n    node_info = self.node_creation_method(image)\n\n    # Extract node features\n    node_features = self.node_feature_method(image, node_info)\n\n    # Create edges\n    edge_index = self.edge_creation_method(node_info, image)\n\n    # Extract edge features (if method provided)\n    if self.edge_feature_method is not None:\n        edge_attr = self.edge_feature_method(image, node_info, edge_index)\n    else:\n        edge_attr = None\n\n    # Create graph data object\n    graph_data = Data(\n        x=node_features,\n        edge_index=edge_index,\n        edge_attr=edge_attr,\n        # Add additional metadata\n        num_nodes=len(node_features),\n        image_size=torch.tensor(image.shape[:2])\n    )\n\n    # Store node info for visualization or further processing\n    graph_data.node_info = node_info\n\n    return graph_data\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.visualize_graph","title":"<code>visualize_graph(image, graph=None)</code>","text":"<p>Visualizes the graph on top of the image.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.visualize_graph--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) graph : torch_geometric.data.Data, optional     Graph to visualize, by default None (build a new graph)</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.GraphBuilder.visualize_graph--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def visualize_graph(self, image, graph=None):\n    \"\"\"\n    Visualizes the graph on top of the image.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    graph : torch_geometric.data.Data, optional\n        Graph to visualize, by default None (build a new graph)\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    # Build graph if not provided\n    if graph is None:\n        graph = self.build_graph(image)\n\n    # Get node positions\n    if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n        node_positions = graph.node_info['centroids']\n    else:\n        raise ValueError(\"Graph does not contain node positions\")\n\n    # Get edge indices\n    edge_index = graph.edge_index.cpu().numpy()\n\n    # Create figure\n    fig, ax = plt.subplots(figsize=(12, 8))\n\n    # Display image\n    ax.imshow(image)\n\n    # Plot nodes\n    ax.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n    # Plot edges\n    for i in range(edge_index.shape[1]):\n        src_idx = edge_index[0, i]\n        dst_idx = edge_index[1, i]\n\n        src_pos = node_positions[src_idx]\n        dst_pos = node_positions[dst_idx]\n\n        ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n    # Set title\n    ax.set_title(f\"Graph Visualization: {len(node_positions)} nodes, {edge_index.shape[1]} edges\")\n\n    # Turn off axis\n    ax.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder","title":"<code>MultiGraphBuilder</code>","text":"<p>A class for constructing multiple graphs from an image using different methods.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder--parameters","title":"Parameters","text":"<p>builders : list of GraphBuilder     List of graph builders to use</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>class MultiGraphBuilder:\n    \"\"\"\n    A class for constructing multiple graphs from an image using different methods.\n\n    Parameters\n    ----------\n    builders : list of GraphBuilder\n        List of graph builders to use\n    \"\"\"\n\n    def __init__(self, builders):\n        \"\"\"Initialize the MultiGraphBuilder.\"\"\"\n        self.builders = builders\n\n    def build_graphs(self, image):\n        \"\"\"\n        Builds multiple graphs from an image.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            Input image with shape (H, W, C)\n\n        Returns\n        -------\n        list of torch_geometric.data.Data\n            List of graph representations of the image\n        \"\"\"\n        return [builder.build_graph(image) for builder in self.builders]\n\n    def __call__(self, image):\n        \"\"\"\n        Alias for build_graphs method.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            Input image with shape (H, W, C)\n\n        Returns\n        -------\n        list of torch_geometric.data.Data\n            List of graph representations of the image\n        \"\"\"\n        return self.build_graphs(image)\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder.__call__","title":"<code>__call__(image)</code>","text":"<p>Alias for build_graphs method.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder.__call__--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C)</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder.__call__--returns","title":"Returns","text":"<p>list of torch_geometric.data.Data     List of graph representations of the image</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def __call__(self, image):\n    \"\"\"\n    Alias for build_graphs method.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n\n    Returns\n    -------\n    list of torch_geometric.data.Data\n        List of graph representations of the image\n    \"\"\"\n    return self.build_graphs(image)\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder.__init__","title":"<code>__init__(builders)</code>","text":"<p>Initialize the MultiGraphBuilder.</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def __init__(self, builders):\n    \"\"\"Initialize the MultiGraphBuilder.\"\"\"\n    self.builders = builders\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder.build_graphs","title":"<code>build_graphs(image)</code>","text":"<p>Builds multiple graphs from an image.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder.build_graphs--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C)</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.MultiGraphBuilder.build_graphs--returns","title":"Returns","text":"<p>list of torch_geometric.data.Data     List of graph representations of the image</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def build_graphs(self, image):\n    \"\"\"\n    Builds multiple graphs from an image.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n\n    Returns\n    -------\n    list of torch_geometric.data.Data\n        List of graph representations of the image\n    \"\"\"\n    return [builder.build_graph(image) for builder in self.builders]\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.combine_edge_features","title":"<code>combine_edge_features(feature_methods, image, node_info, edge_index)</code>","text":"<p>Combines multiple edge feature extraction methods.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.combine_edge_features--parameters","title":"Parameters","text":"<p>feature_methods : list of callable     List of feature extraction methods image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E)</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.combine_edge_features--returns","title":"Returns","text":"<p>torch.Tensor     Combined edge features</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def combine_edge_features(feature_methods, image, node_info, edge_index):\n    \"\"\"\n    Combines multiple edge feature extraction methods.\n\n    Parameters\n    ----------\n    feature_methods : list of callable\n        List of feature extraction methods\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n\n    Returns\n    -------\n    torch.Tensor\n        Combined edge features\n    \"\"\"\n    features = [method(image, node_info, edge_index) for method in feature_methods]\n    return torch.cat(features, dim=1)\n</code></pre>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.combine_features","title":"<code>combine_features(feature_methods, image, node_info)</code>","text":"<p>Combines multiple node feature extraction methods.</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.combine_features--parameters","title":"Parameters","text":"<p>feature_methods : list of callable     List of feature extraction methods image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation</p>"},{"location":"imgraph/data/make_graph/#imgraph.data.make_graph.combine_features--returns","title":"Returns","text":"<p>torch.Tensor     Combined node features</p> Source code in <code>imgraph/data/make_graph.py</code> <pre><code>def combine_features(feature_methods, image, node_info):\n    \"\"\"\n    Combines multiple node feature extraction methods.\n\n    Parameters\n    ----------\n    feature_methods : list of callable\n        List of feature extraction methods\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n\n    Returns\n    -------\n    torch.Tensor\n        Combined node features\n    \"\"\"\n    features = [method(image, node_info) for method in feature_methods]\n    return torch.cat(features, dim=1)\n</code></pre>"},{"location":"imgraph/data/presets/","title":"Presets","text":""},{"location":"imgraph/data/presets/#imgraphdatapresets","title":"<code>imgraph.data.presets</code>","text":"<p>Presets for common graph configurations.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets","title":"<code>GraphPresets</code>","text":"<p>A class providing preset configurations for graph construction.</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>class GraphPresets:\n    \"\"\"\n    A class providing preset configurations for graph construction.\n    \"\"\"\n\n    @staticmethod\n    def slic_mean_color():\n        \"\"\"\n        Create a graph with SLIC superpixel nodes and mean color features.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        node_creator = lambda image: slic_superpixel_nodes(image, n_segments=100, compactness=10)\n        node_featurizer = lambda image, node_info: mean_color_features(image, node_info)\n        edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator)\n\n    @staticmethod\n    def slic_color_position():\n        \"\"\"\n        Create a graph with SLIC superpixel nodes, mean-std color and position features.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        node_creator = lambda image: slic_superpixel_nodes(image, n_segments=100, compactness=10)\n\n        def node_featurizer(image, node_info):\n            color_feat = mean_std_color_features(image, node_info)\n            pos_feat = normalized_position_features(image, node_info)\n            return torch.cat([color_feat, pos_feat], dim=1)\n\n        edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n        edge_featurizer = lambda image, node_info, edge_index: color_difference(image, node_info, edge_index)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n\n    @staticmethod\n    def slic_texture():\n        \"\"\"\n        Create a graph with SLIC superpixel nodes and texture features.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        node_creator = lambda image: slic_superpixel_nodes(image, n_segments=100, compactness=10)\n        node_featurizer = lambda image, node_info: lbp_features(image, node_info)\n        edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator)\n\n    @staticmethod\n    def patches_color():\n        \"\"\"\n        Create a graph with regular patch nodes and color features.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        node_creator = lambda image: regular_patch_nodes(image, patch_size=16)\n        node_featurizer = lambda image, node_info: mean_std_color_features(image, node_info)\n        edge_creator = lambda node_info, image: grid_4_edges(node_info)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator)\n\n    @staticmethod\n    def patches_cnn():\n        \"\"\"\n        Create a graph with regular patch nodes and CNN features.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        node_creator = lambda image: regular_patch_nodes(image, patch_size=32)\n        node_featurizer = lambda image, node_info: pretrained_cnn_features(image, node_info, model_name='resnet18')\n        edge_creator = lambda node_info, image: grid_4_edges(node_info)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator)\n\n    @staticmethod\n    def superpixel_comprehensive():\n        \"\"\"\n        Create a comprehensive graph with superpixel nodes and multiple features.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        node_creator = lambda image: slic_superpixel_nodes(image, n_segments=150, compactness=15)\n\n        def node_featurizer(image, node_info):\n            feature_methods = [\n                lambda img, info: mean_std_color_features(img, info),\n                lambda img, info: lbp_features(img, info),\n                lambda img, info: normalized_position_features(img, info)\n            ]\n            return combine_features(feature_methods, image, node_info)\n\n        edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n        def edge_featurizer(image, node_info, edge_index):\n            feature_methods = [\n                lambda img, info, edges: color_difference(img, info, edges),\n                lambda img, info, edges: distance_features(info, edges),\n                lambda img, info, edges: boundary_strength(img, info, edges)\n            ]\n            return combine_edge_features(feature_methods, image, node_info, edge_index)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n\n    @staticmethod\n    def tiny_graph():\n        \"\"\"\n        Create a small graph with few superpixels for quick testing.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        node_creator = lambda image: slic_superpixel_nodes(image, n_segments=20, compactness=10)\n        node_featurizer = lambda image, node_info: mean_std_color_features(image, node_info)\n        edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator)\n\n    @staticmethod\n    def medical_preset():\n        \"\"\"\n        Create a graph optimized for medical image analysis.\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        # More superpixels and lower compactness for medical images\n        node_creator = lambda image: slic_superpixel_nodes(image, n_segments=200, compactness=5)\n\n        def node_featurizer(image, node_info):\n            feature_methods = [\n                lambda img, info: mean_std_color_features(img, info),\n                lambda img, info: histogram_color_features(img, info, bins=12),\n                lambda img, info: lbp_features(img, info),\n                lambda img, info: normalized_position_features(img, info)\n            ]\n            return combine_features(feature_methods, image, node_info)\n\n        # Use both adjacency and distance for more connections\n        def edge_creator(node_info, image):\n            adj_edges = region_adjacency_edges(node_info)\n            dist_edges = k_nearest_edges(node_info, k=3)\n\n            # Combine edges\n            combined_edges = torch.cat([adj_edges, dist_edges], dim=1)\n\n            # Remove duplicates\n            combined_edges = torch.unique(combined_edges, dim=1)\n\n            return combined_edges\n\n        edge_featurizer = lambda image, node_info, edge_index: distance_features(node_info, edge_index)\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n\n    @staticmethod\n    def custom_preset(config):\n        \"\"\"\n        Create a graph with custom configuration.\n\n        Parameters\n        ----------\n        config : dict\n            Dictionary with configuration parameters\n\n        Returns\n        -------\n        GraphBuilder\n            Graph builder with the specified configuration\n        \"\"\"\n        # Node creation\n        node_creation_config = config.get('node_creation', {})\n        node_method = node_creation_config.get('method', 'slic_superpixel_nodes')\n        node_params = node_creation_config.get('params', {})\n\n        if node_method == 'slic_superpixel_nodes':\n            node_creator = lambda image: slic_superpixel_nodes(image, **node_params)\n        elif node_method == 'felzenszwalb_superpixel_nodes':\n            node_creator = lambda image: felzenszwalb_superpixel_nodes(image, **node_params)\n        elif node_method == 'regular_patch_nodes':\n            node_creator = lambda image: regular_patch_nodes(image, **node_params)\n        elif node_method == 'pixel_nodes':\n            node_creator = lambda image: pixel_nodes(image, **node_params)\n        else:\n            raise ValueError(f\"Unknown node creation method: {node_method}\")\n\n        # Node features\n        node_features_config = config.get('node_features', {})\n        node_feat_method = node_features_config.get('method', 'mean_std_color_features')\n        node_feat_params = node_features_config.get('params', {})\n\n        if node_feat_method == 'mean_color_features':\n            node_featurizer = lambda image, node_info: mean_color_features(image, node_info, **node_feat_params)\n        elif node_feat_method == 'mean_std_color_features':\n            node_featurizer = lambda image, node_info: mean_std_color_features(image, node_info, **node_feat_params)\n        elif node_feat_method == 'histogram_color_features':\n            node_featurizer = lambda image, node_info: histogram_color_features(image, node_info, **node_feat_params)\n        elif node_feat_method == 'lbp_features':\n            node_featurizer = lambda image, node_info: lbp_features(image, node_info, **node_feat_params)\n        elif node_feat_method == 'glcm_features':\n            node_featurizer = lambda image, node_info: glcm_features(image, node_info, **node_feat_params)\n        elif node_feat_method == 'position_features':\n            node_featurizer = lambda image, node_info: position_features(image, node_info, **node_feat_params)\n        elif node_feat_method == 'normalized_position_features':\n            node_featurizer = lambda image, node_info: normalized_position_features(image, node_info, **node_feat_params)\n        elif node_feat_method == 'pretrained_cnn_features':\n            node_featurizer = lambda image, node_info: pretrained_cnn_features(image, node_info, **node_feat_params)\n        else:\n            raise ValueError(f\"Unknown node feature method: {node_feat_method}\")\n\n        # Edge creation\n        edge_creation_config = config.get('edge_creation', {})\n        edge_method = edge_creation_config.get('method', 'region_adjacency_edges')\n        edge_params = edge_creation_config.get('params', {})\n\n        if edge_method == 'region_adjacency_edges':\n            edge_creator = lambda node_info, image: region_adjacency_edges(node_info, **edge_params)\n        elif edge_method == 'grid_4_edges':\n            edge_creator = lambda node_info, image: grid_4_edges(node_info, **edge_params)\n        elif edge_method == 'grid_8_edges':\n            edge_creator = lambda node_info, image: grid_8_edges(node_info, **edge_params)\n        elif edge_method == 'distance_threshold_edges':\n            edge_creator = lambda node_info, image: distance_threshold_edges(node_info, **edge_params)\n        elif edge_method == 'k_nearest_edges':\n            edge_creator = lambda node_info, image: k_nearest_edges(node_info, **edge_params)\n        elif edge_method == 'feature_similarity_edges':\n            # Feature similarity needs node features, so we have to adapt\n            edge_creator = lambda node_info, image: feature_similarity_edges(\n                node_featurizer(image, node_info), **edge_params\n            )\n        else:\n            raise ValueError(f\"Unknown edge creation method: {edge_method}\")\n\n        # Edge features\n        edge_features_config = config.get('edge_features', {})\n        edge_feat_method = edge_features_config.get('method', None)\n        edge_feat_params = edge_features_config.get('params', {})\n\n        if edge_feat_method is None:\n            edge_featurizer = None\n        elif edge_feat_method == 'color_difference':\n            edge_featurizer = lambda image, node_info, edge_index: color_difference(image, node_info, edge_index, **edge_feat_params)\n        elif edge_feat_method == 'distance_features':\n            edge_featurizer = lambda image, node_info, edge_index: distance_features(node_info, edge_index, **edge_feat_params)\n        elif edge_feat_method == 'angle_features':\n            edge_featurizer = lambda image, node_info, edge_index: angle_features(node_info, edge_index, **edge_feat_params)\n        elif edge_feat_method == 'boundary_strength':\n            edge_featurizer = lambda image, node_info, edge_index: boundary_strength(image, node_info, edge_index, **edge_feat_params)\n        else:\n            raise ValueError(f\"Unknown edge feature method: {edge_feat_method}\")\n\n        return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.custom_preset","title":"<code>custom_preset(config)</code>  <code>staticmethod</code>","text":"<p>Create a graph with custom configuration.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.custom_preset--parameters","title":"Parameters","text":"<p>config : dict     Dictionary with configuration parameters</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.custom_preset--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef custom_preset(config):\n    \"\"\"\n    Create a graph with custom configuration.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary with configuration parameters\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    # Node creation\n    node_creation_config = config.get('node_creation', {})\n    node_method = node_creation_config.get('method', 'slic_superpixel_nodes')\n    node_params = node_creation_config.get('params', {})\n\n    if node_method == 'slic_superpixel_nodes':\n        node_creator = lambda image: slic_superpixel_nodes(image, **node_params)\n    elif node_method == 'felzenszwalb_superpixel_nodes':\n        node_creator = lambda image: felzenszwalb_superpixel_nodes(image, **node_params)\n    elif node_method == 'regular_patch_nodes':\n        node_creator = lambda image: regular_patch_nodes(image, **node_params)\n    elif node_method == 'pixel_nodes':\n        node_creator = lambda image: pixel_nodes(image, **node_params)\n    else:\n        raise ValueError(f\"Unknown node creation method: {node_method}\")\n\n    # Node features\n    node_features_config = config.get('node_features', {})\n    node_feat_method = node_features_config.get('method', 'mean_std_color_features')\n    node_feat_params = node_features_config.get('params', {})\n\n    if node_feat_method == 'mean_color_features':\n        node_featurizer = lambda image, node_info: mean_color_features(image, node_info, **node_feat_params)\n    elif node_feat_method == 'mean_std_color_features':\n        node_featurizer = lambda image, node_info: mean_std_color_features(image, node_info, **node_feat_params)\n    elif node_feat_method == 'histogram_color_features':\n        node_featurizer = lambda image, node_info: histogram_color_features(image, node_info, **node_feat_params)\n    elif node_feat_method == 'lbp_features':\n        node_featurizer = lambda image, node_info: lbp_features(image, node_info, **node_feat_params)\n    elif node_feat_method == 'glcm_features':\n        node_featurizer = lambda image, node_info: glcm_features(image, node_info, **node_feat_params)\n    elif node_feat_method == 'position_features':\n        node_featurizer = lambda image, node_info: position_features(image, node_info, **node_feat_params)\n    elif node_feat_method == 'normalized_position_features':\n        node_featurizer = lambda image, node_info: normalized_position_features(image, node_info, **node_feat_params)\n    elif node_feat_method == 'pretrained_cnn_features':\n        node_featurizer = lambda image, node_info: pretrained_cnn_features(image, node_info, **node_feat_params)\n    else:\n        raise ValueError(f\"Unknown node feature method: {node_feat_method}\")\n\n    # Edge creation\n    edge_creation_config = config.get('edge_creation', {})\n    edge_method = edge_creation_config.get('method', 'region_adjacency_edges')\n    edge_params = edge_creation_config.get('params', {})\n\n    if edge_method == 'region_adjacency_edges':\n        edge_creator = lambda node_info, image: region_adjacency_edges(node_info, **edge_params)\n    elif edge_method == 'grid_4_edges':\n        edge_creator = lambda node_info, image: grid_4_edges(node_info, **edge_params)\n    elif edge_method == 'grid_8_edges':\n        edge_creator = lambda node_info, image: grid_8_edges(node_info, **edge_params)\n    elif edge_method == 'distance_threshold_edges':\n        edge_creator = lambda node_info, image: distance_threshold_edges(node_info, **edge_params)\n    elif edge_method == 'k_nearest_edges':\n        edge_creator = lambda node_info, image: k_nearest_edges(node_info, **edge_params)\n    elif edge_method == 'feature_similarity_edges':\n        # Feature similarity needs node features, so we have to adapt\n        edge_creator = lambda node_info, image: feature_similarity_edges(\n            node_featurizer(image, node_info), **edge_params\n        )\n    else:\n        raise ValueError(f\"Unknown edge creation method: {edge_method}\")\n\n    # Edge features\n    edge_features_config = config.get('edge_features', {})\n    edge_feat_method = edge_features_config.get('method', None)\n    edge_feat_params = edge_features_config.get('params', {})\n\n    if edge_feat_method is None:\n        edge_featurizer = None\n    elif edge_feat_method == 'color_difference':\n        edge_featurizer = lambda image, node_info, edge_index: color_difference(image, node_info, edge_index, **edge_feat_params)\n    elif edge_feat_method == 'distance_features':\n        edge_featurizer = lambda image, node_info, edge_index: distance_features(node_info, edge_index, **edge_feat_params)\n    elif edge_feat_method == 'angle_features':\n        edge_featurizer = lambda image, node_info, edge_index: angle_features(node_info, edge_index, **edge_feat_params)\n    elif edge_feat_method == 'boundary_strength':\n        edge_featurizer = lambda image, node_info, edge_index: boundary_strength(image, node_info, edge_index, **edge_feat_params)\n    else:\n        raise ValueError(f\"Unknown edge feature method: {edge_feat_method}\")\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.medical_preset","title":"<code>medical_preset()</code>  <code>staticmethod</code>","text":"<p>Create a graph optimized for medical image analysis.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.medical_preset--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef medical_preset():\n    \"\"\"\n    Create a graph optimized for medical image analysis.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    # More superpixels and lower compactness for medical images\n    node_creator = lambda image: slic_superpixel_nodes(image, n_segments=200, compactness=5)\n\n    def node_featurizer(image, node_info):\n        feature_methods = [\n            lambda img, info: mean_std_color_features(img, info),\n            lambda img, info: histogram_color_features(img, info, bins=12),\n            lambda img, info: lbp_features(img, info),\n            lambda img, info: normalized_position_features(img, info)\n        ]\n        return combine_features(feature_methods, image, node_info)\n\n    # Use both adjacency and distance for more connections\n    def edge_creator(node_info, image):\n        adj_edges = region_adjacency_edges(node_info)\n        dist_edges = k_nearest_edges(node_info, k=3)\n\n        # Combine edges\n        combined_edges = torch.cat([adj_edges, dist_edges], dim=1)\n\n        # Remove duplicates\n        combined_edges = torch.unique(combined_edges, dim=1)\n\n        return combined_edges\n\n    edge_featurizer = lambda image, node_info, edge_index: distance_features(node_info, edge_index)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.patches_cnn","title":"<code>patches_cnn()</code>  <code>staticmethod</code>","text":"<p>Create a graph with regular patch nodes and CNN features.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.patches_cnn--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef patches_cnn():\n    \"\"\"\n    Create a graph with regular patch nodes and CNN features.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    node_creator = lambda image: regular_patch_nodes(image, patch_size=32)\n    node_featurizer = lambda image, node_info: pretrained_cnn_features(image, node_info, model_name='resnet18')\n    edge_creator = lambda node_info, image: grid_4_edges(node_info)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.patches_color","title":"<code>patches_color()</code>  <code>staticmethod</code>","text":"<p>Create a graph with regular patch nodes and color features.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.patches_color--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef patches_color():\n    \"\"\"\n    Create a graph with regular patch nodes and color features.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    node_creator = lambda image: regular_patch_nodes(image, patch_size=16)\n    node_featurizer = lambda image, node_info: mean_std_color_features(image, node_info)\n    edge_creator = lambda node_info, image: grid_4_edges(node_info)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.slic_color_position","title":"<code>slic_color_position()</code>  <code>staticmethod</code>","text":"<p>Create a graph with SLIC superpixel nodes, mean-std color and position features.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.slic_color_position--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef slic_color_position():\n    \"\"\"\n    Create a graph with SLIC superpixel nodes, mean-std color and position features.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    node_creator = lambda image: slic_superpixel_nodes(image, n_segments=100, compactness=10)\n\n    def node_featurizer(image, node_info):\n        color_feat = mean_std_color_features(image, node_info)\n        pos_feat = normalized_position_features(image, node_info)\n        return torch.cat([color_feat, pos_feat], dim=1)\n\n    edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n    edge_featurizer = lambda image, node_info, edge_index: color_difference(image, node_info, edge_index)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.slic_mean_color","title":"<code>slic_mean_color()</code>  <code>staticmethod</code>","text":"<p>Create a graph with SLIC superpixel nodes and mean color features.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.slic_mean_color--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef slic_mean_color():\n    \"\"\"\n    Create a graph with SLIC superpixel nodes and mean color features.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    node_creator = lambda image: slic_superpixel_nodes(image, n_segments=100, compactness=10)\n    node_featurizer = lambda image, node_info: mean_color_features(image, node_info)\n    edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.slic_texture","title":"<code>slic_texture()</code>  <code>staticmethod</code>","text":"<p>Create a graph with SLIC superpixel nodes and texture features.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.slic_texture--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef slic_texture():\n    \"\"\"\n    Create a graph with SLIC superpixel nodes and texture features.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    node_creator = lambda image: slic_superpixel_nodes(image, n_segments=100, compactness=10)\n    node_featurizer = lambda image, node_info: lbp_features(image, node_info)\n    edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.superpixel_comprehensive","title":"<code>superpixel_comprehensive()</code>  <code>staticmethod</code>","text":"<p>Create a comprehensive graph with superpixel nodes and multiple features.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.superpixel_comprehensive--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef superpixel_comprehensive():\n    \"\"\"\n    Create a comprehensive graph with superpixel nodes and multiple features.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    node_creator = lambda image: slic_superpixel_nodes(image, n_segments=150, compactness=15)\n\n    def node_featurizer(image, node_info):\n        feature_methods = [\n            lambda img, info: mean_std_color_features(img, info),\n            lambda img, info: lbp_features(img, info),\n            lambda img, info: normalized_position_features(img, info)\n        ]\n        return combine_features(feature_methods, image, node_info)\n\n    edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n    def edge_featurizer(image, node_info, edge_index):\n        feature_methods = [\n            lambda img, info, edges: color_difference(img, info, edges),\n            lambda img, info, edges: distance_features(info, edges),\n            lambda img, info, edges: boundary_strength(img, info, edges)\n        ]\n        return combine_edge_features(feature_methods, image, node_info, edge_index)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator, edge_featurizer)\n</code></pre>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.tiny_graph","title":"<code>tiny_graph()</code>  <code>staticmethod</code>","text":"<p>Create a small graph with few superpixels for quick testing.</p>"},{"location":"imgraph/data/presets/#imgraph.data.presets.GraphPresets.tiny_graph--returns","title":"Returns","text":"<p>GraphBuilder     Graph builder with the specified configuration</p> Source code in <code>imgraph/data/presets.py</code> <pre><code>@staticmethod\ndef tiny_graph():\n    \"\"\"\n    Create a small graph with few superpixels for quick testing.\n\n    Returns\n    -------\n    GraphBuilder\n        Graph builder with the specified configuration\n    \"\"\"\n    node_creator = lambda image: slic_superpixel_nodes(image, n_segments=20, compactness=10)\n    node_featurizer = lambda image, node_info: mean_std_color_features(image, node_info)\n    edge_creator = lambda node_info, image: region_adjacency_edges(node_info)\n\n    return GraphBuilder(node_creator, node_featurizer, edge_creator)\n</code></pre>"},{"location":"imgraph/data/transform_graph/","title":"Transform Graph","text":""},{"location":"imgraph/data/transform_graph/#imgraphdatatransform_graph","title":"<code>imgraph.data.transform_graph</code>","text":""},{"location":"imgraph/data/transform_graph/#imgraph.data.transform_graph.load_and_transform","title":"<code>load_and_transform(gobject, name, type, task='classification')</code>","text":"<p>gobject: networkx graph object</p> Name Type Description Default <code>name</code> <p>name of the graph</p> required <code>task</code> <p>classification or regression</p> <code>'classification'</code> <code>type</code> <p>type of the graph train/test</p> required <p>Returns: torch_geometric.data.Data object</p> Source code in <code>imgraph/data/transform_graph.py</code> <pre><code>def load_and_transform(gobject,name,type ,task = 'classification'):\n    \"\"\"\n    Args: gobject: networkx graph object\n            name: name of the graph\n            task: classification or regression\n            type: type of the graph train/test\n    Returns: torch_geometric.data.Data object\n    \"\"\"\n    data = Data()\n    try:\n        data = from_networkx(gobject)\n    except Exception as e:\n        print(\"error while nx to data transformation\",e,\"for image\",name)\n        #logging error here.\n        # pass\n    # yy = [0]\n    # if type:\n    #     data.y = [1]\n    #     yy = [1]\n    # else:\n    data.y = [type]\n    data.x = torch.Tensor([torch.flatten(val).tolist() for val in data.x])\n    data.name = name\n    print(data)\n    return data\n</code></pre>"},{"location":"imgraph/data/edge_creation/distance_edges/","title":"Distance Edges","text":""},{"location":"imgraph/data/edge_creation/distance_edges/#imgraphdataedge_creationdistance_edges","title":"<code>imgraph.data.edge_creation.distance_edges</code>","text":"<p>Functions for creating distance-based edges between nodes.</p>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.delaunay_edges","title":"<code>delaunay_edges(node_info, image=None)</code>","text":"<p>Creates edges based on Delaunay triangulation of node centroids.</p>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.delaunay_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.delaunay_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/distance_edges.py</code> <pre><code>def delaunay_edges(node_info, image=None):\n    \"\"\"\n    Creates edges based on Delaunay triangulation of node centroids.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    try:\n        from scipy.spatial import Delaunay\n    except ImportError:\n        raise ImportError(\"scipy is required for Delaunay triangulation.\")\n\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Handle edge cases\n    if len(centroids) &lt; 3:\n        # Fall back to fully connected graph for &lt; 3 nodes\n        edge_list = []\n        for i in range(len(centroids)):\n            for j in range(len(centroids)):\n                if i != j:\n                    edge_list.append((i, j))\n\n        if edge_list:\n            edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n        else:\n            edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n        return edge_index\n\n    # Compute Delaunay triangulation\n    tri = Delaunay(centroids)\n\n    # Create edges from triangulation\n    edge_list = []\n\n    for simplex in tri.simplices:\n        # Create edges for each side of the triangle\n        edge_list.append((simplex[0], simplex[1]))\n        edge_list.append((simplex[1], simplex[0]))\n        edge_list.append((simplex[0], simplex[2]))\n        edge_list.append((simplex[2], simplex[0]))\n        edge_list.append((simplex[1], simplex[2]))\n        edge_list.append((simplex[2], simplex[1]))\n\n    # Remove duplicates\n    edge_list = list(set(edge_list))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.distance_threshold_edges","title":"<code>distance_threshold_edges(node_info, threshold=50, image=None)</code>","text":"<p>Creates edges between nodes within a distance threshold.</p>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.distance_threshold_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation threshold : float, optional     Distance threshold, by default 50 image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.distance_threshold_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/distance_edges.py</code> <pre><code>def distance_threshold_edges(node_info, threshold=50, image=None):\n    \"\"\"\n    Creates edges between nodes within a distance threshold.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    threshold : float, optional\n        Distance threshold, by default 50\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Calculate pairwise distances\n    dist_matrix = distance_matrix(centroids, centroids)\n\n    # Create edges for node pairs within threshold\n    edge_list = []\n\n    for i in range(len(centroids)):\n        for j in range(len(centroids)):\n            if i != j and dist_matrix[i, j] &lt;= threshold:\n                edge_list.append((i, j))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.k_nearest_edges","title":"<code>k_nearest_edges(node_info, k=6, bidirectional=True, image=None)</code>","text":"<p>Creates edges between each node and its k nearest neighbors.</p>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.k_nearest_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation k : int, optional     Number of nearest neighbors, by default 6 bidirectional : bool, optional     Whether to make edges bidirectional, by default True image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_creation/distance_edges/#imgraph.data.edge_creation.distance_edges.k_nearest_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/distance_edges.py</code> <pre><code>def k_nearest_edges(node_info, k=6, bidirectional=True, image=None):\n    \"\"\"\n    Creates edges between each node and its k nearest neighbors.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    k : int, optional\n        Number of nearest neighbors, by default 6\n    bidirectional : bool, optional\n        Whether to make edges bidirectional, by default True\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Handle edge case with too few nodes\n    if len(centroids) &lt;= 1:\n        return torch.zeros((2, 0), dtype=torch.long)\n\n    # Adjust k if needed\n    k = min(k, len(centroids) - 1)\n\n    # Calculate pairwise distances\n    dist_matrix = distance_matrix(centroids, centroids)\n\n    # For each node, find k nearest neighbors\n    edge_list = []\n\n    for i in range(len(centroids)):\n        # Get distances from node i to all other nodes\n        distances = dist_matrix[i]\n\n        # Set self-distance to inf to exclude self\n        distances[i] = float('inf')\n\n        # Get indices of k nearest neighbors\n        nn_indices = np.argpartition(distances, k)[:k]\n\n        # Add edges\n        for j in nn_indices:\n            edge_list.append((i, j))\n\n            # Add reverse edge if bidirectional\n            if bidirectional:\n                edge_list.append((j, i))\n\n    # Remove duplicates if bidirectional\n    if bidirectional:\n        edge_list = list(set(edge_list))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/grid_edges/","title":"Grid Edges","text":""},{"location":"imgraph/data/edge_creation/grid_edges/#imgraphdataedge_creationgrid_edges","title":"<code>imgraph.data.edge_creation.grid_edges</code>","text":"<p>Functions for creating grid-based edges between nodes.</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.dense_grid_edges","title":"<code>dense_grid_edges(node_info, image=None)</code>","text":"<p>Creates fully-connected edges between all nodes in a grid.</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.dense_grid_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.dense_grid_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/grid_edges.py</code> <pre><code>def dense_grid_edges(node_info, image=None):\n    \"\"\"\n    Creates fully-connected edges between all nodes in a grid.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    # Get number of nodes\n    if 'positions' in node_info:\n        n_nodes = len(node_info['positions'])\n    elif 'centroids' in node_info:\n        n_nodes = len(node_info['centroids'])\n    else:\n        raise ValueError(\"Node info does not contain positions or centroids\")\n\n    # Create edge lists\n    edge_list = []\n\n    # Create fully-connected edges\n    for i in range(n_nodes):\n        for j in range(n_nodes):\n            if i != j:\n                edge_list.append((i, j))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_4_edges","title":"<code>grid_4_edges(node_info, image=None)</code>","text":"<p>Creates 4-connected grid edges (up, down, left, right) between nodes.</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_4_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_4_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/grid_edges.py</code> <pre><code>def grid_4_edges(node_info, image=None):\n    \"\"\"\n    Creates 4-connected grid edges (up, down, left, right) between nodes.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'positions' not in node_info:\n        raise ValueError(\"Node info does not contain positions\")\n\n    positions = node_info['positions']\n\n    # Find grid dimensions\n    if positions.size == 0:\n        return torch.zeros((2, 0), dtype=torch.long)\n\n    max_row = int(np.max(positions[:, 0]))\n    max_col = int(np.max(positions[:, 1]))\n\n    # Create position to node index mapping\n    pos_to_idx = {(int(pos[0]), int(pos[1])): i for i, pos in enumerate(positions)}\n\n    # Initialize edge lists\n    edge_list = []\n\n    # Create 4-connected edges\n    for pos, idx in pos_to_idx.items():\n        row, col = pos\n\n        # Up\n        if (row - 1, col) in pos_to_idx:\n            edge_list.append((idx, pos_to_idx[(row - 1, col)]))\n\n        # Down\n        if (row + 1, col) in pos_to_idx:\n            edge_list.append((idx, pos_to_idx[(row + 1, col)]))\n\n        # Left\n        if (row, col - 1) in pos_to_idx:\n            edge_list.append((idx, pos_to_idx[(row, col - 1)]))\n\n        # Right\n        if (row, col + 1) in pos_to_idx:\n            edge_list.append((idx, pos_to_idx[(row, col + 1)]))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_8_edges","title":"<code>grid_8_edges(node_info, image=None)</code>","text":"<p>Creates 8-connected grid edges (including diagonals) between nodes.</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_8_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_8_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/grid_edges.py</code> <pre><code>def grid_8_edges(node_info, image=None):\n    \"\"\"\n    Creates 8-connected grid edges (including diagonals) between nodes.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'positions' not in node_info:\n        raise ValueError(\"Node info does not contain positions\")\n\n    positions = node_info['positions']\n\n    # Find grid dimensions\n    if positions.size == 0:\n        return torch.zeros((2, 0), dtype=torch.long)\n\n    max_row = int(np.max(positions[:, 0]))\n    max_col = int(np.max(positions[:, 1]))\n\n    # Create position to node index mapping\n    pos_to_idx = {(int(pos[0]), int(pos[1])): i for i, pos in enumerate(positions)}\n\n    # Initialize edge lists\n    edge_list = []\n\n    # Create 8-connected edges\n    for pos, idx in pos_to_idx.items():\n        row, col = pos\n\n        # Check all 8 neighbors\n        for dr in [-1, 0, 1]:\n            for dc in [-1, 0, 1]:\n                if dr == 0 and dc == 0:\n                    continue  # Skip self\n\n                neighbor_pos = (row + dr, col + dc)\n                if neighbor_pos in pos_to_idx:\n                    edge_list.append((idx, pos_to_idx[neighbor_pos]))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_radius_edges","title":"<code>grid_radius_edges(node_info, radius=2, image=None)</code>","text":"<p>Creates grid edges within a given radius.</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_radius_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation radius : int, optional     Radius of connectivity, by default 2 image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_creation/grid_edges/#imgraph.data.edge_creation.grid_edges.grid_radius_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/grid_edges.py</code> <pre><code>def grid_radius_edges(node_info, radius=2, image=None):\n    \"\"\"\n    Creates grid edges within a given radius.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    radius : int, optional\n        Radius of connectivity, by default 2\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'positions' not in node_info:\n        raise ValueError(\"Node info does not contain positions\")\n\n    positions = node_info['positions']\n\n    # Create position to node index mapping\n    pos_to_idx = {(int(pos[0]), int(pos[1])): i for i, pos in enumerate(positions)}\n\n    # Initialize edge lists\n    edge_list = []\n\n    # Create radius-connected edges\n    for pos, idx in pos_to_idx.items():\n        row, col = pos\n\n        # Check all neighbors within radius\n        for dr in range(-radius, radius + 1):\n            for dc in range(-radius, radius + 1):\n                if dr == 0 and dc == 0:\n                    continue  # Skip self\n\n                # Check if within radius\n                if dr**2 + dc**2 &gt; radius**2:\n                    continue\n\n                neighbor_pos = (row + dr, col + dc)\n                if neighbor_pos in pos_to_idx:\n                    edge_list.append((idx, pos_to_idx[neighbor_pos]))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/region_adjacency/","title":"Region Adjacency","text":""},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraphdataedge_creationregion_adjacency","title":"<code>imgraph.data.edge_creation.region_adjacency</code>","text":"<p>Functions for creating region adjacency-based edges between nodes.</p>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.region_adjacency_edges","title":"<code>region_adjacency_edges(node_info, image=None, connectivity=2)</code>","text":"<p>Creates edges between adjacent regions (superpixels).</p>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.region_adjacency_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation image : numpy.ndarray, optional     Input image, by default None connectivity : int, optional     Connectivity for determining adjacency, by default 2 (8-connected)     Set to 1 for 4-connected adjacency.</p>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.region_adjacency_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/region_adjacency.py</code> <pre><code>def region_adjacency_edges(node_info, image=None, connectivity=2):\n    \"\"\"\n    Creates edges between adjacent regions (superpixels).\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    image : numpy.ndarray, optional\n        Input image, by default None\n    connectivity : int, optional\n        Connectivity for determining adjacency, by default 2 (8-connected)\n        Set to 1 for 4-connected adjacency.\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'segments' not in node_info:\n        raise ValueError(\"Node info does not contain segments\")\n\n    segments = node_info['segments']\n\n    try:\n        # Try using skimage's RAG functionality\n        from skimage.future import graph\n        rag = graph.RAG(segments, connectivity=connectivity)\n\n        # Convert RAG to edge list\n        edge_list = []\n        for u, v in rag.edges():\n            # Skip self-loops\n            if u != v:\n                edge_list.append((u, v))\n                edge_list.append((v, u))  # Make bidirectional\n    except ImportError:\n        # Manual implementation as fallback\n        # Find boundaries\n        boundaries = find_boundaries(segments, connectivity=connectivity, mode='thick')\n\n        # Create edge list\n        edge_list = []\n        unique_segments = np.unique(segments)\n\n        # For each segment, find its neighbors\n        for i in unique_segments:\n            # Create mask for current segment\n            segment_mask = segments == i\n\n            # Dilate mask to find neighbors\n            from scipy.ndimage import binary_dilation\n\n            # Use 4-connected or 8-connected structure\n            if connectivity == 1:\n                struct = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n            else:\n                struct = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n\n            dilated = binary_dilation(segment_mask, structure=struct)\n\n            # Find neighbors\n            neighbors = np.unique(segments[dilated &amp; ~segment_mask])\n\n            # Add edges\n            for j in neighbors:\n                if i != j:\n                    edge_list.append((i, j))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.region_boundary_edges","title":"<code>region_boundary_edges(node_info, image=None, min_boundary_size=10)</code>","text":"<p>Creates edges between regions with shared boundaries. Also computes boundary length as an edge attribute.</p>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.region_boundary_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation image : numpy.ndarray, optional     Input image, by default None min_boundary_size : int, optional     Minimum boundary size to create an edge, by default 10</p>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.region_boundary_edges--returns","title":"Returns","text":"<p>tuple     (edge_index, edge_attr)     - edge_index: torch.Tensor with shape (2, E)     - edge_attr: torch.Tensor with shape (E, 1) containing boundary sizes</p> Source code in <code>imgraph/data/edge_creation/region_adjacency.py</code> <pre><code>def region_boundary_edges(node_info, image=None, min_boundary_size=10):\n    \"\"\"\n    Creates edges between regions with shared boundaries.\n    Also computes boundary length as an edge attribute.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    image : numpy.ndarray, optional\n        Input image, by default None\n    min_boundary_size : int, optional\n        Minimum boundary size to create an edge, by default 10\n\n    Returns\n    -------\n    tuple\n        (edge_index, edge_attr)\n        - edge_index: torch.Tensor with shape (2, E)\n        - edge_attr: torch.Tensor with shape (E, 1) containing boundary sizes\n    \"\"\"\n    if 'segments' not in node_info:\n        raise ValueError(\"Node info does not contain segments\")\n\n    segments = node_info['segments']\n\n    # Find boundaries\n    boundaries = find_boundaries(segments, mode='thick')\n\n    # Create edge list and boundary sizes\n    edge_list = []\n    boundary_sizes = []\n\n    unique_segments = np.unique(segments)\n\n    # For each pair of segments, find their boundary size\n    for i in unique_segments:\n        for j in unique_segments:\n            if i &gt;= j:  # Skip self and avoid duplicates\n                continue\n\n            # Create masks for both segments\n            mask_i = segments == i\n            mask_j = segments == j\n\n            # Dilate both masks\n            from scipy.ndimage import binary_dilation\n            struct = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n\n            dilated_i = binary_dilation(mask_i, structure=struct)\n            dilated_j = binary_dilation(mask_j, structure=struct)\n\n            # Find shared boundary pixels\n            boundary_ij = boundaries &amp; dilated_i &amp; dilated_j\n            boundary_size = np.sum(boundary_ij)\n\n            # Add edge if boundary is large enough\n            if boundary_size &gt;= min_boundary_size:\n                edge_list.append((i, j))\n                edge_list.append((j, i))  # Make bidirectional\n                boundary_sizes.append(boundary_size)\n                boundary_sizes.append(boundary_size)  # Same for bidirectional\n\n    # Convert to tensors\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n        edge_attr = torch.tensor(boundary_sizes, dtype=torch.float).view(-1, 1)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n        edge_attr = torch.zeros((0, 1), dtype=torch.float)\n\n    return edge_index, edge_attr\n</code></pre>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.superpixel_containment_edges","title":"<code>superpixel_containment_edges(node_info, image=None, levels=2)</code>","text":"<p>Creates hierarchical edges between superpixels at different levels. Useful for creating hierarchical graph representations.</p>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.superpixel_containment_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation image : numpy.ndarray, optional     Input image, by default None levels : int, optional     Number of hierarchical levels, by default 2</p>"},{"location":"imgraph/data/edge_creation/region_adjacency/#imgraph.data.edge_creation.region_adjacency.superpixel_containment_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/region_adjacency.py</code> <pre><code>def superpixel_containment_edges(node_info, image=None, levels=2):\n    \"\"\"\n    Creates hierarchical edges between superpixels at different levels.\n    Useful for creating hierarchical graph representations.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    image : numpy.ndarray, optional\n        Input image, by default None\n    levels : int, optional\n        Number of hierarchical levels, by default 2\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'segments' not in node_info:\n        raise ValueError(\"Node info does not contain segments\")\n\n    try:\n        from skimage.segmentation import slic\n    except ImportError:\n        raise ImportError(\"scikit-image is required for hierarchical superpixel segmentation.\")\n\n    segments = node_info['segments']\n\n    # Get image\n    if image is None:\n        raise ValueError(\"Image is required for hierarchical segmentation\")\n\n    # Number of segments at the finest level\n    n_segments_base = len(np.unique(segments))\n\n    # Create segments at coarser levels\n    hierarchical_segments = [segments]  # Start with the finest level\n\n    for level in range(1, levels):\n        n_segments = max(n_segments_base // (2**level), 2)  # Reduce by factor of 2 per level\n\n        # Create coarser segmentation\n        coarse_segments = slic(image, n_segments=n_segments, compactness=10, sigma=0, start_label=0)\n        hierarchical_segments.append(coarse_segments)\n\n    # Create edge list for hierarchical connections\n    edge_list = []\n\n    # Connect fine to coarse segments\n    for level in range(levels - 1):\n        fine_segments = hierarchical_segments[level]\n        coarse_segments = hierarchical_segments[level + 1]\n\n        # Find containment relationships\n        for i in np.unique(fine_segments):\n            # Find which coarse segment contains this fine segment\n            fine_mask = fine_segments == i\n            coarse_labels = coarse_segments[fine_mask]\n\n            # Most common coarse label\n            from scipy.stats import mode\n            if len(coarse_labels) &gt; 0:\n                most_common = mode(coarse_labels)[0][0]\n\n                # Add containment edge\n                edge_list.append((i, most_common + n_segments_base))  # Offset coarse indices\n                edge_list.append((most_common + n_segments_base, i))  # Bidirectional\n\n    # Add regular adjacency edges for each level\n    for segments in hierarchical_segments:\n        try:\n            from skimage.future import graph\n            rag = graph.RAG(segments)\n\n            # Add edges\n            for u, v in rag.edges():\n                if u != v:\n                    edge_list.append((u, v))\n                    edge_list.append((v, u))\n        except ImportError:\n            # Simple adjacency fallback\n            pass\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/similarity_edges/","title":"Similarity Edges","text":""},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraphdataedge_creationsimilarity_edges","title":"<code>imgraph.data.edge_creation.similarity_edges</code>","text":"<p>Functions for creating similarity-based edges between nodes.</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.color_similarity_edges","title":"<code>color_similarity_edges(image, node_info, threshold=0.8, k=None)</code>","text":"<p>Creates edges between nodes based on color similarity.</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.color_similarity_edges--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation threshold : float, optional     Similarity threshold, by default 0.8 k : int, optional     If provided, connect each node to its k most similar neighbors,     ignoring the threshold, by default None</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.color_similarity_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/similarity_edges.py</code> <pre><code>def color_similarity_edges(image, node_info, threshold=0.8, k=None):\n    \"\"\"\n    Creates edges between nodes based on color similarity.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    threshold : float, optional\n        Similarity threshold, by default 0.8\n    k : int, optional\n        If provided, connect each node to its k most similar neighbors,\n        ignoring the threshold, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    # Extract mean colors per node\n    colors = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n            if np.any(mask):\n                region = image[mask]\n                mean_color = np.mean(region, axis=0)\n                colors.append(mean_color)\n            else:\n                # Handle empty segments (should not happen)\n                colors.append(np.zeros(image.shape[-1]))\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n\n        for patch in patches:\n            mean_color = np.mean(patch, axis=(0, 1))\n            colors.append(mean_color)\n\n    elif 'values' in node_info:  # Pixel nodes\n        colors = node_info['values']\n\n    else:\n        raise ValueError(\"Unsupported node type\")\n\n    # Convert to numpy array\n    colors = np.array(colors)\n\n    # Compute feature similarity edges\n    return feature_similarity_edges(colors, threshold=threshold, metric='cosine', k=k)\n</code></pre>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.feature_similarity_edges","title":"<code>feature_similarity_edges(node_features, threshold=0.7, metric='cosine', k=None)</code>","text":"<p>Creates edges between nodes based on feature similarity.</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.feature_similarity_edges--parameters","title":"Parameters","text":"<p>node_features : torch.Tensor     Node feature tensor with shape (N, F) threshold : float, optional     Similarity threshold, by default 0.7 metric : str, optional     Similarity metric, by default 'cosine'     Options: 'cosine', 'euclidean', 'correlation' k : int, optional     If provided, connect each node to its k most similar neighbors,     ignoring the threshold, by default None</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.feature_similarity_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/similarity_edges.py</code> <pre><code>def feature_similarity_edges(node_features, threshold=0.7, metric='cosine', k=None):\n    \"\"\"\n    Creates edges between nodes based on feature similarity.\n\n    Parameters\n    ----------\n    node_features : torch.Tensor\n        Node feature tensor with shape (N, F)\n    threshold : float, optional\n        Similarity threshold, by default 0.7\n    metric : str, optional\n        Similarity metric, by default 'cosine'\n        Options: 'cosine', 'euclidean', 'correlation'\n    k : int, optional\n        If provided, connect each node to its k most similar neighbors,\n        ignoring the threshold, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    # Convert tensor to numpy if needed\n    if isinstance(node_features, torch.Tensor):\n        node_features_np = node_features.detach().cpu().numpy()\n    else:\n        node_features_np = node_features\n\n    # Calculate similarity matrix\n    if metric == 'cosine':\n        # Higher value = more similar\n        sim_matrix = cosine_similarity(node_features_np)\n    elif metric == 'euclidean':\n        # Lower value = more similar, so invert\n        dist_matrix = distance_matrix(node_features_np, node_features_np)\n        max_dist = np.max(dist_matrix) if np.max(dist_matrix) &gt; 0 else 1\n        sim_matrix = 1 - (dist_matrix / max_dist)\n    elif metric == 'correlation':\n        # Use correlation coefficient\n        # Normalize features\n        normalized = node_features_np - np.mean(node_features_np, axis=1, keepdims=True)\n        norms = np.linalg.norm(normalized, axis=1, keepdims=True)\n        normalized = normalized / (norms + 1e-8)\n\n        # Compute correlation\n        sim_matrix = np.dot(normalized, normalized.T)\n    else:\n        raise ValueError(f\"Unsupported metric: {metric}\")\n\n    # Create edge list\n    edge_list = []\n\n    if k is not None:\n        # K most similar for each node\n        k = min(k, len(node_features_np) - 1)\n\n        for i in range(len(node_features_np)):\n            # Get similarities to node i\n            similarities = sim_matrix[i]\n\n            # Set self-similarity to -inf to exclude self\n            similarities[i] = -np.inf\n\n            # Get indices of k most similar nodes\n            most_similar = np.argpartition(similarities, -k)[-k:]\n\n            # Add edges\n            for j in most_similar:\n                edge_list.append((i, j))\n    else:\n        # Threshold-based\n        for i in range(len(node_features_np)):\n            for j in range(len(node_features_np)):\n                if i != j and sim_matrix[i, j] &gt;= threshold:\n                    edge_list.append((i, j))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.spatial_similarity_edges","title":"<code>spatial_similarity_edges(node_info, threshold=0.2, k=None)</code>","text":"<p>Creates edges between nodes based on spatial proximity.</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.spatial_similarity_edges--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation threshold : float, optional     Similarity threshold as a fraction of the image size, by default 0.2 k : int, optional     If provided, connect each node to its k closest neighbors,     ignoring the threshold, by default None</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.spatial_similarity_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/similarity_edges.py</code> <pre><code>def spatial_similarity_edges(node_info, threshold=0.2, k=None):\n    \"\"\"\n    Creates edges between nodes based on spatial proximity.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    threshold : float, optional\n        Similarity threshold as a fraction of the image size, by default 0.2\n    k : int, optional\n        If provided, connect each node to its k closest neighbors,\n        ignoring the threshold, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Calculate pairwise distances\n    dist_matrix = distance_matrix(centroids, centroids)\n\n    # Calculate max possible distance for normalization\n    max_dist = np.max(dist_matrix) if np.max(dist_matrix) &gt; 0 else 1\n\n    # Convert to similarity (higher = more similar)\n    sim_matrix = 1 - (dist_matrix / max_dist)\n\n    # Create edge list\n    edge_list = []\n\n    if k is not None:\n        # K closest neighbors for each node\n        k = min(k, len(centroids) - 1)\n\n        for i in range(len(centroids)):\n            # Get similarities to node i\n            similarities = sim_matrix[i]\n\n            # Set self-similarity to -inf to exclude self\n            similarities[i] = -np.inf\n\n            # Get indices of k most similar nodes\n            most_similar = np.argpartition(similarities, -k)[-k:]\n\n            # Add edges\n            for j in most_similar:\n                edge_list.append((i, j))\n    else:\n        # Threshold-based\n        for i in range(len(centroids)):\n            for j in range(len(centroids)):\n                if i != j and sim_matrix[i, j] &gt;= threshold:\n                    edge_list.append((i, j))\n\n    # Convert to tensor\n    if edge_list:\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    return edge_index\n</code></pre>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.texture_similarity_edges","title":"<code>texture_similarity_edges(image, node_info, threshold=0.7, k=None)</code>","text":"<p>Creates edges between nodes based on texture similarity.</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.texture_similarity_edges--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation threshold : float, optional     Similarity threshold, by default 0.7 k : int, optional     If provided, connect each node to its k most similar neighbors,     ignoring the threshold, by default None</p>"},{"location":"imgraph/data/edge_creation/similarity_edges/#imgraph.data.edge_creation.similarity_edges.texture_similarity_edges--returns","title":"Returns","text":"<p>torch.Tensor     Edge index tensor with shape (2, E)</p> Source code in <code>imgraph/data/edge_creation/similarity_edges.py</code> <pre><code>def texture_similarity_edges(image, node_info, threshold=0.7, k=None):\n    \"\"\"\n    Creates edges between nodes based on texture similarity.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    threshold : float, optional\n        Similarity threshold, by default 0.7\n    k : int, optional\n        If provided, connect each node to its k most similar neighbors,\n        ignoring the threshold, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge index tensor with shape (2, E)\n    \"\"\"\n    try:\n        from skimage.feature import local_binary_pattern\n    except ImportError:\n        raise ImportError(\"scikit-image is required for texture similarity.\")\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        try:\n            import cv2\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        except ImportError:\n            # Simple grayscale conversion as fallback\n            gray = np.mean(image, axis=2)\n    else:\n        gray = image\n\n    # Ensure image is in float format\n    if gray.dtype == np.uint8:\n        gray = gray.astype(np.float32) / 255.0\n\n    # Compute LBP\n    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n\n    # Extract texture histograms\n    histograms = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n            if np.any(mask):\n                region_lbp = lbp[mask]\n                hist, _ = np.histogram(region_lbp, bins=10, range=(0, 10), density=True)\n                histograms.append(hist)\n            else:\n                # Handle empty segments (should not happen)\n                histograms.append(np.zeros(10))\n\n    elif 'patches' in node_info:  # Patch nodes\n        bboxes = node_info['bboxes']\n\n        for bbox in bboxes:\n            top, left, bottom, right = bbox\n            patch_lbp = lbp[top:bottom, left:right]\n\n            if patch_lbp.size &gt; 0:\n                hist, _ = np.histogram(patch_lbp, bins=10, range=(0, 10), density=True)\n                histograms.append(hist)\n            else:\n                # Handle empty patches (should not happen)\n                histograms.append(np.zeros(10))\n\n    elif 'values' in node_info:  # Pixel nodes\n        # For pixel nodes, we can't compute texture features\n        # Just use dummy features\n        histograms = np.random.rand(len(node_info['positions']), 10)\n\n    else:\n        raise ValueError(\"Unsupported node type\")\n\n    # Convert to numpy array\n    histograms = np.array(histograms)\n\n    # Compute feature similarity edges\n    return feature_similarity_edges(histograms, threshold=threshold, metric='cosine', k=k)\n</code></pre>"},{"location":"imgraph/data/edge_features/boundary_features/","title":"Boundary Features","text":""},{"location":"imgraph/data/edge_features/boundary_features/#imgraphdataedge_featuresboundary_features","title":"<code>imgraph.data.edge_features.boundary_features</code>","text":"<p>Functions for creating edge features based on region boundaries.</p>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_contrast","title":"<code>boundary_contrast(image, node_info, edge_index, normalize=True)</code>","text":"<p>Computes edge features based on contrast across region boundaries.</p>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_contrast--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) normalize : bool, optional     Whether to normalize features, by default True</p>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_contrast--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, 1) containing boundary contrast</p> Source code in <code>imgraph/data/edge_features/boundary_features.py</code> <pre><code>def boundary_contrast(image, node_info, edge_index, normalize=True):\n    \"\"\"\n    Computes edge features based on contrast across region boundaries.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    normalize : bool, optional\n        Whether to normalize features, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, 1) containing boundary contrast\n    \"\"\"\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, 1), dtype=torch.float)\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        try:\n            import cv2\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        except ImportError:\n            # Simple grayscale conversion as fallback\n            gray = np.mean(image, axis=2)\n    else:\n        gray = image\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Compute region means based on node type\n    region_means = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        # Calculate mean intensity for each segment\n        for i in unique_segments:\n            mask = segments == i\n            region = gray[mask]\n\n            if region.size &gt; 0:\n                mean = np.mean(region)\n            else:\n                mean = 0\n\n            region_means.append(mean)\n\n        # Compute contrast for each edge\n        contrasts = []\n\n        for s, d in zip(src.tolist(), dst.tolist()):\n            # Get mean intensities\n            mean_s = region_means[s]\n            mean_d = region_means[d]\n\n            # Compute absolute difference\n            contrast = abs(mean_s - mean_d)\n            contrasts.append(contrast)\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n\n        # Calculate mean intensity for each patch\n        for patch in patches:\n            if patch.ndim == 3:\n                # Convert to grayscale if needed\n                if patch.shape[2] &gt; 1:\n                    patch_gray = np.mean(patch, axis=2)\n                else:\n                    patch_gray = patch[:, :, 0]\n            else:\n                patch_gray = patch\n\n            if patch_gray.size &gt; 0:\n                mean = np.mean(patch_gray)\n            else:\n                mean = 0\n\n            region_means.append(mean)\n\n        # Compute contrast for each edge\n        contrasts = []\n\n        for s, d in zip(src.tolist(), dst.tolist()):\n            # Get mean intensities\n            mean_s = region_means[s]\n            mean_d = region_means[d]\n\n            # Compute absolute difference\n            contrast = abs(mean_s - mean_d)\n            contrasts.append(contrast)\n\n    else:\n        # For other node types, use default values\n        contrasts = [0] * edge_index.shape[1]\n\n    # Convert to tensor\n    contrasts = torch.tensor(contrasts, dtype=torch.float).unsqueeze(1)\n\n    # Normalize if requested\n    if normalize:\n        max_contrast = torch.max(contrasts)\n        if max_contrast &gt; 0:\n            contrasts = contrasts / max_contrast\n\n    return contrasts\n</code></pre>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_orientation","title":"<code>boundary_orientation(image, node_info, edge_index, num_bins=8)</code>","text":"<p>Computes edge features based on boundary orientation between regions.</p>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_orientation--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) num_bins : int, optional     Number of orientation bins, by default 8</p>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_orientation--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, num_bins) containing orientation histograms</p> Source code in <code>imgraph/data/edge_features/boundary_features.py</code> <pre><code>def boundary_orientation(image, node_info, edge_index, num_bins=8):\n    \"\"\"\n    Computes edge features based on boundary orientation between regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    num_bins : int, optional\n        Number of orientation bins, by default 8\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, num_bins) containing orientation histograms\n    \"\"\"\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, num_bins), dtype=torch.float)\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        try:\n            import cv2\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        except ImportError:\n            # Simple grayscale conversion as fallback\n            gray = np.mean(image, axis=2)\n    else:\n        gray = image\n\n    # Ensure image is in float format\n    if gray.dtype == np.uint8:\n        gray = gray.astype(np.float32) / 255.0\n\n    try:\n        # Compute gradients\n        from scipy import ndimage\n\n        # Sobel gradients\n        grad_y = ndimage.sobel(gray, axis=0)\n        grad_x = ndimage.sobel(gray, axis=1)\n\n        # Compute gradient magnitude and orientation\n        grad_mag = np.sqrt(grad_y**2 + grad_x**2)\n        grad_orientation = np.arctan2(grad_y, grad_x)\n\n        # Convert orientation to degrees and shift to [0, 180)\n        grad_orientation_deg = np.rad2deg(grad_orientation) % 180\n    except ImportError:\n        # Fallback method for gradient computation\n        grad_mag = np.zeros_like(gray)\n        grad_orientation_deg = np.zeros_like(gray)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Compute orientation histograms based on node type\n    orientation_histograms = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n\n        # Compute segment boundaries\n        segment_boundaries = find_boundaries(segments, mode='thin')\n\n        # For each edge, compute the orientation histogram at the boundary\n        for s, d in zip(src.tolist(), dst.tolist()):\n            # Create masks for both segments\n            mask_s = segments == s\n            mask_d = segments == d\n\n            # Dilate both masks\n            from scipy.ndimage import binary_dilation\n            struct = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n\n            dilated_s = binary_dilation(mask_s, structure=struct)\n            dilated_d = binary_dilation(mask_d, structure=struct)\n\n            # Find boundary pixels between s and d\n            boundary_region = dilated_s &amp; dilated_d &amp; segment_boundaries\n\n            # Compute orientation histogram\n            if np.any(boundary_region):\n                # Extract orientations and magnitudes at boundary\n                boundary_orientations = grad_orientation_deg[boundary_region]\n                boundary_magnitudes = grad_mag[boundary_region]\n\n                # Compute weighted histogram\n                hist, _ = np.histogram(\n                    boundary_orientations,\n                    bins=num_bins,\n                    range=(0, 180),\n                    weights=boundary_magnitudes\n                )\n\n                # Normalize histogram\n                hist_sum = np.sum(hist)\n                if hist_sum &gt; 0:\n                    hist = hist / hist_sum\n            else:\n                # No boundary found\n                hist = np.zeros(num_bins)\n\n            orientation_histograms.append(hist)\n\n    elif 'patches' in node_info:  # Patch nodes\n        positions = node_info['positions']\n        bboxes = node_info['bboxes']\n\n        # For each edge, estimate the boundary orientation\n        for s, d in zip(src.tolist(), dst.tolist()):\n            # Get positions of the patches\n            pos_s = positions[s]\n            pos_d = positions[d]\n\n            # Check if patches are adjacent\n            if (abs(pos_s[0] - pos_d[0]) &lt;= 1 and abs(pos_s[1] - pos_d[1]) &lt;= 1):\n                # Get bounding boxes\n                bbox_s = bboxes[s]\n                bbox_d = bboxes[d]\n\n                # Determine boundary region\n                top_s, left_s, bottom_s, right_s = bbox_s\n                top_d, left_d, bottom_d, right_d = bbox_d\n\n                # Find overlapping region\n                top = max(top_s, top_d)\n                bottom = min(bottom_s, bottom_d)\n                left = max(left_s, left_d)\n                right = min(right_s, right_d)\n\n                # Check if there's a valid boundary\n                if top &lt; bottom and left &lt; right:\n                    # Extract boundary orientations and magnitudes\n                    boundary_orientations = grad_orientation_deg[top:bottom, left:right]\n                    boundary_magnitudes = grad_mag[top:bottom, left:right]\n\n                    # Compute weighted histogram\n                    hist, _ = np.histogram(\n                        boundary_orientations.flatten(),\n                        bins=num_bins,\n                        range=(0, 180),\n                        weights=boundary_magnitudes.flatten()\n                    )\n\n                    # Normalize histogram\n                    hist_sum = np.sum(hist)\n                    if hist_sum &gt; 0:\n                        hist = hist / hist_sum\n                else:\n                    # Patches are adjacent but don't share a boundary\n                    hist = np.zeros(num_bins)\n            else:\n                # Patches are not adjacent\n                hist = np.zeros(num_bins)\n\n            orientation_histograms.append(hist)\n\n    else:\n        # For other node types, use a uniform distribution\n        uniform_hist = np.ones(num_bins) / num_bins\n        orientation_histograms = [uniform_hist] * edge_index.shape[1]\n\n    # Convert to tensor\n    return torch.tensor(np.array(orientation_histograms), dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_strength","title":"<code>boundary_strength(image, node_info, edge_index, normalize=True)</code>","text":"<p>Computes edge features based on boundary strength between regions.</p>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_strength--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) normalize : bool, optional     Whether to normalize features, by default True</p>"},{"location":"imgraph/data/edge_features/boundary_features/#imgraph.data.edge_features.boundary_features.boundary_strength--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, 1) containing boundary strengths</p> Source code in <code>imgraph/data/edge_features/boundary_features.py</code> <pre><code>def boundary_strength(image, node_info, edge_index, normalize=True):\n    \"\"\"\n    Computes edge features based on boundary strength between regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    normalize : bool, optional\n        Whether to normalize features, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, 1) containing boundary strengths\n    \"\"\"\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, 1), dtype=torch.float)\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        try:\n            import cv2\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        except ImportError:\n            # Simple grayscale conversion as fallback\n            gray = np.mean(image, axis=2)\n    else:\n        gray = image\n\n    # Ensure image is in float format\n    if gray.dtype == np.uint8:\n        gray = gray.astype(np.float32) / 255.0\n\n    # Compute image edges using Canny edge detector\n    edges = canny(gray, sigma=1.0)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Compute boundary strengths based on node type\n    boundary_strengths = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n\n        # Compute segment boundaries\n        segment_boundaries = find_boundaries(segments, mode='thick')\n\n        # Combine with edge detection for stronger boundaries\n        combined_boundaries = np.logical_or(segment_boundaries, edges)\n\n        # For each edge, compute the boundary strength between the two segments\n        for s, d in zip(src.tolist(), dst.tolist()):\n            # Create masks for both segments\n            mask_s = segments == s\n            mask_d = segments == d\n\n            # Dilate both masks\n            from scipy.ndimage import binary_dilation\n            struct = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n\n            dilated_s = binary_dilation(mask_s, structure=struct)\n            dilated_d = binary_dilation(mask_d, structure=struct)\n\n            # Find boundary pixels between s and d\n            boundary_region = dilated_s &amp; dilated_d\n\n            # Count boundary pixels\n            if np.any(boundary_region):\n                # Measure boundary strength as the average edge response\n                boundary_edge_response = combined_boundaries[boundary_region]\n                strength = np.mean(boundary_edge_response) if np.any(boundary_edge_response) else 0\n            else:\n                strength = 0\n\n            boundary_strengths.append(strength)\n\n    elif 'patches' in node_info:  # Patch nodes\n        positions = node_info['positions']\n        bboxes = node_info['bboxes']\n\n        # Create patch index mask\n        patch_mask = np.zeros(gray.shape, dtype=int)\n\n        for i, bbox in enumerate(bboxes):\n            top, left, bottom, right = bbox\n            patch_mask[top:bottom, left:right] = i\n\n        # For each edge, estimate the boundary strength\n        for s, d in zip(src.tolist(), dst.tolist()):\n            # Get positions of the patches\n            pos_s = positions[s]\n            pos_d = positions[d]\n\n            # Check if patches are adjacent\n            if (abs(pos_s[0] - pos_d[0]) &lt;= 1 and abs(pos_s[1] - pos_d[1]) &lt;= 1):\n                # Get bounding boxes\n                bbox_s = bboxes[s]\n                bbox_d = bboxes[d]\n\n                # Determine boundary region\n                top_s, left_s, bottom_s, right_s = bbox_s\n                top_d, left_d, bottom_d, right_d = bbox_d\n\n                # Find overlapping region\n                top = max(top_s, top_d)\n                bottom = min(bottom_s, bottom_d)\n                left = max(left_s, left_d)\n                right = min(right_s, right_d)\n\n                # Check if there's a valid boundary\n                if top &lt; bottom and left &lt; right:\n                    # Extract boundary region\n                    boundary_region = edges[top:bottom, left:right]\n\n                    # Measure boundary strength\n                    strength = np.mean(boundary_region) if boundary_region.size &gt; 0 else 0\n                else:\n                    # Patches are adjacent but don't share a boundary\n                    strength = 0\n            else:\n                # Patches are not adjacent\n                strength = 0\n\n            boundary_strengths.append(strength)\n\n    else:\n        # For other node types, use a default value\n        boundary_strengths = [0] * edge_index.shape[1]\n\n    # Convert to tensor\n    boundary_strengths = torch.tensor(boundary_strengths, dtype=torch.float).unsqueeze(1)\n\n    # Normalize if requested\n    if normalize:\n        max_strength = torch.max(boundary_strengths)\n        if max_strength &gt; 0:\n            boundary_strengths = boundary_strengths / max_strength\n\n    return boundary_strengths\n</code></pre>"},{"location":"imgraph/data/edge_features/feature_diff/","title":"Feature Diff","text":""},{"location":"imgraph/data/edge_features/feature_diff/#imgraphdataedge_featuresfeature_diff","title":"<code>imgraph.data.edge_features.feature_diff</code>","text":"<p>Functions for creating edge features based on node feature differences.</p>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.color_difference","title":"<code>color_difference(image, node_info, edge_index, color_space='rgb', normalize=True)</code>","text":"<p>Computes edge features as color differences between nodes.</p>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.color_difference--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) color_space : str, optional     Color space to use, by default 'rgb'     Options: 'rgb', 'hsv', 'lab' normalize : bool, optional     Whether to normalize features, by default True</p>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.color_difference--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, C)</p> Source code in <code>imgraph/data/edge_features/feature_diff.py</code> <pre><code>def color_difference(image, node_info, edge_index, color_space='rgb', normalize=True):\n    \"\"\"\n    Computes edge features as color differences between nodes.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    color_space : str, optional\n        Color space to use, by default 'rgb'\n        Options: 'rgb', 'hsv', 'lab'\n    normalize : bool, optional\n        Whether to normalize features, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, C)\n    \"\"\"\n    # Extract mean colors per node\n    colors = []\n\n    # Convert color space if necessary\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\"OpenCV (cv2) is required for color space conversion.\")\n\n    if color_space.lower() != 'rgb':\n        if color_space.lower() == 'hsv':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif color_space.lower() == 'lab':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n        elif color_space.lower() == 'ycrcb':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n        else:\n            raise ValueError(f\"Unsupported color space: {color_space}\")\n    else:\n        img_converted = image.copy()\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n            if np.any(mask):\n                region = img_converted[mask]\n                mean_color = np.mean(region, axis=0)\n                colors.append(mean_color)\n            else:\n                # Handle empty segments (should not happen)\n                colors.append(np.zeros(img_converted.shape[-1]))\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n\n        for patch in patches:\n            mean_color = np.mean(patch, axis=(0, 1))\n            colors.append(mean_color)\n\n    elif 'values' in node_info:  # Pixel nodes\n        colors = node_info['values']\n\n    else:\n        raise ValueError(\"Unsupported node type\")\n\n    # Convert to tensor\n    node_colors = torch.tensor(np.array(colors), dtype=torch.float)\n\n    # Compute feature differences\n    return feature_difference(node_colors, edge_index, normalize=normalize, mode='absolute')\n</code></pre>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.feature_difference","title":"<code>feature_difference(node_features, edge_index, normalize=True, mode='absolute')</code>","text":"<p>Computes edge features as differences between node features.</p>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.feature_difference--parameters","title":"Parameters","text":"<p>node_features : torch.Tensor     Node feature tensor with shape (N, F) edge_index : torch.Tensor     Edge index tensor with shape (2, E) normalize : bool, optional     Whether to normalize features, by default True mode : str, optional     Difference mode, by default 'absolute'     Options: 'absolute', 'squared', 'relative', 'concat'</p>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.feature_difference--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, F) or (E, 2*F) if mode='concat'</p> Source code in <code>imgraph/data/edge_features/feature_diff.py</code> <pre><code>def feature_difference(node_features, edge_index, normalize=True, mode='absolute'):\n    \"\"\"\n    Computes edge features as differences between node features.\n\n    Parameters\n    ----------\n    node_features : torch.Tensor\n        Node feature tensor with shape (N, F)\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    normalize : bool, optional\n        Whether to normalize features, by default True\n    mode : str, optional\n        Difference mode, by default 'absolute'\n        Options: 'absolute', 'squared', 'relative', 'concat'\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, F) or (E, 2*F) if mode='concat'\n    \"\"\"\n    if edge_index.shape[1] == 0:\n        if mode == 'concat':\n            return torch.zeros((0, 2 * node_features.shape[1]), dtype=torch.float)\n        else:\n            return torch.zeros((0, node_features.shape[1]), dtype=torch.float)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Extract source and target node features\n    src_features = node_features[src]\n    dst_features = node_features[dst]\n\n    # Compute differences\n    if mode == 'absolute':\n        diff = torch.abs(src_features - dst_features)\n    elif mode == 'squared':\n        diff = (src_features - dst_features) ** 2\n    elif mode == 'relative':\n        # Avoid division by zero\n        epsilon = 1e-10\n        diff = torch.abs(src_features - dst_features) / (torch.abs(src_features) + torch.abs(dst_features) + epsilon)\n    elif mode == 'concat':\n        # Concatenate source and target features\n        diff = torch.cat([src_features, dst_features], dim=1)\n    else:\n        raise ValueError(f\"Unsupported mode: {mode}\")\n\n    # Normalize if requested\n    if normalize and mode != 'concat':\n        # Compute max value for each feature dimension\n        max_values, _ = torch.max(diff, dim=0, keepdim=True)\n        max_values[max_values == 0] = 1.0  # Avoid division by zero\n\n        # Normalize\n        diff = diff / max_values\n\n    return diff\n</code></pre>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.normalized_color_difference","title":"<code>normalized_color_difference(image, node_info, edge_index, color_space='lab')</code>","text":"<p>Computes normalized color differences between nodes, optimized for perceptual similarity.</p>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.normalized_color_difference--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) color_space : str, optional     Color space to use, by default 'lab'</p>"},{"location":"imgraph/data/edge_features/feature_diff/#imgraph.data.edge_features.feature_diff.normalized_color_difference--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, 1) containing perceptual color differences</p> Source code in <code>imgraph/data/edge_features/feature_diff.py</code> <pre><code>def normalized_color_difference(image, node_info, edge_index, color_space='lab'):\n    \"\"\"\n    Computes normalized color differences between nodes, optimized for perceptual similarity.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    color_space : str, optional\n        Color space to use, by default 'lab'\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, 1) containing perceptual color differences\n    \"\"\"\n    # For perceptual color difference, LAB color space is best\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\"OpenCV (cv2) is required for color space conversion.\")\n\n    # Convert to LAB color space\n    if color_space.lower() == 'lab':\n        img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    else:\n        # Use requested color space\n        if color_space.lower() == 'hsv':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif color_space.lower() == 'ycrcb':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n        elif color_space.lower() == 'rgb':\n            img_converted = image.copy()\n        else:\n            raise ValueError(f\"Unsupported color space: {color_space}\")\n\n    # Extract mean colors per node\n    colors = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n            if np.any(mask):\n                region = img_converted[mask]\n                mean_color = np.mean(region, axis=0)\n                colors.append(mean_color)\n            else:\n                colors.append(np.zeros(img_converted.shape[-1]))\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n        bboxes = node_info['bboxes']\n\n        for i, bbox in enumerate(bboxes):\n            top, left, bottom, right = bbox\n            region = img_converted[top:bottom, left:right]\n\n            if region.size &gt; 0:\n                mean_color = np.mean(region, axis=(0, 1))\n                colors.append(mean_color)\n            else:\n                colors.append(np.zeros(img_converted.shape[-1]))\n\n    elif 'values' in node_info:  # Pixel nodes\n        colors = node_info['values']\n\n    else:\n        raise ValueError(\"Unsupported node type\")\n\n    # Convert to tensor\n    node_colors = torch.tensor(np.array(colors), dtype=torch.float)\n\n    # Extract source and target node indices\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, 1), dtype=torch.float)\n\n    src, dst = edge_index\n\n    # Extract source and target node colors\n    src_colors = node_colors[src]\n    dst_colors = node_colors[dst]\n\n    # Compute color differences\n    if color_space.lower() == 'lab':\n        # Delta E in CIELAB space\n        L1, a1, b1 = src_colors[:, 0], src_colors[:, 1], src_colors[:, 2]\n        L2, a2, b2 = dst_colors[:, 0], dst_colors[:, 1], dst_colors[:, 2]\n\n        # Delta E formula\n        delta_E = torch.sqrt((L2 - L1) ** 2 + (a2 - a1) ** 2 + (b2 - b1) ** 2)\n\n        # Normalize\n        delta_E = delta_E / 100.0  # Typical range of Delta E is 0-100\n\n        return delta_E.unsqueeze(1)\n    else:\n        # Use Euclidean distance for other color spaces\n        diff = torch.sqrt(torch.sum((dst_colors - src_colors) ** 2, dim=1))\n\n        # Normalize\n        max_diff = torch.max(diff)\n        if max_diff &gt; 0:\n            diff = diff / max_diff\n\n        return diff.unsqueeze(1)\n</code></pre>"},{"location":"imgraph/data/edge_features/geometric_features/","title":"Geometric Features","text":""},{"location":"imgraph/data/edge_features/geometric_features/#imgraphdataedge_featuresgeometric_features","title":"<code>imgraph.data.edge_features.geometric_features</code>","text":"<p>Functions for creating geometric edge features.</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.angle_features","title":"<code>angle_features(node_info, edge_index, normalize=True)</code>","text":"<p>Computes edge features based on the angle between nodes.</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.angle_features--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) normalize : bool, optional     Whether to normalize angles, by default True</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.angle_features--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, 2) containing [sin(\u03b8), cos(\u03b8)]</p> Source code in <code>imgraph/data/edge_features/geometric_features.py</code> <pre><code>def angle_features(node_info, edge_index, normalize=True):\n    \"\"\"\n    Computes edge features based on the angle between nodes.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    normalize : bool, optional\n        Whether to normalize angles, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, 2) containing [sin(\u03b8), cos(\u03b8)]\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Convert to tensor\n    centroids_tensor = torch.tensor(centroids, dtype=torch.float)\n\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, 2), dtype=torch.float)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Extract source and target node centroids\n    src_centroids = centroids_tensor[src]\n    dst_centroids = centroids_tensor[dst]\n\n    # Compute displacement vectors\n    delta = dst_centroids - src_centroids\n\n    # Compute angles\n    angles = torch.atan2(delta[:, 0], delta[:, 1])  # atan2(y, x)\n\n    # Convert to sin and cos for periodicity-invariant representation\n    sin_angles = torch.sin(angles)\n    cos_angles = torch.cos(angles)\n\n    # Combine sine and cosine\n    angle_features = torch.stack([sin_angles, cos_angles], dim=1)\n\n    return angle_features\n</code></pre>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.direction_encoding_features","title":"<code>direction_encoding_features(node_info, edge_index, num_directions=8)</code>","text":"<p>Encodes edge directions using a one-hot encoding.</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.direction_encoding_features--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) num_directions : int, optional     Number of direction bins, by default 8</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.direction_encoding_features--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, num_directions)</p> Source code in <code>imgraph/data/edge_features/geometric_features.py</code> <pre><code>def direction_encoding_features(node_info, edge_index, num_directions=8):\n    \"\"\"\n    Encodes edge directions using a one-hot encoding.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    num_directions : int, optional\n        Number of direction bins, by default 8\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, num_directions)\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Convert to tensor\n    centroids_tensor = torch.tensor(centroids, dtype=torch.float)\n\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, num_directions), dtype=torch.float)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Extract source and target node centroids\n    src_centroids = centroids_tensor[src]\n    dst_centroids = centroids_tensor[dst]\n\n    # Compute displacement vectors\n    delta = dst_centroids - src_centroids\n\n    # Compute angles in radians (range: [-\u03c0, \u03c0])\n    angles = torch.atan2(delta[:, 0], delta[:, 1])\n\n    # Convert to the range [0, 2\u03c0]\n    angles = (angles + 2*np.pi) % (2*np.pi)\n\n    # Determine direction bin for each edge\n    bin_width = 2*np.pi / num_directions\n    direction_bins = (angles / bin_width).long() % num_directions\n\n    # Create one-hot encoding\n    one_hot = torch.zeros((len(direction_bins), num_directions), dtype=torch.float)\n    one_hot.scatter_(1, direction_bins.unsqueeze(1), 1)\n\n    return one_hot\n</code></pre>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.distance_features","title":"<code>distance_features(node_info, edge_index, normalize=True)</code>","text":"<p>Computes edge features based on the distance between node centroids.</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.distance_features--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) normalize : bool, optional     Whether to normalize distances, by default True</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.distance_features--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, 1) containing distances</p> Source code in <code>imgraph/data/edge_features/geometric_features.py</code> <pre><code>def distance_features(node_info, edge_index, normalize=True):\n    \"\"\"\n    Computes edge features based on the distance between node centroids.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    normalize : bool, optional\n        Whether to normalize distances, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, 1) containing distances\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Convert to tensor\n    centroids_tensor = torch.tensor(centroids, dtype=torch.float)\n\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, 1), dtype=torch.float)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Extract source and target node centroids\n    src_centroids = centroids_tensor[src]\n    dst_centroids = centroids_tensor[dst]\n\n    # Compute Euclidean distances\n    distances = torch.sqrt(torch.sum((dst_centroids - src_centroids) ** 2, dim=1))\n\n    # Normalize if requested\n    if normalize:\n        max_dist = torch.max(distances)\n        if max_dist &gt; 0:\n            distances = distances / max_dist\n\n    # Reshape to (E, 1)\n    return distances.unsqueeze(1)\n</code></pre>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.relative_position_features","title":"<code>relative_position_features(node_info, edge_index, image=None)</code>","text":"<p>Computes edge features based on the relative position between nodes.</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.relative_position_features--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.relative_position_features--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, 2) containing normalized relative positions</p> Source code in <code>imgraph/data/edge_features/geometric_features.py</code> <pre><code>def relative_position_features(node_info, edge_index, image=None):\n    \"\"\"\n    Computes edge features based on the relative position between nodes.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, 2) containing normalized relative positions\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Convert to tensor\n    centroids_tensor = torch.tensor(centroids, dtype=torch.float)\n\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, 2), dtype=torch.float)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Extract source and target node centroids\n    src_centroids = centroids_tensor[src]\n    dst_centroids = centroids_tensor[dst]\n\n    # Compute relative positions\n    rel_pos = dst_centroids - src_centroids\n\n    # Normalize by image size if available\n    if image is not None:\n        height, width = image.shape[:2]\n        rel_pos[:, 0] = rel_pos[:, 0] / height  # y-coordinate\n        rel_pos[:, 1] = rel_pos[:, 1] / width   # x-coordinate\n    else:\n        # Normalize by maximum absolute value\n        max_abs = torch.max(torch.abs(rel_pos))\n        if max_abs &gt; 0:\n            rel_pos = rel_pos / max_abs\n\n    return rel_pos\n</code></pre>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.spatial_relationship_features","title":"<code>spatial_relationship_features(node_info, edge_index, image=None)</code>","text":"<p>Computes comprehensive edge features based on spatial relationships.</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.spatial_relationship_features--parameters","title":"Parameters","text":"<p>node_info : dict     Node information dictionary from node creation edge_index : torch.Tensor     Edge index tensor with shape (2, E) image : numpy.ndarray, optional     Input image, by default None</p>"},{"location":"imgraph/data/edge_features/geometric_features/#imgraph.data.edge_features.geometric_features.spatial_relationship_features--returns","title":"Returns","text":"<p>torch.Tensor     Edge feature tensor with shape (E, 5) containing:     [distance, sin(\u03b8), cos(\u03b8), \u0394y, \u0394x]</p> Source code in <code>imgraph/data/edge_features/geometric_features.py</code> <pre><code>def spatial_relationship_features(node_info, edge_index, image=None):\n    \"\"\"\n    Computes comprehensive edge features based on spatial relationships.\n\n    Parameters\n    ----------\n    node_info : dict\n        Node information dictionary from node creation\n    edge_index : torch.Tensor\n        Edge index tensor with shape (2, E)\n    image : numpy.ndarray, optional\n        Input image, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Edge feature tensor with shape (E, 5) containing:\n        [distance, sin(\u03b8), cos(\u03b8), \u0394y, \u0394x]\n    \"\"\"\n    if 'centroids' not in node_info:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    centroids = node_info['centroids']\n\n    # Convert to tensor\n    centroids_tensor = torch.tensor(centroids, dtype=torch.float)\n\n    # Handle empty edge index\n    if edge_index.shape[1] == 0:\n        return torch.zeros((0, 5), dtype=torch.float)\n\n    # Extract source and target node indices\n    src, dst = edge_index\n\n    # Extract source and target node centroids\n    src_centroids = centroids_tensor[src]\n    dst_centroids = centroids_tensor[dst]\n\n    # Compute displacement vectors\n    delta = dst_centroids - src_centroids\n\n    # Compute distances\n    distances = torch.sqrt(torch.sum(delta ** 2, dim=1))\n\n    # Normalize distances\n    max_dist = torch.max(distances)\n    if max_dist &gt; 0:\n        norm_distances = distances / max_dist\n    else:\n        norm_distances = distances\n\n    # Compute angles\n    angles = torch.atan2(delta[:, 0], delta[:, 1])  # atan2(y, x)\n\n    # Convert to sin and cos\n    sin_angles = torch.sin(angles)\n    cos_angles = torch.cos(angles)\n\n    # Normalize relative positions\n    if image is not None:\n        height, width = image.shape[:2]\n        norm_delta_y = delta[:, 0] / height\n        norm_delta_x = delta[:, 1] / width\n    else:\n        # Normalize by maximum absolute value\n        max_abs_y = torch.max(torch.abs(delta[:, 0]))\n        max_abs_x = torch.max(torch.abs(delta[:, 1]))\n\n        if max_abs_y &gt; 0:\n            norm_delta_y = delta[:, 0] / max_abs_y\n        else:\n            norm_delta_y = delta[:, 0]\n\n        if max_abs_x &gt; 0:\n            norm_delta_x = delta[:, 1] / max_abs_x\n        else:\n            norm_delta_x = delta[:, 1]\n\n    # Combine all features\n    features = torch.stack([\n        norm_distances,\n        sin_angles,\n        cos_angles,\n        norm_delta_y,\n        norm_delta_x\n    ], dim=1)\n\n    return features\n</code></pre>"},{"location":"imgraph/data/node_creation/patch_nodes/","title":"Patch Nodes","text":""},{"location":"imgraph/data/node_creation/patch_nodes/#imgraphdatanode_creationpatch_nodes","title":"<code>imgraph.data.node_creation.patch_nodes</code>","text":"<p>Functions for creating graph nodes based on regular image patches.</p>"},{"location":"imgraph/data/node_creation/patch_nodes/#imgraph.data.node_creation.patch_nodes.regular_patch_nodes","title":"<code>regular_patch_nodes(image, patch_size=16, stride=None)</code>","text":"<p>Creates graph nodes by dividing the image into regular patches.</p>"},{"location":"imgraph/data/node_creation/patch_nodes/#imgraph.data.node_creation.patch_nodes.regular_patch_nodes--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) patch_size : int or tuple, optional     Size of the patches to extract, by default 16.     If int, square patches of size (patch_size, patch_size) are extracted.     If tuple, patches of size (patch_size[0], patch_size[1]) are extracted. stride : int or tuple, optional     Stride between patches, by default None (equal to patch_size).     If int, stride of (stride, stride) is used.     If tuple, stride of (stride[0], stride[1]) is used.</p>"},{"location":"imgraph/data/node_creation/patch_nodes/#imgraph.data.node_creation.patch_nodes.regular_patch_nodes--returns","title":"Returns","text":"<p>dict     Dictionary containing node information:     - 'patches' : list of image patches     - 'centroids' : node centroid coordinates (N, 2)     - 'bboxes' : bounding boxes for each patch (N, 4) as (min_row, min_col, max_row, max_col)     - 'positions' : grid positions of each patch (N, 2) as (row_idx, col_idx)</p> Source code in <code>imgraph/data/node_creation/patch_nodes.py</code> <pre><code>def regular_patch_nodes(image, patch_size=16, stride=None):\n    \"\"\"\n    Creates graph nodes by dividing the image into regular patches.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    patch_size : int or tuple, optional\n        Size of the patches to extract, by default 16.\n        If int, square patches of size (patch_size, patch_size) are extracted.\n        If tuple, patches of size (patch_size[0], patch_size[1]) are extracted.\n    stride : int or tuple, optional\n        Stride between patches, by default None (equal to patch_size).\n        If int, stride of (stride, stride) is used.\n        If tuple, stride of (stride[0], stride[1]) is used.\n\n    Returns\n    -------\n    dict\n        Dictionary containing node information:\n        - 'patches' : list of image patches\n        - 'centroids' : node centroid coordinates (N, 2)\n        - 'bboxes' : bounding boxes for each patch (N, 4) as (min_row, min_col, max_row, max_col)\n        - 'positions' : grid positions of each patch (N, 2) as (row_idx, col_idx)\n    \"\"\"\n    # Handle different patch_size and stride formats\n    if isinstance(patch_size, int):\n        patch_size = (patch_size, patch_size)\n\n    if stride is None:\n        stride = patch_size\n    elif isinstance(stride, int):\n        stride = (stride, stride)\n\n    # Get image dimensions\n    height, width = image.shape[:2]\n\n    # Calculate number of patches in each dimension\n    n_patches_h = 1 + (height - patch_size[0]) // stride[0]\n    n_patches_w = 1 + (width - patch_size[1]) // stride[1]\n\n    # Initialize variables\n    patches = []\n    centroids = []\n    bboxes = []\n    positions = []\n\n    # Extract patches\n    for i in range(n_patches_h):\n        for j in range(n_patches_w):\n            # Calculate patch coordinates\n            top = i * stride[0]\n            left = j * stride[1]\n            bottom = min(top + patch_size[0], height)\n            right = min(left + patch_size[1], width)\n\n            # Extract patch\n            patch = image[top:bottom, left:right]\n\n            # Handle boundary patches (resize if needed)\n            if patch.shape[0] != patch_size[0] or patch.shape[1] != patch_size[1]:\n                # Skip incomplete patches option 1:\n                # continue\n\n                # Use partial patches option 2:\n                pass\n\n            # Calculate centroid\n            centroid = (top + (bottom - top) / 2, left + (right - left) / 2)\n\n            # Store patch information\n            patches.append(patch)\n            centroids.append(centroid)\n            bboxes.append((top, left, bottom, right))\n            positions.append((i, j))\n\n    return {\n        'patches': patches,\n        'centroids': np.array(centroids),\n        'bboxes': np.array(bboxes),\n        'positions': np.array(positions)\n    }\n</code></pre>"},{"location":"imgraph/data/node_creation/pixel_nodes/","title":"Pixel Nodes","text":""},{"location":"imgraph/data/node_creation/pixel_nodes/#imgraphdatanode_creationpixel_nodes","title":"<code>imgraph.data.node_creation.pixel_nodes</code>","text":"<p>Functions for creating graph nodes from individual pixels.</p>"},{"location":"imgraph/data/node_creation/pixel_nodes/#imgraph.data.node_creation.pixel_nodes.keypoint_nodes","title":"<code>keypoint_nodes(image, detector='sift', max_keypoints=100)</code>","text":"<p>Creates graph nodes from keypoints detected in the image.</p>"},{"location":"imgraph/data/node_creation/pixel_nodes/#imgraph.data.node_creation.pixel_nodes.keypoint_nodes--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) detector : str, optional     Keypoint detector to use ('sift', 'orb', 'fast'), by default 'sift' max_keypoints : int, optional     Maximum number of keypoints to return, by default 100</p>"},{"location":"imgraph/data/node_creation/pixel_nodes/#imgraph.data.node_creation.pixel_nodes.keypoint_nodes--returns","title":"Returns","text":"<p>dict     Dictionary containing node information:     - 'keypoints' : list of cv2.KeyPoint objects     - 'centroids' : node centroid coordinates (N, 2)     - 'descriptors' : keypoint descriptors (N, D)</p> Source code in <code>imgraph/data/node_creation/pixel_nodes.py</code> <pre><code>def keypoint_nodes(image, detector='sift', max_keypoints=100):\n    \"\"\"\n    Creates graph nodes from keypoints detected in the image.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    detector : str, optional\n        Keypoint detector to use ('sift', 'orb', 'fast'), by default 'sift'\n    max_keypoints : int, optional\n        Maximum number of keypoints to return, by default 100\n\n    Returns\n    -------\n    dict\n        Dictionary containing node information:\n        - 'keypoints' : list of cv2.KeyPoint objects\n        - 'centroids' : node centroid coordinates (N, 2)\n        - 'descriptors' : keypoint descriptors (N, D)\n    \"\"\"\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\"OpenCV (cv2) is required for keypoint detection.\")\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray = image\n\n    # Ensure image is in the right format\n    if gray.dtype != np.uint8:\n        gray = (gray * 255).astype(np.uint8)\n\n    # Initialize detector\n    if detector.lower() == 'sift':\n        det = cv2.SIFT_create(nfeatures=max_keypoints)\n    elif detector.lower() == 'orb':\n        det = cv2.ORB_create(nfeatures=max_keypoints)\n    elif detector.lower() == 'fast':\n        # FAST doesn't compute descriptors\n        fast = cv2.FastFeatureDetector_create()\n        keypoints = fast.detect(gray, None)\n        # Use BRIEF for descriptors\n        brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n        keypoints, descriptors = brief.compute(gray, keypoints)\n\n        # Limit to max_keypoints\n        if len(keypoints) &gt; max_keypoints:\n            keypoints = keypoints[:max_keypoints]\n            descriptors = descriptors[:max_keypoints]\n\n        # Extract centroids\n        centroids = np.array([(kp.pt[1], kp.pt[0]) for kp in keypoints])  # (y, x) format\n\n        return {\n            'keypoints': keypoints,\n            'centroids': centroids,\n            'descriptors': descriptors\n        }\n    else:\n        raise ValueError(f\"Unsupported detector: {detector}. Use 'sift', 'orb', or 'fast'.\")\n\n    # Detect keypoints and compute descriptors\n    keypoints, descriptors = det.detectAndCompute(gray, None)\n\n    # Limit to max_keypoints\n    if len(keypoints) &gt; max_keypoints:\n        keypoints = keypoints[:max_keypoints]\n        descriptors = descriptors[:max_keypoints]\n\n    # Extract centroids\n    centroids = np.array([(kp.pt[1], kp.pt[0]) for kp in keypoints])  # (y, x) format\n\n    return {\n        'keypoints': keypoints,\n        'centroids': centroids,\n        'descriptors': descriptors\n    }\n</code></pre>"},{"location":"imgraph/data/node_creation/pixel_nodes/#imgraph.data.node_creation.pixel_nodes.pixel_nodes","title":"<code>pixel_nodes(image, downsample_factor=1)</code>","text":"<p>Creates graph nodes from individual pixels, with optional downsampling.</p>"},{"location":"imgraph/data/node_creation/pixel_nodes/#imgraph.data.node_creation.pixel_nodes.pixel_nodes--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) downsample_factor : int, optional     Factor by which to downsample the image, by default 1 (no downsampling)</p>"},{"location":"imgraph/data/node_creation/pixel_nodes/#imgraph.data.node_creation.pixel_nodes.pixel_nodes--returns","title":"Returns","text":"<p>dict     Dictionary containing node information:     - 'pixels' : downsampled image     - 'centroids' : node centroid coordinates (N, 2)     - 'positions' : grid positions of each pixel (N, 2) as (row_idx, col_idx)     - 'values' : pixel values (N, C)</p> Source code in <code>imgraph/data/node_creation/pixel_nodes.py</code> <pre><code>def pixel_nodes(image, downsample_factor=1):\n    \"\"\"\n    Creates graph nodes from individual pixels, with optional downsampling.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    downsample_factor : int, optional\n        Factor by which to downsample the image, by default 1 (no downsampling)\n\n    Returns\n    -------\n    dict\n        Dictionary containing node information:\n        - 'pixels' : downsampled image\n        - 'centroids' : node centroid coordinates (N, 2)\n        - 'positions' : grid positions of each pixel (N, 2) as (row_idx, col_idx)\n        - 'values' : pixel values (N, C)\n    \"\"\"\n    # Downsample image if needed\n    if downsample_factor &gt; 1:\n        height, width = image.shape[:2]\n        new_height = height // downsample_factor\n        new_width = width // downsample_factor\n\n        # Simple downsampling by taking strided pixels\n        # For more sophisticated downsampling, consider using cv2.resize or skimage.transform.resize\n        downsampled = image[::downsample_factor, ::downsample_factor]\n    else:\n        downsampled = image\n\n    # Get dimensions of downsampled image\n    height, width = downsampled.shape[:2]\n\n    # Create pixel positions and centroids\n    positions = []\n    centroids = []\n    values = []\n\n    for i in range(height):\n        for j in range(width):\n            positions.append((i, j))\n            # Centroids are shifted by 0.5 to be at the center of the pixel\n            centroids.append((i + 0.5, j + 0.5))\n            values.append(downsampled[i, j])\n\n    return {\n        'pixels': downsampled,\n        'centroids': np.array(centroids),\n        'positions': np.array(positions),\n        'values': np.array(values)\n    }\n</code></pre>"},{"location":"imgraph/data/node_creation/superpixel_nodes/","title":"Superpixel Nodes","text":""},{"location":"imgraph/data/node_creation/superpixel_nodes/#imgraphdatanode_creationsuperpixel_nodes","title":"<code>imgraph.data.node_creation.superpixel_nodes</code>","text":"<p>Functions for creating graph nodes from superpixel segmentation.</p>"},{"location":"imgraph/data/node_creation/superpixel_nodes/#imgraph.data.node_creation.superpixel_nodes.felzenszwalb_superpixel_nodes","title":"<code>felzenszwalb_superpixel_nodes(image, scale=100, sigma=0.5, min_size=50)</code>","text":"<p>Creates graph nodes using Felzenszwalb's superpixel segmentation.</p>"},{"location":"imgraph/data/node_creation/superpixel_nodes/#imgraph.data.node_creation.superpixel_nodes.felzenszwalb_superpixel_nodes--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) scale : float, optional     Scale parameter for Felzenszwalb, by default 100 sigma : float, optional     Width of Gaussian smoothing kernel for pre-processing, by default 0.5 min_size : int, optional     Minimum component size, by default 50</p>"},{"location":"imgraph/data/node_creation/superpixel_nodes/#imgraph.data.node_creation.superpixel_nodes.felzenszwalb_superpixel_nodes--returns","title":"Returns","text":"<p>dict     Dictionary containing node information:     - 'segments' : superpixel segmentation map     - 'centroids' : node centroid coordinates (N, 2)     - 'masks' : binary masks for each superpixel     - 'bboxes' : bounding boxes for each superpixel (N, 4) as (min_row, min_col, max_row, max_col)</p> Source code in <code>imgraph/data/node_creation/superpixel_nodes.py</code> <pre><code>def felzenszwalb_superpixel_nodes(image, scale=100, sigma=0.5, min_size=50):\n    \"\"\"\n    Creates graph nodes using Felzenszwalb's superpixel segmentation.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    scale : float, optional\n        Scale parameter for Felzenszwalb, by default 100\n    sigma : float, optional\n        Width of Gaussian smoothing kernel for pre-processing, by default 0.5\n    min_size : int, optional\n        Minimum component size, by default 50\n\n    Returns\n    -------\n    dict\n        Dictionary containing node information:\n        - 'segments' : superpixel segmentation map\n        - 'centroids' : node centroid coordinates (N, 2)\n        - 'masks' : binary masks for each superpixel\n        - 'bboxes' : bounding boxes for each superpixel (N, 4) as (min_row, min_col, max_row, max_col)\n    \"\"\"\n    # Ensure image is in the right format\n    if image.dtype == np.uint8:\n        image = image.astype(np.float32) / 255.0\n\n    # Apply Felzenszwalb segmentation\n    segments = felzenszwalb(image, scale=scale, sigma=sigma, min_size=min_size)\n\n    # Extract region properties\n    regions = regionprops(segments + 1)  # Add 1 to avoid background=0 issues\n\n    # Extract centroid coordinates\n    centroids = np.array([region.centroid for region in regions])\n\n    # Create masks for each superpixel\n    unique_segments = np.unique(segments)\n    masks = []\n    bboxes = []\n\n    for i in unique_segments:\n        mask = segments == i\n        masks.append(mask)\n\n        # Get bounding box\n        props = regionprops(mask.astype(int))\n        if props:\n            minr, minc, maxr, maxc = props[0].bbox\n            bboxes.append((minr, minc, maxr, maxc))\n        else:\n            # Fallback if regionprops fails\n            rows, cols = np.where(mask)\n            if len(rows) &gt; 0 and len(cols) &gt; 0:\n                minr, maxr = np.min(rows), np.max(rows)\n                minc, maxc = np.min(cols), np.max(cols)\n                bboxes.append((minr, minc, maxr, maxc))\n            else:\n                bboxes.append((0, 0, 1, 1))  # Fallback bbox\n\n    return {\n        'segments': segments,\n        'centroids': centroids,\n        'masks': masks,\n        'bboxes': bboxes\n    }\n</code></pre>"},{"location":"imgraph/data/node_creation/superpixel_nodes/#imgraph.data.node_creation.superpixel_nodes.slic_superpixel_nodes","title":"<code>slic_superpixel_nodes(image, n_segments=100, compactness=10, sigma=0)</code>","text":"<p>Creates graph nodes using SLIC superpixel segmentation.</p>"},{"location":"imgraph/data/node_creation/superpixel_nodes/#imgraph.data.node_creation.superpixel_nodes.slic_superpixel_nodes--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) n_segments : int, optional     Number of segments to extract, by default 100 compactness : float, optional     Compactness parameter for SLIC, by default 10 sigma : float, optional     Width of Gaussian smoothing kernel for pre-processing, by default 0</p>"},{"location":"imgraph/data/node_creation/superpixel_nodes/#imgraph.data.node_creation.superpixel_nodes.slic_superpixel_nodes--returns","title":"Returns","text":"<p>dict     Dictionary containing node information:     - 'segments' : superpixel segmentation map     - 'centroids' : node centroid coordinates (N, 2)     - 'masks' : binary masks for each superpixel     - 'bboxes' : bounding boxes for each superpixel (N, 4) as (min_row, min_col, max_row, max_col)</p> Source code in <code>imgraph/data/node_creation/superpixel_nodes.py</code> <pre><code>def slic_superpixel_nodes(image, n_segments=100, compactness=10, sigma=0):\n    \"\"\"\n    Creates graph nodes using SLIC superpixel segmentation.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    n_segments : int, optional\n        Number of segments to extract, by default 100\n    compactness : float, optional\n        Compactness parameter for SLIC, by default 10\n    sigma : float, optional\n        Width of Gaussian smoothing kernel for pre-processing, by default 0\n\n    Returns\n    -------\n    dict\n        Dictionary containing node information:\n        - 'segments' : superpixel segmentation map\n        - 'centroids' : node centroid coordinates (N, 2)\n        - 'masks' : binary masks for each superpixel\n        - 'bboxes' : bounding boxes for each superpixel (N, 4) as (min_row, min_col, max_row, max_col)\n    \"\"\"\n    # Ensure image is in the right format\n    if image.dtype == np.uint8:\n        image = image.astype(np.float32) / 255.0\n\n    # Apply SLIC segmentation\n    segments = slic(image, n_segments=n_segments, compactness=compactness, \n                    sigma=sigma, start_label=0)\n\n    # Extract region properties\n    regions = regionprops(segments + 1)  # Add 1 to avoid background=0 issues\n\n    # Extract centroid coordinates\n    centroids = np.array([region.centroid for region in regions])\n\n    # Create masks for each superpixel\n    unique_segments = np.unique(segments)\n    masks = []\n    bboxes = []\n\n    for i in unique_segments:\n        mask = segments == i\n        masks.append(mask)\n\n        # Get bounding box\n        props = regionprops(mask.astype(int))\n        if props:\n            minr, minc, maxr, maxc = props[0].bbox\n            bboxes.append((minr, minc, maxr, maxc))\n        else:\n            # Fallback if regionprops fails\n            rows, cols = np.where(mask)\n            if len(rows) &gt; 0 and len(cols) &gt; 0:\n                minr, maxr = np.min(rows), np.max(rows)\n                minc, maxc = np.min(cols), np.max(cols)\n                bboxes.append((minr, minc, maxr, maxc))\n            else:\n                bboxes.append((0, 0, 1, 1))  # Fallback bbox\n\n    return {\n        'segments': segments,\n        'centroids': centroids,\n        'masks': masks,\n        'bboxes': bboxes\n    }\n</code></pre>"},{"location":"imgraph/data/node_features/cnn_features/","title":"CNN Features","text":""},{"location":"imgraph/data/node_features/cnn_features/#imgraphdatanode_featurescnn_features","title":"<code>imgraph.data.node_features.cnn_features</code>","text":"<p>Functions for extracting CNN features from image regions.</p>"},{"location":"imgraph/data/node_features/cnn_features/#imgraph.data.node_features.cnn_features.pretrained_cnn_features","title":"<code>pretrained_cnn_features(image, node_info, model_name='resnet18', layer=None, transform=None)</code>","text":"<p>Extracts features from a pretrained CNN model.</p>"},{"location":"imgraph/data/node_features/cnn_features/#imgraph.data.node_features.cnn_features.pretrained_cnn_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation model_name : str, optional     Name of the pretrained model to use, by default 'resnet18' layer : str, optional     Name of the layer to extract features from, by default None (use final feature layer) transform : callable, optional     Transformation to apply to image patches, by default None</p>"},{"location":"imgraph/data/node_features/cnn_features/#imgraph.data.node_features.cnn_features.pretrained_cnn_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, F) where F depends on the model and layer</p> Source code in <code>imgraph/data/node_features/cnn_features.py</code> <pre><code>def pretrained_cnn_features(image, node_info, model_name='resnet18', layer=None, transform=None):\n    \"\"\"\n    Extracts features from a pretrained CNN model.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    model_name : str, optional\n        Name of the pretrained model to use, by default 'resnet18'\n    layer : str, optional\n        Name of the layer to extract features from, by default None (use final feature layer)\n    transform : callable, optional\n        Transformation to apply to image patches, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, F) where F depends on the model and layer\n    \"\"\"\n    try:\n        import torchvision.models as models\n        from torchvision import transforms\n        import cv2\n    except ImportError:\n        raise ImportError(\"torchvision and opencv-python are required for CNN feature extraction.\")\n\n    # Check if CUDA is available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Load pretrained model\n    if model_name == 'resnet18':\n        model = models.resnet18(pretrained=True)\n        if layer is None:\n            layer = 'avgpool'\n    elif model_name == 'resnet50':\n        model = models.resnet50(pretrained=True)\n        if layer is None:\n            layer = 'avgpool'\n    elif model_name == 'vgg16':\n        model = models.vgg16(pretrained=True)\n        if layer is None:\n            layer = 'features.29'\n    elif model_name == 'mobilenet_v2':\n        model = models.mobilenet_v2(pretrained=True)\n        if layer is None:\n            layer = 'features.18'\n    else:\n        raise ValueError(f\"Unsupported model: {model_name}\")\n\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Setup feature extraction\n    features = []\n    activation = {}\n\n    def get_activation(name):\n        def hook(model, input, output):\n            activation[name] = output.detach()\n        return hook\n\n    # Register hook to extract features\n    try:\n        if '.' in layer:\n            # For nested modules like 'features.29' in VGG\n            main_module, sub_module = layer.split('.', 1)\n            sub_module = int(sub_module) if sub_module.isdigit() else sub_module\n            getattr(getattr(model, main_module), sub_module).register_forward_hook(get_activation(layer))\n        else:\n            # For direct modules like 'avgpool' in ResNet\n            getattr(model, layer).register_forward_hook(get_activation(layer))\n    except Exception as e:\n        warnings.warn(f\"Error registering hook for layer {layer}: {e}. Using final layer.\")\n        # Use the final feature layer as fallback\n        if hasattr(model, 'avgpool'):\n            model.avgpool.register_forward_hook(get_activation('avgpool'))\n        else:\n            model.features[-1].register_forward_hook(get_activation('features'))\n\n    # Define default transform if none provided\n    if transform is None:\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    # Extract features based on node type\n    with torch.no_grad():\n        if 'segments' in node_info:  # Superpixel nodes\n            masks = node_info['masks']\n            bboxes = node_info['bboxes']\n\n            for mask, bbox in zip(masks, bboxes):\n                if isinstance(bbox, tuple):\n                    top, left, bottom, right = bbox\n                else:\n                    top, left, bottom, right = bbox\n\n                # Extract patch\n                patch = image[top:bottom, left:right].copy()\n\n                # Skip empty patches\n                if patch.size == 0 or patch.shape[0] == 0 or patch.shape[1] == 0:\n                    # Use zeros as features\n                    if len(features) &gt; 0:\n                        features.append(np.zeros_like(features[0]))\n                    else:\n                        # Determine feature size by running a dummy forward pass\n                        dummy = np.zeros((224, 224, 3), dtype=np.uint8)\n                        dummy_tensor = transform(dummy).unsqueeze(0).to(device)\n                        model(dummy_tensor)\n                        feat_size = activation[list(activation.keys())[0]].cpu().numpy().flatten().shape[0]\n                        features.append(np.zeros(feat_size))\n                    continue\n\n                # Apply mask within the bbox\n                mask_cropped = mask[top:bottom, left:right]\n\n                # Create masked patch\n                patch_masked = patch.copy()\n                if len(patch.shape) == 3:\n                    for c in range(patch.shape[2]):\n                        patch_masked[:, :, c] = patch[:, :, c] * mask_cropped\n                else:\n                    patch_masked = patch * mask_cropped\n\n                # Resize to model's expected input size\n                patch_resized = cv2.resize(patch_masked, (224, 224))\n\n                # Convert to tensor and normalize\n                patch_tensor = transform(patch_resized).unsqueeze(0).to(device)\n\n                # Forward pass\n                model(patch_tensor)\n\n                # Get features\n                key = list(activation.keys())[0]\n                feat = activation[key].cpu().numpy().flatten()\n                features.append(feat)\n\n        elif 'patches' in node_info:  # Patch nodes\n            patches = node_info['patches']\n\n            for patch in patches:\n                # Skip empty patches\n                if patch.size == 0 or patch.shape[0] == 0 or patch.shape[1] == 0:\n                    # Use zeros as features\n                    if len(features) &gt; 0:\n                        features.append(np.zeros_like(features[0]))\n                    else:\n                        # Determine feature size by running a dummy forward pass\n                        dummy = np.zeros((224, 224, 3), dtype=np.uint8)\n                        dummy_tensor = transform(dummy).unsqueeze(0).to(device)\n                        model(dummy_tensor)\n                        feat_size = activation[list(activation.keys())[0]].cpu().numpy().flatten().shape[0]\n                        features.append(np.zeros(feat_size))\n                    continue\n\n                # Resize to model's expected input size\n                patch_resized = cv2.resize(patch, (224, 224))\n\n                # Convert to tensor and normalize\n                patch_tensor = transform(patch_resized).unsqueeze(0).to(device)\n\n                # Forward pass\n                model(patch_tensor)\n\n                # Get features\n                key = list(activation.keys())[0]\n                feat = activation[key].cpu().numpy().flatten()\n                features.append(feat)\n\n        elif 'values' in node_info:  # Pixel nodes\n            # For pixel nodes, we can't extract meaningful CNN features\n            # Just return zeros or compute features from small patches around each pixel\n            positions = node_info['positions']\n\n            # Determine feature size by running a dummy forward pass\n            dummy = np.zeros((224, 224, 3), dtype=np.uint8)\n            dummy_tensor = transform(dummy).unsqueeze(0).to(device)\n            model(dummy_tensor)\n            feat_size = activation[list(activation.keys())[0]].cpu().numpy().flatten().shape[0]\n\n            features = np.zeros((len(positions), feat_size))\n\n    # Convert to tensor\n    return torch.tensor(np.array(features), dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/color_features/","title":"Color Features","text":""},{"location":"imgraph/data/node_features/color_features/#imgraphdatanode_featurescolor_features","title":"<code>imgraph.data.node_features.color_features</code>","text":"<p>Functions for extracting color features from image regions.</p>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.histogram_color_features","title":"<code>histogram_color_features(image, node_info, bins=8, color_space='rgb', normalize=True)</code>","text":"<p>Extracts color histogram features from image regions.</p>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.histogram_color_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation bins : int, optional     Number of bins per channel, by default 8 color_space : str, optional     Color space to use for feature extraction, by default 'rgb'.     Options: 'rgb', 'hsv', 'lab', 'ycrcb' normalize : bool, optional     Whether to normalize histograms, by default True</p>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.histogram_color_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, bins*C) where C is the number of color channels</p> Source code in <code>imgraph/data/node_features/color_features.py</code> <pre><code>def histogram_color_features(image, node_info, bins=8, color_space='rgb', normalize=True):\n    \"\"\"\n    Extracts color histogram features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    bins : int, optional\n        Number of bins per channel, by default 8\n    color_space : str, optional\n        Color space to use for feature extraction, by default 'rgb'.\n        Options: 'rgb', 'hsv', 'lab', 'ycrcb'\n    normalize : bool, optional\n        Whether to normalize histograms, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, bins*C) where C is the number of color channels\n    \"\"\"\n    # Convert color space if necessary\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\"OpenCV (cv2) is required for color space conversion.\")\n\n    if color_space.lower() != 'rgb':\n        if color_space.lower() == 'hsv':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif color_space.lower() == 'lab':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n        elif color_space.lower() == 'ycrcb':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n        else:\n            raise ValueError(f\"Unsupported color space: {color_space}\")\n    else:\n        img_converted = image.copy()\n\n    # Define histogram bins\n    num_channels = img_converted.shape[-1]\n    channels = list(range(num_channels))\n    hist_bins = [bins] * num_channels\n    hist_ranges = [(0, 256)] * num_channels if img_converted.dtype == np.uint8 else [(0, 1)] * num_channels\n\n    # Extract features based on node type\n    features = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n\n            # Create masked image for histogram calculation\n            masked_img = np.zeros_like(img_converted)\n            masked_img[mask] = img_converted[mask]\n\n            # Calculate histogram for each channel\n            region_hist = []\n            for c in range(num_channels):\n                hist, _ = np.histogram(\n                    img_converted[mask, c].flatten(), \n                    bins=bins, \n                    range=hist_ranges[c], \n                    density=normalize\n                )\n                region_hist.extend(hist)\n\n            features.append(region_hist)\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n\n        for patch in patches:\n            # Calculate histogram for each channel\n            patch_hist = []\n            for c in range(num_channels):\n                hist, _ = np.histogram(\n                    patch[:, :, c].flatten(), \n                    bins=bins, \n                    range=hist_ranges[c], \n                    density=normalize\n                )\n                patch_hist.extend(hist)\n\n            features.append(patch_hist)\n\n    elif 'values' in node_info:  # Pixel nodes\n        # For pixel nodes, we can't compute histograms\n        # Instead, we return one-hot encoded bins\n        values = node_info['values']\n\n        for value in values:\n            pixel_hist = []\n            for c in range(num_channels):\n                bin_idx = min(int(value[c] * bins) if img_converted.dtype != np.uint8 else int(value[c] / 256 * bins), bins - 1)\n                one_hot = np.zeros(bins)\n                one_hot[bin_idx] = 1\n                pixel_hist.extend(one_hot)\n\n            features.append(pixel_hist)\n\n    # Convert to tensor\n    return torch.tensor(np.array(features), dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.mean_color_features","title":"<code>mean_color_features(image, node_info, color_space='rgb')</code>","text":"<p>Extracts mean color features from image regions.</p>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.mean_color_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation color_space : str, optional     Color space to use for feature extraction, by default 'rgb'.     Options: 'rgb', 'hsv', 'lab', 'ycrcb'</p>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.mean_color_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, C) where C is the number of color channels</p> Source code in <code>imgraph/data/node_features/color_features.py</code> <pre><code>def mean_color_features(image, node_info, color_space='rgb'):\n    \"\"\"\n    Extracts mean color features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    color_space : str, optional\n        Color space to use for feature extraction, by default 'rgb'.\n        Options: 'rgb', 'hsv', 'lab', 'ycrcb'\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, C) where C is the number of color channels\n    \"\"\"\n    # Convert color space if necessary\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\"OpenCV (cv2) is required for color space conversion.\")\n\n    if color_space.lower() != 'rgb':\n        if color_space.lower() == 'hsv':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif color_space.lower() == 'lab':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n        elif color_space.lower() == 'ycrcb':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n        else:\n            raise ValueError(f\"Unsupported color space: {color_space}\")\n    else:\n        img_converted = image.copy()\n\n    # Extract features based on node type\n    features = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n            region = img_converted[mask]\n\n            if len(region) &gt; 0:\n                mean_values = np.mean(region, axis=0)\n            else:\n                mean_values = np.zeros(img_converted.shape[-1])\n\n            features.append(mean_values)\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n\n        for patch in patches:\n            mean_values = np.mean(patch, axis=(0, 1))\n            features.append(mean_values)\n\n    elif 'values' in node_info:  # Pixel nodes\n        # For pixel nodes, values are already stored\n        features = node_info['values']\n\n    # Convert to tensor\n    return torch.tensor(np.array(features), dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.mean_std_color_features","title":"<code>mean_std_color_features(image, node_info, color_space='rgb', normalize=True)</code>","text":"<p>Extracts mean and standard deviation color features from image regions.</p>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.mean_std_color_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation color_space : str, optional     Color space to use for feature extraction, by default 'rgb'.     Options: 'rgb', 'hsv', 'lab', 'ycrcb' normalize : bool, optional     Whether to normalize features, by default True</p>"},{"location":"imgraph/data/node_features/color_features/#imgraph.data.node_features.color_features.mean_std_color_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, 2*C) where C is the number of color channels</p> Source code in <code>imgraph/data/node_features/color_features.py</code> <pre><code>def mean_std_color_features(image, node_info, color_space='rgb', normalize=True):\n    \"\"\"\n    Extracts mean and standard deviation color features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    color_space : str, optional\n        Color space to use for feature extraction, by default 'rgb'.\n        Options: 'rgb', 'hsv', 'lab', 'ycrcb'\n    normalize : bool, optional\n        Whether to normalize features, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, 2*C) where C is the number of color channels\n    \"\"\"\n    # Convert color space if necessary\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\"OpenCV (cv2) is required for color space conversion.\")\n\n    if color_space.lower() != 'rgb':\n        if color_space.lower() == 'hsv':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif color_space.lower() == 'lab':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n        elif color_space.lower() == 'ycrcb':\n            img_converted = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n        else:\n            raise ValueError(f\"Unsupported color space: {color_space}\")\n    else:\n        img_converted = image.copy()\n\n    # Extract features based on node type\n    features = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n            region = img_converted[mask]\n\n            if len(region) &gt; 0:\n                mean_values = np.mean(region, axis=0)\n                std_values = np.std(region, axis=0)\n            else:\n                mean_values = np.zeros(img_converted.shape[-1])\n                std_values = np.zeros(img_converted.shape[-1])\n\n            # Concatenate mean and std\n            features.append(np.concatenate([mean_values, std_values]))\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n\n        for patch in patches:\n            mean_values = np.mean(patch, axis=(0, 1))\n            std_values = np.std(patch, axis=(0, 1))\n\n            # Concatenate mean and std\n            features.append(np.concatenate([mean_values, std_values]))\n\n    elif 'values' in node_info:  # Pixel nodes\n        # For pixel nodes, we can't compute std, so we just duplicate the values\n        values = node_info['values']\n\n        for value in values:\n            features.append(np.concatenate([value, np.zeros_like(value)]))\n\n    # Convert to numpy array\n    features = np.array(features)\n\n    # Normalize if requested\n    if normalize:\n        scaler = StandardScaler()\n        features = scaler.fit_transform(features)\n\n    # Convert to tensor\n    return torch.tensor(features, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/position_features/","title":"Position Features","text":""},{"location":"imgraph/data/node_features/position_features/#imgraphdatanode_featuresposition_features","title":"<code>imgraph.data.node_features.position_features</code>","text":"<p>Functions for extracting positional features from image regions.</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.grid_position_features","title":"<code>grid_position_features(image, node_info, grid_size=(4, 4))</code>","text":"<p>Extracts grid-based position features using one-hot encoding.</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.grid_position_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation grid_size : tuple, optional     Size of the grid (rows, cols), by default (4, 4)</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.grid_position_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, grid_size[0] * grid_size[1])</p> Source code in <code>imgraph/data/node_features/position_features.py</code> <pre><code>def grid_position_features(image, node_info, grid_size=(4, 4)):\n    \"\"\"\n    Extracts grid-based position features using one-hot encoding.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    grid_size : tuple, optional\n        Size of the grid (rows, cols), by default (4, 4)\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, grid_size[0] * grid_size[1])\n    \"\"\"\n    # Extract centroids\n    if 'centroids' in node_info:\n        centroids = node_info['centroids']\n    else:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    # Get image dimensions\n    height, width = image.shape[:2]\n\n    # Calculate grid cell size\n    cell_height = height / grid_size[0]\n    cell_width = width / grid_size[1]\n\n    # Initialize features\n    features = np.zeros((centroids.shape[0], grid_size[0] * grid_size[1]), dtype=np.float32)\n\n    # Assign nodes to grid cells\n    for i, (y, x) in enumerate(centroids):\n        # Calculate grid indices\n        grid_y = min(int(y / cell_height), grid_size[0] - 1)\n        grid_x = min(int(x / cell_width), grid_size[1] - 1)\n\n        # Calculate linear index\n        linear_idx = grid_y * grid_size[1] + grid_x\n\n        # Set the corresponding feature to 1\n        features[i, linear_idx] = 1.0\n\n    # Convert to tensor\n    return torch.tensor(features, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.normalized_position_features","title":"<code>normalized_position_features(image, node_info, include_distance=True)</code>","text":"<p>Extracts normalized positional features from image regions.</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.normalized_position_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation include_distance : bool, optional     Whether to include distance to center, by default True</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.normalized_position_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, 2) or (N, 3) if include_distance is True</p> Source code in <code>imgraph/data/node_features/position_features.py</code> <pre><code>def normalized_position_features(image, node_info, include_distance=True):\n    \"\"\"\n    Extracts normalized positional features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    include_distance : bool, optional\n        Whether to include distance to center, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, 2) or (N, 3) if include_distance is True\n    \"\"\"\n    # Extract centroids\n    if 'centroids' in node_info:\n        centroids = node_info['centroids']\n    else:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    # Get image dimensions\n    height, width = image.shape[:2]\n\n    # Normalize coordinates\n    normalized = np.zeros_like(centroids, dtype=np.float32)\n    normalized[:, 0] = centroids[:, 0] / height  # y-coordinate\n    normalized[:, 1] = centroids[:, 1] / width   # x-coordinate\n\n    # Include distance to center if requested\n    if include_distance:\n        # Calculate image center\n        center_y, center_x = height / 2, width / 2\n\n        # Calculate normalized distances\n        distances = np.sqrt(\n            ((centroids[:, 0] - center_y) / height) ** 2 + \n            ((centroids[:, 1] - center_x) / width) ** 2\n        )\n\n        # Normalize by max possible distance (corner to center)\n        max_distance = np.sqrt(0.5)\n        distances = distances / max_distance\n\n        # Add distances as a third feature\n        normalized = np.column_stack((normalized, distances))\n\n    # Convert to tensor\n    return torch.tensor(normalized, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.polar_position_features","title":"<code>polar_position_features(image, node_info)</code>","text":"<p>Extracts polar position features from image regions.</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.polar_position_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.polar_position_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, 2) containing (r, theta) coordinates</p> Source code in <code>imgraph/data/node_features/position_features.py</code> <pre><code>def polar_position_features(image, node_info):\n    \"\"\"\n    Extracts polar position features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, 2) containing (r, theta) coordinates\n    \"\"\"\n    # Extract centroids\n    if 'centroids' in node_info:\n        centroids = node_info['centroids']\n    else:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    # Get image dimensions\n    height, width = image.shape[:2]\n\n    # Calculate image center\n    center_y, center_x = height / 2, width / 2\n\n    # Calculate polar coordinates\n    polar = np.zeros((centroids.shape[0], 2), dtype=np.float32)\n\n    # Calculate normalized distances (r)\n    polar[:, 0] = np.sqrt(\n        ((centroids[:, 0] - center_y) / height) ** 2 + \n        ((centroids[:, 1] - center_x) / width) ** 2\n    )\n\n    # Normalize by max possible distance (corner to center)\n    max_distance = np.sqrt(0.5)\n    polar[:, 0] = polar[:, 0] / max_distance\n\n    # Calculate angles (theta)\n    polar[:, 1] = np.arctan2(\n        centroids[:, 0] - center_y,\n        centroids[:, 1] - center_x\n    )\n\n    # Normalize angles to [0, 1]\n    polar[:, 1] = (polar[:, 1] + np.pi) / (2 * np.pi)\n\n    # Convert to tensor\n    return torch.tensor(polar, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.position_features","title":"<code>position_features(image, node_info)</code>","text":"<p>Extracts positional features from image regions.</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.position_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation</p>"},{"location":"imgraph/data/node_features/position_features/#imgraph.data.node_features.position_features.position_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, 2) containing (y, x) coordinates</p> Source code in <code>imgraph/data/node_features/position_features.py</code> <pre><code>def position_features(image, node_info):\n    \"\"\"\n    Extracts positional features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, 2) containing (y, x) coordinates\n    \"\"\"\n    # Extract centroids\n    if 'centroids' in node_info:\n        centroids = node_info['centroids']\n    else:\n        raise ValueError(\"Node info does not contain centroids\")\n\n    # Convert to tensor\n    return torch.tensor(centroids, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/texture_features/","title":"Texture Features","text":""},{"location":"imgraph/data/node_features/texture_features/#imgraphdatanode_featurestexture_features","title":"<code>imgraph.data.node_features.texture_features</code>","text":"<p>Functions for extracting texture features from image regions.</p>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.gabor_features","title":"<code>gabor_features(image, node_info, frequencies=[0.1, 0.25, 0.5], orientations=[0, np.pi / 4, np.pi / 2, 3 * np.pi / 4], normalize=True)</code>","text":"<p>Extracts Gabor filter features from image regions.</p>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.gabor_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation frequencies : list, optional     List of frequencies for Gabor filters, by default [0.1, 0.25, 0.5] orientations : list, optional     List of orientations for Gabor filters, by default [0, np.pi/4, np.pi/2, 3*np.pi/4] normalize : bool, optional     Whether to normalize features, by default True</p>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.gabor_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, len(frequencies)len(orientations)2)</p> Source code in <code>imgraph/data/node_features/texture_features.py</code> <pre><code>def gabor_features(image, node_info, frequencies=[0.1, 0.25, 0.5], orientations=[0, np.pi/4, np.pi/2, 3*np.pi/4], normalize=True):\n    \"\"\"\n    Extracts Gabor filter features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    frequencies : list, optional\n        List of frequencies for Gabor filters, by default [0.1, 0.25, 0.5]\n    orientations : list, optional\n        List of orientations for Gabor filters, by default [0, np.pi/4, np.pi/2, 3*np.pi/4]\n    normalize : bool, optional\n        Whether to normalize features, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, len(frequencies)*len(orientations)*2)\n    \"\"\"\n    try:\n        from skimage.filters import gabor_kernel\n        from scipy import ndimage as ndi\n    except ImportError:\n        raise ImportError(\"scikit-image and scipy are required for Gabor feature extraction.\")\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        try:\n            import cv2\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        except ImportError:\n            # Simple grayscale conversion as fallback\n            gray = np.mean(image, axis=2)\n    else:\n        gray = image\n\n    # Ensure image is in float format\n    if gray.dtype == np.uint8:\n        gray = gray.astype(np.float32) / 255.0\n\n    # Generate Gabor kernels\n    kernels = []\n    for frequency in frequencies:\n        for orientation in orientations:\n            kernel = gabor_kernel(frequency, theta=orientation)\n            kernels.append(kernel)\n\n    # Apply Gabor filters to full image\n    gabor_responses = []\n    for kernel in kernels:\n        # Apply filter\n        filtered_real = ndi.convolve(gray, np.real(kernel), mode='wrap')\n        filtered_imag = ndi.convolve(gray, np.imag(kernel), mode='wrap')\n\n        # Compute magnitude\n        magnitude = np.sqrt(filtered_real**2 + filtered_imag**2)\n\n        gabor_responses.append(magnitude)\n\n    # Extract features based on node type\n    features = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n\n            # Compute mean and std of Gabor responses for each kernel\n            feature_vector = []\n            for response in gabor_responses:\n                region_response = response[mask]\n\n                if len(region_response) &gt; 0:\n                    mean = np.mean(region_response)\n                    std = np.std(region_response)\n                else:\n                    mean = 0\n                    std = 0\n\n                feature_vector.extend([mean, std])\n\n            features.append(feature_vector)\n\n    elif 'patches' in node_info:  # Patch nodes\n        bboxes = node_info['bboxes']\n\n        for bbox in bboxes:\n            top, left, bottom, right = bbox\n\n            # Compute mean and std of Gabor responses for each kernel\n            feature_vector = []\n            for response in gabor_responses:\n                patch_response = response[top:bottom, left:right]\n\n                if patch_response.size &gt; 0:\n                    mean = np.mean(patch_response)\n                    std = np.std(patch_response)\n                else:\n                    mean = 0\n                    std = 0\n\n                feature_vector.extend([mean, std])\n\n            features.append(feature_vector)\n\n    elif 'values' in node_info:  # Pixel nodes\n        positions = node_info['positions']\n\n        for pos in positions:\n            i, j = pos\n\n            # Sample Gabor responses at pixel position\n            feature_vector = []\n            for response in gabor_responses:\n                value = response[i, j]\n                # For pixel nodes, we don't have std\n                feature_vector.extend([value, 0])\n\n            features.append(feature_vector)\n\n    # Convert to numpy array\n    features = np.array(features)\n\n    # Normalize if requested\n    if normalize:\n        scaler = StandardScaler()\n        features = scaler.fit_transform(features)\n\n    # Convert to tensor\n    return torch.tensor(features, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.glcm_features","title":"<code>glcm_features(image, node_info, distances=[1], angles=[0, np.pi / 4, np.pi / 2, 3 * np.pi / 4], normalize=True)</code>","text":"<p>Extracts Gray-Level Co-occurrence Matrix (GLCM) features from image regions.</p>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.glcm_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation distances : list, optional     List of distances for GLCM, by default [1] angles : list, optional     List of angles for GLCM, by default [0, np.pi/4, np.pi/2, 3*np.pi/4] normalize : bool, optional     Whether to normalize features, by default True</p>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.glcm_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, F) where F depends on the properties computed</p> Source code in <code>imgraph/data/node_features/texture_features.py</code> <pre><code>def glcm_features(image, node_info, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], normalize=True):\n    \"\"\"\n    Extracts Gray-Level Co-occurrence Matrix (GLCM) features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    distances : list, optional\n        List of distances for GLCM, by default [1]\n    angles : list, optional\n        List of angles for GLCM, by default [0, np.pi/4, np.pi/2, 3*np.pi/4]\n    normalize : bool, optional\n        Whether to normalize features, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, F) where F depends on the properties computed\n    \"\"\"\n    try:\n        from skimage.feature import graycomatrix, graycoprops\n    except ImportError:\n        raise ImportError(\"scikit-image is required for GLCM feature extraction.\")\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        try:\n            import cv2\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        except ImportError:\n            # Simple grayscale conversion as fallback\n            gray = np.mean(image, axis=2)\n    else:\n        gray = image\n\n    # Ensure image is in uint8 format for GLCM\n    if gray.dtype != np.uint8:\n        gray = (gray * 255).astype(np.uint8)\n\n    # Properties to compute\n    properties = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n\n    # Extract features based on node type\n    features = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n\n            # Get bounding box of segment for efficiency\n            rows, cols = np.where(mask)\n            if len(rows) == 0 or len(cols) == 0:\n                # Empty segment, add zeros\n                feature_vector = np.zeros(len(properties) * len(distances) * len(angles))\n                features.append(feature_vector)\n                continue\n\n            rmin, rmax = np.min(rows), np.max(rows)\n            cmin, cmax = np.min(cols), np.max(cols)\n\n            # Create binary mask for the region\n            region_mask = mask[rmin:rmax+1, cmin:cmax+1]\n\n            # Extract the region\n            region = gray[rmin:rmax+1, cmin:cmax+1].copy()\n\n            # Set pixels outside the region to 0\n            region[~region_mask] = 0\n\n            # Skip tiny regions\n            if region.shape[0] &lt; 2 or region.shape[1] &lt; 2:\n                feature_vector = np.zeros(len(properties) * len(distances) * len(angles))\n                features.append(feature_vector)\n                continue\n\n            # Compute GLCM\n            try:\n                glcm = graycomatrix(region, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n\n                # Compute properties\n                feature_vector = []\n                for prop in properties:\n                    feature_vector.extend(graycoprops(glcm, prop).flatten())\n\n                features.append(feature_vector)\n            except:\n                # Fallback if GLCM computation fails\n                feature_vector = np.zeros(len(properties) * len(distances) * len(angles))\n                features.append(feature_vector)\n\n    elif 'patches' in node_info:  # Patch nodes\n        bboxes = node_info['bboxes']\n\n        for bbox in bboxes:\n            top, left, bottom, right = bbox\n            patch = gray[top:bottom, left:right]\n\n            # Skip tiny patches\n            if patch.shape[0] &lt; 2 or patch.shape[1] &lt; 2:\n                feature_vector = np.zeros(len(properties) * len(distances) * len(angles))\n                features.append(feature_vector)\n                continue\n\n            # Compute GLCM\n            try:\n                glcm = graycomatrix(patch, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n\n                # Compute properties\n                feature_vector = []\n                for prop in properties:\n                    feature_vector.extend(graycoprops(glcm, prop).flatten())\n\n                features.append(feature_vector)\n            except:\n                # Fallback if GLCM computation fails\n                feature_vector = np.zeros(len(properties) * len(distances) * len(angles))\n                features.append(feature_vector)\n\n    elif 'values' in node_info:  # Pixel nodes\n        # For pixel nodes, we can't compute texture features\n        # Just return zeros\n        positions = node_info['positions']\n        features = np.zeros((len(positions), len(properties) * len(distances) * len(angles)))\n\n    # Convert to numpy array\n    features = np.array(features)\n\n    # Normalize if requested\n    if normalize:\n        scaler = StandardScaler()\n        features = scaler.fit_transform(features)\n\n    # Convert to tensor\n    return torch.tensor(features, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.lbp_features","title":"<code>lbp_features(image, node_info, radius=1, n_points=8, normalize=True)</code>","text":"<p>Extracts Local Binary Pattern (LBP) features from image regions.</p>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.lbp_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C) node_info : dict     Node information dictionary from node creation radius : int, optional     Radius of the LBP circle, by default 1 n_points : int, optional     Number of points around the circle, by default 8 normalize : bool, optional     Whether to normalize histograms, by default True</p>"},{"location":"imgraph/data/node_features/texture_features/#imgraph.data.node_features.texture_features.lbp_features--returns","title":"Returns","text":"<p>torch.Tensor     Node features tensor with shape (N, 2^n_points)</p> Source code in <code>imgraph/data/node_features/texture_features.py</code> <pre><code>def lbp_features(image, node_info, radius=1, n_points=8, normalize=True):\n    \"\"\"\n    Extracts Local Binary Pattern (LBP) features from image regions.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n    node_info : dict\n        Node information dictionary from node creation\n    radius : int, optional\n        Radius of the LBP circle, by default 1\n    n_points : int, optional\n        Number of points around the circle, by default 8\n    normalize : bool, optional\n        Whether to normalize histograms, by default True\n\n    Returns\n    -------\n    torch.Tensor\n        Node features tensor with shape (N, 2^n_points)\n    \"\"\"\n    try:\n        from skimage.feature import local_binary_pattern\n    except ImportError:\n        raise ImportError(\"scikit-image is required for LBP feature extraction.\")\n\n    # Convert to grayscale if needed\n    if len(image.shape) == 3 and image.shape[2] &gt; 1:\n        try:\n            import cv2\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        except ImportError:\n            # Simple grayscale conversion as fallback\n            gray = np.mean(image, axis=2)\n    else:\n        gray = image\n\n    # Ensure image is in float format\n    if gray.dtype == np.uint8:\n        gray = gray.astype(np.float32) / 255.0\n\n    # Compute LBP texture\n    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n\n    # Number of bins - uniform LBP has n_points + 2 patterns\n    n_bins = n_points + 2\n\n    # Extract features based on node type\n    features = []\n\n    if 'segments' in node_info:  # Superpixel nodes\n        segments = node_info['segments']\n        unique_segments = np.unique(segments)\n\n        for i in unique_segments:\n            mask = segments == i\n            region_lbp = lbp[mask]\n\n            if len(region_lbp) &gt; 0:\n                hist, _ = np.histogram(region_lbp, bins=n_bins, range=(0, n_bins), density=normalize)\n            else:\n                hist = np.zeros(n_bins)\n\n            features.append(hist)\n\n    elif 'patches' in node_info:  # Patch nodes\n        patches = node_info['patches']\n        bboxes = node_info['bboxes']\n\n        for bbox in bboxes:\n            top, left, bottom, right = bbox\n            patch_lbp = lbp[top:bottom, left:right]\n\n            if patch_lbp.size &gt; 0:\n                hist, _ = np.histogram(patch_lbp, bins=n_bins, range=(0, n_bins), density=normalize)\n            else:\n                hist = np.zeros(n_bins)\n\n            features.append(hist)\n\n    elif 'values' in node_info:  # Pixel nodes\n        # For pixel nodes, we can't compute texture features\n        # Just return zeros\n        positions = node_info['positions']\n        features = np.zeros((len(positions), n_bins))\n\n    # Convert to tensor\n    features = np.array(features)\n\n    # Normalize if requested\n    if normalize:\n        features = features / (np.sum(features, axis=1, keepdims=True) + 1e-10)\n\n    return torch.tensor(features, dtype=torch.float)\n</code></pre>"},{"location":"imgraph/datasets/image_folder/","title":"Image Folder","text":""},{"location":"imgraph/datasets/image_folder/#imgraphdatasetsimage_folder","title":"<code>imgraph.datasets.image_folder</code>","text":"<p>Image folder dataset for graph neural networks.</p>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset","title":"<code>ImageFolderGraphDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Graph dataset for image folders.</p>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset--parameters","title":"Parameters","text":"<p>root : str     Root directory containing the image folder preset : str or callable, optional     Graph preset or custom graph builder function, by default 'slic_mean_color' transform : callable, optional     Transform function applied to graphs, by default None pre_transform : callable, optional     Pre-transform function applied to images before creating graphs, by default None image_transform : callable, optional     Transform function applied to images, by default None force_reload : bool, optional     Whether to force reload the dataset, by default False max_in_memory : int, optional     Maximum number of graphs to keep in memory at once, by default 1000</p> Source code in <code>imgraph/datasets/image_folder.py</code> <pre><code>class ImageFolderGraphDataset(Dataset):\n    \"\"\"\n    Graph dataset for image folders.\n\n    Parameters\n    ----------\n    root : str\n        Root directory containing the image folder\n    preset : str or callable, optional\n        Graph preset or custom graph builder function, by default 'slic_mean_color'\n    transform : callable, optional\n        Transform function applied to graphs, by default None\n    pre_transform : callable, optional\n        Pre-transform function applied to images before creating graphs, by default None\n    image_transform : callable, optional\n        Transform function applied to images, by default None\n    force_reload : bool, optional\n        Whether to force reload the dataset, by default False\n    max_in_memory : int, optional\n        Maximum number of graphs to keep in memory at once, by default 1000\n    \"\"\"\n\n    def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, \n                 image_transform=None, force_reload=False, max_in_memory=1000):\n        \"\"\"Initialize the ImageFolder graph dataset.\"\"\"\n        self.preset = preset\n        self.force_reload = force_reload\n        self.image_transform = image_transform or transforms.ToTensor()\n        self.max_in_memory = max_in_memory\n\n        # Only store image indices and labels, not the actual graphs\n        self.all_indices = []\n        self.all_labels = []\n        self.processed_path = None\n\n        # Create graph builder\n        if isinstance(preset, str):\n            if preset == 'slic_mean_color':\n                self.graph_builder = GraphPresets.slic_mean_color()\n            elif preset == 'slic_color_position':\n                self.graph_builder = GraphPresets.slic_color_position()\n            elif preset == 'patches_color':\n                self.graph_builder = GraphPresets.patches_color()\n            elif preset == 'tiny_graph':\n                self.graph_builder = GraphPresets.tiny_graph()\n            elif preset == 'superpixel_comprehensive':\n                self.graph_builder = GraphPresets.superpixel_comprehensive()\n            else:\n                raise ValueError(f\"Unknown preset: {preset}\")\n        else:\n            # Custom graph builder function\n            self.graph_builder = preset\n\n        # Create ImageFolder dataset for reference\n        self.raw_dirpath = os.path.join(root, 'raw')\n        os.makedirs(self.raw_dirpath, exist_ok=True)\n\n        try:\n            self.image_dataset = ImageFolder(self.raw_dirpath, transform=self.image_transform)\n            # Set class names\n            self.classes = self.image_dataset.classes\n            self.class_to_idx = self.image_dataset.class_to_idx\n            print(f\"Found ImageFolder dataset with {len(self.image_dataset)} images and {len(self.classes)} classes\")\n        except (FileNotFoundError, RuntimeError) as e:\n            print(f\"Warning: Could not create ImageFolder dataset: {e}\")\n            self.image_dataset = None\n            self.classes = []\n            self.class_to_idx = {}\n            print(\"No image dataset created!\")\n\n        # Initialize dataset\n        super(ImageFolderGraphDataset, self).__init__(root, transform, pre_transform)\n\n        # Process images\n        self.process()\n\n        print(f\"Dataset initialized with {len(self.all_indices)} images\")\n\n    @property\n    def raw_dir(self):\n        \"\"\"Get the raw directory.\"\"\"\n        return self.raw_dirpath\n\n    @property\n    def processed_dir(self):\n        \"\"\"Get the processed directory.\"\"\"\n        processed_dirpath = os.path.join(self.root, 'processed')\n        os.makedirs(processed_dirpath, exist_ok=True)\n        return processed_dirpath\n\n    @property\n    def raw_file_names(self):\n        \"\"\"List of raw file names.\"\"\"\n        return []\n\n    @property\n    def processed_file_names(self):\n        \"\"\"List of processed file names.\"\"\"\n        return ['dataset_info.pt']\n\n    def download(self):\n        \"\"\"Download the dataset (not required for ImageFolder).\"\"\"\n        pass\n\n    def process(self):\n        \"\"\"Process the dataset into graphs.\"\"\"\n        processed_path = self.processed_paths[0]\n        self.processed_path = processed_path\n\n        # Check if the dataset has already been processed\n        if os.path.exists(processed_path) and not self.force_reload:\n            try:\n                # Load dataset info\n                dataset_info = torch.load(processed_path)\n                self.all_indices = dataset_info['indices']\n                self.all_labels = dataset_info['labels']\n                print(f\"Loaded dataset info with {len(self.all_indices)} samples\")\n                return\n            except Exception as e:\n                print(f\"Failed to load dataset info: {e}\")\n                print(\"Will process dataset from scratch\")\n\n        # Check if we have an image dataset\n        if self.image_dataset is None or len(self.image_dataset) == 0:\n            print(\"No images found in the dataset. Cannot process.\")\n            return\n\n        # Process images\n        print(f\"Processing {len(self.image_dataset)} images into graphs...\")\n        indices = []\n        labels = []\n\n        for idx in tqdm(range(len(self.image_dataset))):\n            try:\n                # Get image and label\n                _, label = self.image_dataset[idx]\n\n                # Store index and label\n                indices.append(idx)\n                labels.append(label)\n\n                # Process in small batches to save memory\n                if len(indices) % 100 == 0:\n                    # Save progress periodically\n                    dataset_info = {\n                        'indices': indices,\n                        'labels': labels\n                    }\n                    torch.save(dataset_info, processed_path)\n\n            except Exception as e:\n                print(f\"Error processing image {idx}: {e}\")\n\n        # Save final dataset info\n        dataset_info = {\n            'indices': indices,\n            'labels': labels\n        }\n        torch.save(dataset_info, processed_path)\n\n        # Set instance variables\n        self.all_indices = indices\n        self.all_labels = labels\n\n        print(f\"Processed {len(indices)} images successfully\")\n        print(\"Done!\")\n\n    def _process_image(self, idx):\n        \"\"\"Process an image into a graph.\"\"\"\n        if self.image_dataset is None:\n            raise RuntimeError(\"No image dataset available\")\n\n        # Get image and label\n        img, label = self.image_dataset[idx]\n\n        # Convert to numpy array if needed\n        if isinstance(img, torch.Tensor):\n            img = img.permute(1, 2, 0).numpy()\n\n        # Apply pre-transform if provided\n        if self.pre_transform is not None:\n            img = self.pre_transform(img)\n\n        # Convert to graph\n        graph = self.graph_builder(img)\n\n        # Add label\n        graph.y = torch.tensor(label, dtype=torch.long)\n\n        return graph\n\n    def len(self):\n        \"\"\"Get the number of graphs in the dataset.\"\"\"\n        return len(self.all_indices)\n\n    def get(self, idx):\n        \"\"\"Get a graph by index.\"\"\"\n        if idx &gt;= len(self.all_indices):\n            raise IndexError(f\"Index {idx} out of range for dataset with {len(self.all_indices)} samples\")\n\n        # Process the image on-the-fly to save memory\n        image_idx = self.all_indices[idx]\n\n        # Process the image\n        data = self._process_image(image_idx)\n\n        # Apply transform if needed\n        if self.transform is not None:\n            data = self.transform(data)\n\n        return data\n\n    def get_train_test_split(self, train_ratio=0.8, stratify=True, random_seed=42):\n        \"\"\"Split the dataset into train and test sets.\"\"\"\n        # Check if we have enough data\n        if len(self) &lt; 2:\n            raise ValueError(f\"Dataset has only {len(self)} samples, need at least 2 for splitting\")\n\n        from torch.utils.data import Subset\n        from sklearn.model_selection import train_test_split\n\n        # Get indices and labels\n        indices = list(range(len(self)))\n        labels = self.all_labels\n\n        # Split indices\n        if stratify and len(set(labels)) &gt; 1:\n            train_indices, test_indices = train_test_split(\n                indices, train_size=train_ratio, stratify=labels, random_state=random_seed\n            )\n        else:\n            train_indices, test_indices = train_test_split(\n                indices, train_size=train_ratio, random_state=random_seed\n            )\n\n        # Create datasets\n        train_dataset = Subset(self, train_indices)\n        test_dataset = Subset(self, test_indices)\n\n        return train_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.processed_dir","title":"<code>processed_dir</code>  <code>property</code>","text":"<p>Get the processed directory.</p>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.processed_file_names","title":"<code>processed_file_names</code>  <code>property</code>","text":"<p>List of processed file names.</p>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.raw_dir","title":"<code>raw_dir</code>  <code>property</code>","text":"<p>Get the raw directory.</p>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.raw_file_names","title":"<code>raw_file_names</code>  <code>property</code>","text":"<p>List of raw file names.</p>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.__init__","title":"<code>__init__(root, preset='slic_mean_color', transform=None, pre_transform=None, image_transform=None, force_reload=False, max_in_memory=1000)</code>","text":"<p>Initialize the ImageFolder graph dataset.</p> Source code in <code>imgraph/datasets/image_folder.py</code> <pre><code>def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, \n             image_transform=None, force_reload=False, max_in_memory=1000):\n    \"\"\"Initialize the ImageFolder graph dataset.\"\"\"\n    self.preset = preset\n    self.force_reload = force_reload\n    self.image_transform = image_transform or transforms.ToTensor()\n    self.max_in_memory = max_in_memory\n\n    # Only store image indices and labels, not the actual graphs\n    self.all_indices = []\n    self.all_labels = []\n    self.processed_path = None\n\n    # Create graph builder\n    if isinstance(preset, str):\n        if preset == 'slic_mean_color':\n            self.graph_builder = GraphPresets.slic_mean_color()\n        elif preset == 'slic_color_position':\n            self.graph_builder = GraphPresets.slic_color_position()\n        elif preset == 'patches_color':\n            self.graph_builder = GraphPresets.patches_color()\n        elif preset == 'tiny_graph':\n            self.graph_builder = GraphPresets.tiny_graph()\n        elif preset == 'superpixel_comprehensive':\n            self.graph_builder = GraphPresets.superpixel_comprehensive()\n        else:\n            raise ValueError(f\"Unknown preset: {preset}\")\n    else:\n        # Custom graph builder function\n        self.graph_builder = preset\n\n    # Create ImageFolder dataset for reference\n    self.raw_dirpath = os.path.join(root, 'raw')\n    os.makedirs(self.raw_dirpath, exist_ok=True)\n\n    try:\n        self.image_dataset = ImageFolder(self.raw_dirpath, transform=self.image_transform)\n        # Set class names\n        self.classes = self.image_dataset.classes\n        self.class_to_idx = self.image_dataset.class_to_idx\n        print(f\"Found ImageFolder dataset with {len(self.image_dataset)} images and {len(self.classes)} classes\")\n    except (FileNotFoundError, RuntimeError) as e:\n        print(f\"Warning: Could not create ImageFolder dataset: {e}\")\n        self.image_dataset = None\n        self.classes = []\n        self.class_to_idx = {}\n        print(\"No image dataset created!\")\n\n    # Initialize dataset\n    super(ImageFolderGraphDataset, self).__init__(root, transform, pre_transform)\n\n    # Process images\n    self.process()\n\n    print(f\"Dataset initialized with {len(self.all_indices)} images\")\n</code></pre>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.download","title":"<code>download()</code>","text":"<p>Download the dataset (not required for ImageFolder).</p> Source code in <code>imgraph/datasets/image_folder.py</code> <pre><code>def download(self):\n    \"\"\"Download the dataset (not required for ImageFolder).\"\"\"\n    pass\n</code></pre>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.get","title":"<code>get(idx)</code>","text":"<p>Get a graph by index.</p> Source code in <code>imgraph/datasets/image_folder.py</code> <pre><code>def get(self, idx):\n    \"\"\"Get a graph by index.\"\"\"\n    if idx &gt;= len(self.all_indices):\n        raise IndexError(f\"Index {idx} out of range for dataset with {len(self.all_indices)} samples\")\n\n    # Process the image on-the-fly to save memory\n    image_idx = self.all_indices[idx]\n\n    # Process the image\n    data = self._process_image(image_idx)\n\n    # Apply transform if needed\n    if self.transform is not None:\n        data = self.transform(data)\n\n    return data\n</code></pre>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.get_train_test_split","title":"<code>get_train_test_split(train_ratio=0.8, stratify=True, random_seed=42)</code>","text":"<p>Split the dataset into train and test sets.</p> Source code in <code>imgraph/datasets/image_folder.py</code> <pre><code>def get_train_test_split(self, train_ratio=0.8, stratify=True, random_seed=42):\n    \"\"\"Split the dataset into train and test sets.\"\"\"\n    # Check if we have enough data\n    if len(self) &lt; 2:\n        raise ValueError(f\"Dataset has only {len(self)} samples, need at least 2 for splitting\")\n\n    from torch.utils.data import Subset\n    from sklearn.model_selection import train_test_split\n\n    # Get indices and labels\n    indices = list(range(len(self)))\n    labels = self.all_labels\n\n    # Split indices\n    if stratify and len(set(labels)) &gt; 1:\n        train_indices, test_indices = train_test_split(\n            indices, train_size=train_ratio, stratify=labels, random_state=random_seed\n        )\n    else:\n        train_indices, test_indices = train_test_split(\n            indices, train_size=train_ratio, random_state=random_seed\n        )\n\n    # Create datasets\n    train_dataset = Subset(self, train_indices)\n    test_dataset = Subset(self, test_indices)\n\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.len","title":"<code>len()</code>","text":"<p>Get the number of graphs in the dataset.</p> Source code in <code>imgraph/datasets/image_folder.py</code> <pre><code>def len(self):\n    \"\"\"Get the number of graphs in the dataset.\"\"\"\n    return len(self.all_indices)\n</code></pre>"},{"location":"imgraph/datasets/image_folder/#imgraph.datasets.image_folder.ImageFolderGraphDataset.process","title":"<code>process()</code>","text":"<p>Process the dataset into graphs.</p> Source code in <code>imgraph/datasets/image_folder.py</code> <pre><code>def process(self):\n    \"\"\"Process the dataset into graphs.\"\"\"\n    processed_path = self.processed_paths[0]\n    self.processed_path = processed_path\n\n    # Check if the dataset has already been processed\n    if os.path.exists(processed_path) and not self.force_reload:\n        try:\n            # Load dataset info\n            dataset_info = torch.load(processed_path)\n            self.all_indices = dataset_info['indices']\n            self.all_labels = dataset_info['labels']\n            print(f\"Loaded dataset info with {len(self.all_indices)} samples\")\n            return\n        except Exception as e:\n            print(f\"Failed to load dataset info: {e}\")\n            print(\"Will process dataset from scratch\")\n\n    # Check if we have an image dataset\n    if self.image_dataset is None or len(self.image_dataset) == 0:\n        print(\"No images found in the dataset. Cannot process.\")\n        return\n\n    # Process images\n    print(f\"Processing {len(self.image_dataset)} images into graphs...\")\n    indices = []\n    labels = []\n\n    for idx in tqdm(range(len(self.image_dataset))):\n        try:\n            # Get image and label\n            _, label = self.image_dataset[idx]\n\n            # Store index and label\n            indices.append(idx)\n            labels.append(label)\n\n            # Process in small batches to save memory\n            if len(indices) % 100 == 0:\n                # Save progress periodically\n                dataset_info = {\n                    'indices': indices,\n                    'labels': labels\n                }\n                torch.save(dataset_info, processed_path)\n\n        except Exception as e:\n            print(f\"Error processing image {idx}: {e}\")\n\n    # Save final dataset info\n    dataset_info = {\n        'indices': indices,\n        'labels': labels\n    }\n    torch.save(dataset_info, processed_path)\n\n    # Set instance variables\n    self.all_indices = indices\n    self.all_labels = labels\n\n    print(f\"Processed {len(indices)} images successfully\")\n    print(\"Done!\")\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/","title":"MedMNIST Dataset","text":""},{"location":"imgraph/datasets/medmnist_dataset/#imgraphdatasetsmedmnist_dataset","title":"<code>imgraph.datasets.medmnist_dataset</code>","text":"<p>MedMNIST graph dataset.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset","title":"<code>MedMNISTGraphDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Graph dataset for MedMNIST.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset--parameters","title":"Parameters","text":"<p>root : str     Root directory for dataset storage name : str     Name of the MedMNIST dataset (e.g., 'pathmnist', 'dermamnist', etc.) preset : str or callable, optional     Graph preset or custom graph builder function, by default 'slic_color_position' transform : callable, optional     Transform function applied to graphs, by default None pre_transform : callable, optional     Pre-transform function applied to images before creating graphs, by default None force_reload : bool, optional     Whether to force reload the dataset, by default False</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>class MedMNISTGraphDataset(Dataset):\n    \"\"\"\n    Graph dataset for MedMNIST.\n\n    Parameters\n    ----------\n    root : str\n        Root directory for dataset storage\n    name : str\n        Name of the MedMNIST dataset (e.g., 'pathmnist', 'dermamnist', etc.)\n    preset : str or callable, optional\n        Graph preset or custom graph builder function, by default 'slic_color_position'\n    transform : callable, optional\n        Transform function applied to graphs, by default None\n    pre_transform : callable, optional\n        Pre-transform function applied to images before creating graphs, by default None\n    force_reload : bool, optional\n        Whether to force reload the dataset, by default False\n    \"\"\"\n\n    def __init__(self, root, name, preset='slic_color_position', transform=None, pre_transform=None, force_reload=False):\n        \"\"\"Initialize the MedMNIST graph dataset.\"\"\"\n        self.name = name.lower()\n        self.preset = preset\n        self.force_reload = force_reload\n\n        # Create graph builder\n        if isinstance(preset, str):\n            if preset == 'slic_mean_color':\n                self.graph_builder = GraphPresets.slic_mean_color()\n            elif preset == 'slic_color_position':\n                self.graph_builder = GraphPresets.slic_color_position()\n            elif preset == 'patches_color':\n                self.graph_builder = GraphPresets.patches_color()\n            elif preset == 'tiny_graph':\n                self.graph_builder = GraphPresets.tiny_graph()\n            elif preset == 'superpixel_comprehensive':\n                self.graph_builder = GraphPresets.superpixel_comprehensive()\n            else:\n                raise ValueError(f\"Unknown preset: {preset}\")\n        else:\n            # Custom graph builder function\n            self.graph_builder = preset\n\n        # Initialize dataset\n        super(MedMNISTGraphDataset, self).__init__(root, transform, pre_transform)\n\n        # Load processed data\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_dir(self):\n        \"\"\"\n        Get the raw directory.\n\n        Returns\n        -------\n        str\n            Raw directory\n        \"\"\"\n        return os.path.join(self.root, 'raw')\n\n    @property\n    def processed_dir(self):\n        \"\"\"\n        Get the processed directory.\n\n        Returns\n        -------\n        str\n            Processed directory\n        \"\"\"\n        return os.path.join(self.root, 'processed')\n\n    @property\n    def raw_file_names(self):\n        \"\"\"List of raw file names.\"\"\"\n        return [\n            f'{self.name}_train.npz',\n            f'{self.name}_val.npz',\n            f'{self.name}_test.npz'\n        ]\n\n    @property\n    def processed_file_names(self):\n        \"\"\"List of processed file names.\"\"\"\n        return [f'{self.name}_graphs_{self.preset}.pt']\n\n    def download(self):\n        \"\"\"Download the MedMNIST dataset.\"\"\"\n        try:\n            import medmnist\n            from medmnist import INFO\n        except ImportError:\n            raise ImportError(\"MedMNIST is required. Install it with pip install medmnist\")\n\n        # Check if dataset exists\n        for file_name in self.raw_file_names:\n            file_path = os.path.join(self.raw_dir, file_name)\n            if not os.path.exists(file_path):\n                # Create directory if it doesn't exist\n                if not os.path.exists(self.raw_dir):\n                    os.makedirs(self.raw_dir)\n\n                # Download dataset\n                print(f\"Downloading {file_name}...\")\n\n                # Get dataset info\n                dataset_info = INFO[self.name]\n\n                # Get dataset class\n                DataClass = getattr(medmnist, dataset_info['python_class'])\n\n                # Download train set\n                if 'train' in file_name:\n                    DataClass(split='train', download=True, root=self.raw_dir)\n\n                # Download validation set\n                elif 'val' in file_name:\n                    DataClass(split='val', download=True, root=self.raw_dir)\n\n                # Download test set\n                elif 'test' in file_name:\n                    DataClass(split='test', download=True, root=self.raw_dir)\n\n    def process(self):\n        \"\"\"Process the MedMNIST dataset into graphs.\"\"\"\n        # Check if processed file exists and force_reload is False\n        if os.path.exists(self.processed_paths[0]) and not self.force_reload:\n            return\n\n        try:\n            import medmnist\n            from medmnist import INFO\n        except ImportError:\n            raise ImportError(\"MedMNIST is required. Install it with pip install medmnist\")\n\n        # Get dataset info\n        dataset_info = INFO[self.name]\n\n        # Get class names\n        if 'task' in dataset_info and dataset_info['task'] == 'multi-label, binary-class':\n            self.task = 'multi-label'\n            self.classes = dataset_info.get('label', {})\n        else:\n            self.task = 'multi-class'\n            self.classes = dataset_info.get('label', {})\n\n        # Process data\n        data_list = []\n\n        # Process each split\n        for split in ['train', 'val', 'test']:\n            # Load data\n            data_path = os.path.join(self.raw_dir, f'{self.name}_{split}.npz')\n            data = np.load(data_path)\n\n            images = data['images']\n            labels = data['labels']\n\n            print(f\"Processing {split} set ({len(images)} images)...\")\n\n            for i, (image, label) in enumerate(tqdm(zip(images, labels), total=len(images))):\n                # Convert to RGB if needed\n                if image.shape[-1] != 3:\n                    image = np.stack([image.squeeze()] * 3, axis=-1)\n\n                # Pre-transform if available\n                if self.pre_transform is not None:\n                    image = self.pre_transform(image)\n\n                # Convert to graph\n                try:\n                    graph = self.graph_builder(image)\n\n                    # Add label\n                    if self.task == 'multi-label':\n                        graph.y = torch.tensor(label, dtype=torch.float)\n                    else:\n                        graph.y = torch.tensor(label.squeeze(), dtype=torch.long)\n\n                    # Add split info\n                    graph.split = split\n\n                    # Add to data list\n                    data_list.append(graph)\n                except Exception as e:\n                    print(f\"Error processing image {split}/{i}: {e}\")\n\n        # Save processed data\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])\n\n    def len(self):\n        \"\"\"\n        Get the number of graphs in the dataset.\n\n        Returns\n        -------\n        int\n            Number of graphs\n        \"\"\"\n        return self.data.y.size(0)\n\n    def get(self, idx):\n        \"\"\"\n        Get a graph by index.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the graph\n\n        Returns\n        -------\n        torch_geometric.data.Data\n            Graph data object\n        \"\"\"\n        data = Data()\n        for key in self.data.keys:\n            item, slices = self.data[key], self.slices[key]\n            start, end = slices[idx].item(), slices[idx + 1].item()\n            if torch.is_tensor(item):\n                s = list(item.size())\n                if len(s) &gt; 0:\n                    s[0] = end - start\n                    data[key] = item[start:end].view(s)\n                else:\n                    data[key] = item[start:end]\n            elif start &lt; end:\n                data[key] = item[start:end]\n        return data\n\n    def visualize(self, idx):\n        \"\"\"\n        Visualize a graph.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the graph to visualize\n\n        Returns\n        -------\n        matplotlib.figure.Figure\n            Figure with the visualization\n        \"\"\"\n        # Get graph\n        graph = self.get(idx)\n\n        # Get image\n        try:\n            import medmnist\n            from medmnist import INFO\n\n            # Get dataset info\n            dataset_info = INFO[self.name]\n\n            # Get dataset class\n            DataClass = getattr(medmnist, dataset_info['python_class'])\n\n            # Get split\n            split = graph.split if hasattr(graph, 'split') else 'train'\n\n            # Load dataset\n            medmnist_dataset = DataClass(split=split, root=self.raw_dir)\n\n            # Find index in the original dataset\n            # This is an approximation as we don't store the original index\n            label = graph.y.cpu().numpy()\n\n            # Load all images and labels from the dataset\n            images = medmnist_dataset.imgs\n            labels = medmnist_dataset.labels\n\n            # Find the first image with the same label\n            # This is an approximation\n            img_idx = None\n            for i, l in enumerate(labels):\n                if np.array_equal(l, label):\n                    img_idx = i\n                    break\n\n            if img_idx is not None:\n                # Get image\n                img = images[img_idx]\n\n                # Convert to RGB if needed\n                if img.shape[-1] != 3:\n                    img = np.stack([img.squeeze()] * 3, axis=-1)\n            else:\n                # Use a synthetic image\n                img = np.ones((28, 28, 3)) * 200\n        except:\n            # Use a synthetic image if MedMNIST is not available\n            img = np.ones((28, 28, 3)) * 200\n\n        # Create figure\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n        # Display original image\n        ax1.imshow(img)\n\n        # Get label text\n        if self.task == 'multi-label':\n            label_text = ', '.join([self.classes.get(str(i), str(i)) for i, l in enumerate(graph.y) if l &gt; 0.5])\n        else:\n            label_idx = graph.y.item()\n            label_text = self.classes.get(str(label_idx), str(label_idx))\n\n        ax1.set_title(f\"Original Image (Class: {label_text})\")\n        ax1.axis('off')\n\n        # Display graph\n        if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n            node_positions = graph.node_info['centroids']\n\n            # Display image in background\n            ax2.imshow(img)\n\n            # Plot nodes\n            ax2.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n            # Plot edges\n            edge_index = graph.edge_index.cpu().numpy()\n            for i in range(edge_index.shape[1]):\n                src_idx = edge_index[0, i]\n                dst_idx = edge_index[1, i]\n\n                src_pos = node_positions[src_idx]\n                dst_pos = node_positions[dst_idx]\n\n                ax2.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n            ax2.set_title(f\"Graph: {graph.num_nodes} nodes, {graph.edge_index.shape[1]} edges\")\n            ax2.axis('off')\n        else:\n            ax2.text(0.5, 0.5, \"Graph does not contain node positions\", ha='center', va='center')\n            ax2.axis('off')\n\n        return fig\n\n    def get_splits(self):\n        \"\"\"\n        Get train, validation, and test datasets.\n\n        Returns\n        -------\n        tuple\n            (train_dataset, val_dataset, test_dataset)\n        \"\"\"\n        from torch.utils.data import Subset\n\n        # Get indices for each split\n        train_indices = []\n        val_indices = []\n        test_indices = []\n\n        for i in range(len(self)):\n            graph = self.get(i)\n            if hasattr(graph, 'split'):\n                if graph.split == 'train':\n                    train_indices.append(i)\n                elif graph.split == 'val':\n                    val_indices.append(i)\n                elif graph.split == 'test':\n                    test_indices.append(i)\n            else:\n                # If no split info, assume train\n                train_indices.append(i)\n\n        # Create datasets\n        train_dataset = Subset(self, train_indices)\n        val_dataset = Subset(self, val_indices)\n        test_dataset = Subset(self, test_indices)\n\n        return train_dataset, val_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.processed_dir","title":"<code>processed_dir</code>  <code>property</code>","text":"<p>Get the processed directory.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.processed_dir--returns","title":"Returns","text":"<p>str     Processed directory</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.processed_file_names","title":"<code>processed_file_names</code>  <code>property</code>","text":"<p>List of processed file names.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.raw_dir","title":"<code>raw_dir</code>  <code>property</code>","text":"<p>Get the raw directory.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.raw_dir--returns","title":"Returns","text":"<p>str     Raw directory</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.raw_file_names","title":"<code>raw_file_names</code>  <code>property</code>","text":"<p>List of raw file names.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.__init__","title":"<code>__init__(root, name, preset='slic_color_position', transform=None, pre_transform=None, force_reload=False)</code>","text":"<p>Initialize the MedMNIST graph dataset.</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>def __init__(self, root, name, preset='slic_color_position', transform=None, pre_transform=None, force_reload=False):\n    \"\"\"Initialize the MedMNIST graph dataset.\"\"\"\n    self.name = name.lower()\n    self.preset = preset\n    self.force_reload = force_reload\n\n    # Create graph builder\n    if isinstance(preset, str):\n        if preset == 'slic_mean_color':\n            self.graph_builder = GraphPresets.slic_mean_color()\n        elif preset == 'slic_color_position':\n            self.graph_builder = GraphPresets.slic_color_position()\n        elif preset == 'patches_color':\n            self.graph_builder = GraphPresets.patches_color()\n        elif preset == 'tiny_graph':\n            self.graph_builder = GraphPresets.tiny_graph()\n        elif preset == 'superpixel_comprehensive':\n            self.graph_builder = GraphPresets.superpixel_comprehensive()\n        else:\n            raise ValueError(f\"Unknown preset: {preset}\")\n    else:\n        # Custom graph builder function\n        self.graph_builder = preset\n\n    # Initialize dataset\n    super(MedMNISTGraphDataset, self).__init__(root, transform, pre_transform)\n\n    # Load processed data\n    self.data, self.slices = torch.load(self.processed_paths[0])\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.download","title":"<code>download()</code>","text":"<p>Download the MedMNIST dataset.</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>def download(self):\n    \"\"\"Download the MedMNIST dataset.\"\"\"\n    try:\n        import medmnist\n        from medmnist import INFO\n    except ImportError:\n        raise ImportError(\"MedMNIST is required. Install it with pip install medmnist\")\n\n    # Check if dataset exists\n    for file_name in self.raw_file_names:\n        file_path = os.path.join(self.raw_dir, file_name)\n        if not os.path.exists(file_path):\n            # Create directory if it doesn't exist\n            if not os.path.exists(self.raw_dir):\n                os.makedirs(self.raw_dir)\n\n            # Download dataset\n            print(f\"Downloading {file_name}...\")\n\n            # Get dataset info\n            dataset_info = INFO[self.name]\n\n            # Get dataset class\n            DataClass = getattr(medmnist, dataset_info['python_class'])\n\n            # Download train set\n            if 'train' in file_name:\n                DataClass(split='train', download=True, root=self.raw_dir)\n\n            # Download validation set\n            elif 'val' in file_name:\n                DataClass(split='val', download=True, root=self.raw_dir)\n\n            # Download test set\n            elif 'test' in file_name:\n                DataClass(split='test', download=True, root=self.raw_dir)\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.get","title":"<code>get(idx)</code>","text":"<p>Get a graph by index.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.get--parameters","title":"Parameters","text":"<p>idx : int     Index of the graph</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.get--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph data object</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>def get(self, idx):\n    \"\"\"\n    Get a graph by index.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the graph\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph data object\n    \"\"\"\n    data = Data()\n    for key in self.data.keys:\n        item, slices = self.data[key], self.slices[key]\n        start, end = slices[idx].item(), slices[idx + 1].item()\n        if torch.is_tensor(item):\n            s = list(item.size())\n            if len(s) &gt; 0:\n                s[0] = end - start\n                data[key] = item[start:end].view(s)\n            else:\n                data[key] = item[start:end]\n        elif start &lt; end:\n            data[key] = item[start:end]\n    return data\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.get_splits","title":"<code>get_splits()</code>","text":"<p>Get train, validation, and test datasets.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.get_splits--returns","title":"Returns","text":"<p>tuple     (train_dataset, val_dataset, test_dataset)</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>def get_splits(self):\n    \"\"\"\n    Get train, validation, and test datasets.\n\n    Returns\n    -------\n    tuple\n        (train_dataset, val_dataset, test_dataset)\n    \"\"\"\n    from torch.utils.data import Subset\n\n    # Get indices for each split\n    train_indices = []\n    val_indices = []\n    test_indices = []\n\n    for i in range(len(self)):\n        graph = self.get(i)\n        if hasattr(graph, 'split'):\n            if graph.split == 'train':\n                train_indices.append(i)\n            elif graph.split == 'val':\n                val_indices.append(i)\n            elif graph.split == 'test':\n                test_indices.append(i)\n        else:\n            # If no split info, assume train\n            train_indices.append(i)\n\n    # Create datasets\n    train_dataset = Subset(self, train_indices)\n    val_dataset = Subset(self, val_indices)\n    test_dataset = Subset(self, test_indices)\n\n    return train_dataset, val_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.len","title":"<code>len()</code>","text":"<p>Get the number of graphs in the dataset.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.len--returns","title":"Returns","text":"<p>int     Number of graphs</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>def len(self):\n    \"\"\"\n    Get the number of graphs in the dataset.\n\n    Returns\n    -------\n    int\n        Number of graphs\n    \"\"\"\n    return self.data.y.size(0)\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.process","title":"<code>process()</code>","text":"<p>Process the MedMNIST dataset into graphs.</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>def process(self):\n    \"\"\"Process the MedMNIST dataset into graphs.\"\"\"\n    # Check if processed file exists and force_reload is False\n    if os.path.exists(self.processed_paths[0]) and not self.force_reload:\n        return\n\n    try:\n        import medmnist\n        from medmnist import INFO\n    except ImportError:\n        raise ImportError(\"MedMNIST is required. Install it with pip install medmnist\")\n\n    # Get dataset info\n    dataset_info = INFO[self.name]\n\n    # Get class names\n    if 'task' in dataset_info and dataset_info['task'] == 'multi-label, binary-class':\n        self.task = 'multi-label'\n        self.classes = dataset_info.get('label', {})\n    else:\n        self.task = 'multi-class'\n        self.classes = dataset_info.get('label', {})\n\n    # Process data\n    data_list = []\n\n    # Process each split\n    for split in ['train', 'val', 'test']:\n        # Load data\n        data_path = os.path.join(self.raw_dir, f'{self.name}_{split}.npz')\n        data = np.load(data_path)\n\n        images = data['images']\n        labels = data['labels']\n\n        print(f\"Processing {split} set ({len(images)} images)...\")\n\n        for i, (image, label) in enumerate(tqdm(zip(images, labels), total=len(images))):\n            # Convert to RGB if needed\n            if image.shape[-1] != 3:\n                image = np.stack([image.squeeze()] * 3, axis=-1)\n\n            # Pre-transform if available\n            if self.pre_transform is not None:\n                image = self.pre_transform(image)\n\n            # Convert to graph\n            try:\n                graph = self.graph_builder(image)\n\n                # Add label\n                if self.task == 'multi-label':\n                    graph.y = torch.tensor(label, dtype=torch.float)\n                else:\n                    graph.y = torch.tensor(label.squeeze(), dtype=torch.long)\n\n                # Add split info\n                graph.split = split\n\n                # Add to data list\n                data_list.append(graph)\n            except Exception as e:\n                print(f\"Error processing image {split}/{i}: {e}\")\n\n    # Save processed data\n    data, slices = self.collate(data_list)\n    torch.save((data, slices), self.processed_paths[0])\n</code></pre>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.visualize","title":"<code>visualize(idx)</code>","text":"<p>Visualize a graph.</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.visualize--parameters","title":"Parameters","text":"<p>idx : int     Index of the graph to visualize</p>"},{"location":"imgraph/datasets/medmnist_dataset/#imgraph.datasets.medmnist_dataset.MedMNISTGraphDataset.visualize--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/datasets/medmnist_dataset.py</code> <pre><code>def visualize(self, idx):\n    \"\"\"\n    Visualize a graph.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the graph to visualize\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Get graph\n    graph = self.get(idx)\n\n    # Get image\n    try:\n        import medmnist\n        from medmnist import INFO\n\n        # Get dataset info\n        dataset_info = INFO[self.name]\n\n        # Get dataset class\n        DataClass = getattr(medmnist, dataset_info['python_class'])\n\n        # Get split\n        split = graph.split if hasattr(graph, 'split') else 'train'\n\n        # Load dataset\n        medmnist_dataset = DataClass(split=split, root=self.raw_dir)\n\n        # Find index in the original dataset\n        # This is an approximation as we don't store the original index\n        label = graph.y.cpu().numpy()\n\n        # Load all images and labels from the dataset\n        images = medmnist_dataset.imgs\n        labels = medmnist_dataset.labels\n\n        # Find the first image with the same label\n        # This is an approximation\n        img_idx = None\n        for i, l in enumerate(labels):\n            if np.array_equal(l, label):\n                img_idx = i\n                break\n\n        if img_idx is not None:\n            # Get image\n            img = images[img_idx]\n\n            # Convert to RGB if needed\n            if img.shape[-1] != 3:\n                img = np.stack([img.squeeze()] * 3, axis=-1)\n        else:\n            # Use a synthetic image\n            img = np.ones((28, 28, 3)) * 200\n    except:\n        # Use a synthetic image if MedMNIST is not available\n        img = np.ones((28, 28, 3)) * 200\n\n    # Create figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Display original image\n    ax1.imshow(img)\n\n    # Get label text\n    if self.task == 'multi-label':\n        label_text = ', '.join([self.classes.get(str(i), str(i)) for i, l in enumerate(graph.y) if l &gt; 0.5])\n    else:\n        label_idx = graph.y.item()\n        label_text = self.classes.get(str(label_idx), str(label_idx))\n\n    ax1.set_title(f\"Original Image (Class: {label_text})\")\n    ax1.axis('off')\n\n    # Display graph\n    if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n        node_positions = graph.node_info['centroids']\n\n        # Display image in background\n        ax2.imshow(img)\n\n        # Plot nodes\n        ax2.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n        # Plot edges\n        edge_index = graph.edge_index.cpu().numpy()\n        for i in range(edge_index.shape[1]):\n            src_idx = edge_index[0, i]\n            dst_idx = edge_index[1, i]\n\n            src_pos = node_positions[src_idx]\n            dst_pos = node_positions[dst_idx]\n\n            ax2.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n        ax2.set_title(f\"Graph: {graph.num_nodes} nodes, {graph.edge_index.shape[1]} edges\")\n        ax2.axis('off')\n    else:\n        ax2.text(0.5, 0.5, \"Graph does not contain node positions\", ha='center', va='center')\n        ax2.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/","title":"MNIST Dataset","text":""},{"location":"imgraph/datasets/mnist_dataset/#imgraphdatasetsmnist_dataset","title":"<code>imgraph.datasets.mnist_dataset</code>","text":"<p>MNIST graph dataset.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset","title":"<code>MNISTGraphDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Graph dataset for MNIST.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset--parameters","title":"Parameters","text":"<p>root : str     Root directory for dataset storage preset : str or callable, optional     Graph preset or custom graph builder function, by default 'slic_mean_color' transform : callable, optional     Transform function applied to graphs, by default None pre_transform : callable, optional     Pre-transform function applied to images before creating graphs, by default None force_reload : bool, optional     Whether to force reload the dataset, by default False</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>class MNISTGraphDataset(Dataset):\n    \"\"\"\n    Graph dataset for MNIST.\n\n    Parameters\n    ----------\n    root : str\n        Root directory for dataset storage\n    preset : str or callable, optional\n        Graph preset or custom graph builder function, by default 'slic_mean_color'\n    transform : callable, optional\n        Transform function applied to graphs, by default None\n    pre_transform : callable, optional\n        Pre-transform function applied to images before creating graphs, by default None\n    force_reload : bool, optional\n        Whether to force reload the dataset, by default False\n    \"\"\"\n\n    def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n        \"\"\"Initialize the MNIST graph dataset.\"\"\"\n        self.preset = preset\n        self.force_reload = force_reload\n\n        # Create graph builder\n        if isinstance(preset, str):\n            if preset == 'slic_mean_color':\n                self.graph_builder = GraphPresets.slic_mean_color()\n            elif preset == 'slic_color_position':\n                self.graph_builder = GraphPresets.slic_color_position()\n            elif preset == 'patches_color':\n                self.graph_builder = GraphPresets.patches_color()\n            elif preset == 'tiny_graph':\n                self.graph_builder = GraphPresets.tiny_graph()\n            elif preset == 'superpixel_comprehensive':\n                self.graph_builder = GraphPresets.superpixel_comprehensive()\n            else:\n                raise ValueError(f\"Unknown preset: {preset}\")\n        else:\n            # Custom graph builder function\n            self.graph_builder = preset\n\n        # Initialize dataset\n        super(MNISTGraphDataset, self).__init__(root, transform, pre_transform)\n\n        # Load processed data\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n        # Set class names\n        self.classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n    @property\n    def raw_file_names(self):\n        \"\"\"List of raw file names.\"\"\"\n        return ['training.pt', 'test.pt']\n\n    @property\n    def processed_file_names(self):\n        \"\"\"List of processed file names.\"\"\"\n        return [f'mnist_graphs_{self.preset}.pt']\n\n    def download(self):\n        \"\"\"Download the MNIST dataset.\"\"\"\n        # Download MNIST\n        MNIST(self.raw_dir, train=True, download=True)\n        MNIST(self.raw_dir, train=False, download=True)\n\n    def process(self):\n        \"\"\"Process the MNIST dataset into graphs.\"\"\"\n        # Check if processed file exists and force_reload is False\n        if os.path.exists(self.processed_paths[0]) and not self.force_reload:\n            return\n\n        # Load MNIST\n        train_dataset = MNIST(self.raw_dir, train=True, download=True)\n        test_dataset = MNIST(self.raw_dir, train=False, download=True)\n\n        # Combine train and test datasets\n        data_list = []\n\n        # Process training data\n        print(\"Processing MNIST training data...\")\n        for img, label in tqdm(train_dataset):\n            # Convert to numpy array\n            img_np = np.array(img)\n\n            # Add channel dimension for grayscale\n            img_np = np.stack([img_np, img_np, img_np], axis=2)\n\n            # Pre-transform if available\n            if self.pre_transform is not None:\n                img_np = self.pre_transform(img_np)\n\n            # Convert to graph\n            try:\n                graph = self.graph_builder(img_np)\n\n                # Add label\n                graph.y = torch.tensor(label, dtype=torch.long)\n\n                # Add to data list\n                data_list.append(graph)\n            except Exception as e:\n                print(f\"Error processing image: {e}\")\n\n        # Process test data\n        print(\"Processing MNIST test data...\")\n        for img, label in tqdm(test_dataset):\n            # Convert to numpy array\n            img_np = np.array(img)\n\n            # Add channel dimension for grayscale\n            img_np = np.stack([img_np, img_np, img_np], axis=2)\n\n            # Pre-transform if available\n            if self.pre_transform is not None:\n                img_np = self.pre_transform(img_np)\n\n            # Convert to graph\n            try:\n                graph = self.graph_builder(img_np)\n\n                # Add label\n                graph.y = torch.tensor(label, dtype=torch.long)\n\n                # Add to data list\n                data_list.append(graph)\n            except Exception as e:\n                print(f\"Error processing image: {e}\")\n\n        # Save processed data\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])\n\n    def len(self):\n        \"\"\"\n        Get the number of graphs in the dataset.\n\n        Returns\n        -------\n        int\n            Number of graphs\n        \"\"\"\n        return self.data.y.size(0)\n\n    def get(self, idx):\n        \"\"\"\n        Get a graph by index.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the graph\n\n        Returns\n        -------\n        torch_geometric.data.Data\n            Graph data object\n        \"\"\"\n        data = Data()\n        for key in self.data.keys:\n            item, slices = self.data[key], self.slices[key]\n            start, end = slices[idx].item(), slices[idx + 1].item()\n            if torch.is_tensor(item):\n                s = list(item.size())\n                if len(s) &gt; 0:\n                    s[0] = end - start\n                    data[key] = item[start:end].view(s)\n                else:\n                    data[key] = item[start:end]\n            elif start &lt; end:\n                data[key] = item[start:end]\n        return data\n\n    def visualize(self, idx, show_graph=True):\n        \"\"\"\n        Visualize a graph.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the graph to visualize\n        show_graph : bool, optional\n            Whether to show the graph representation, by default True\n\n        Returns\n        -------\n        matplotlib.figure.Figure\n            Figure with the visualization\n        \"\"\"\n        # Get graph\n        graph = self.get(idx)\n\n        # Get original image from MNIST\n        mnist = MNIST(self.raw_dir, train=idx &lt; 60000, download=False)\n        img, label = mnist[idx % 60000]\n\n        # Convert to numpy array\n        img_np = np.array(img)\n\n        # Add channel dimension for grayscale\n        img_np = np.stack([img_np, img_np, img_np], axis=2)\n\n        # Create figure\n        if show_graph:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n            # Display original image\n            ax1.imshow(img_np)\n            ax1.set_title(f\"Original Image (Class: {label})\")\n            ax1.axis('off')\n\n            # Display graph\n            if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n                node_positions = graph.node_info['centroids']\n\n                # Display image in background\n                ax2.imshow(img_np)\n\n                # Plot nodes\n                ax2.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n                # Plot edges\n                edge_index = graph.edge_index.cpu().numpy()\n                for i in range(edge_index.shape[1]):\n                    src_idx = edge_index[0, i]\n                    dst_idx = edge_index[1, i]\n\n                    src_pos = node_positions[src_idx]\n                    dst_pos = node_positions[dst_idx]\n\n                    ax2.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n                ax2.set_title(f\"Graph: {graph.num_nodes} nodes, {graph.edge_index.shape[1]} edges\")\n                ax2.axis('off')\n            else:\n                ax2.text(0.5, 0.5, \"Graph does not contain node positions\", ha='center', va='center')\n                ax2.axis('off')\n        else:\n            fig, ax = plt.subplots(figsize=(5, 5))\n\n            # Display original image\n            ax.imshow(img_np)\n            ax.set_title(f\"Original Image (Class: {label})\")\n            ax.axis('off')\n\n        return fig\n\n    def get_train_test_split(self, train_ratio=0.8, random_seed=42):\n        \"\"\"\n        Split the dataset into train and test sets.\n\n        Parameters\n        ----------\n        train_ratio : float, optional\n            Ratio of training samples, by default 0.8\n        random_seed : int, optional\n            Random seed, by default 42\n\n        Returns\n        -------\n        tuple\n            (train_dataset, test_dataset)\n        \"\"\"\n        from torch_geometric.data import Dataset\n\n        # Set random seed\n        np.random.seed(random_seed)\n\n        # Get indices\n        train_indices = np.arange(60000)\n        test_indices = np.arange(60000, len(self))\n\n        # Create datasets\n        train_dataset = torch.utils.data.Subset(self, train_indices)\n        test_dataset = torch.utils.data.Subset(self, test_indices)\n\n        return train_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.processed_file_names","title":"<code>processed_file_names</code>  <code>property</code>","text":"<p>List of processed file names.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.raw_file_names","title":"<code>raw_file_names</code>  <code>property</code>","text":"<p>List of raw file names.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.__init__","title":"<code>__init__(root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False)</code>","text":"<p>Initialize the MNIST graph dataset.</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n    \"\"\"Initialize the MNIST graph dataset.\"\"\"\n    self.preset = preset\n    self.force_reload = force_reload\n\n    # Create graph builder\n    if isinstance(preset, str):\n        if preset == 'slic_mean_color':\n            self.graph_builder = GraphPresets.slic_mean_color()\n        elif preset == 'slic_color_position':\n            self.graph_builder = GraphPresets.slic_color_position()\n        elif preset == 'patches_color':\n            self.graph_builder = GraphPresets.patches_color()\n        elif preset == 'tiny_graph':\n            self.graph_builder = GraphPresets.tiny_graph()\n        elif preset == 'superpixel_comprehensive':\n            self.graph_builder = GraphPresets.superpixel_comprehensive()\n        else:\n            raise ValueError(f\"Unknown preset: {preset}\")\n    else:\n        # Custom graph builder function\n        self.graph_builder = preset\n\n    # Initialize dataset\n    super(MNISTGraphDataset, self).__init__(root, transform, pre_transform)\n\n    # Load processed data\n    self.data, self.slices = torch.load(self.processed_paths[0])\n\n    # Set class names\n    self.classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.download","title":"<code>download()</code>","text":"<p>Download the MNIST dataset.</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def download(self):\n    \"\"\"Download the MNIST dataset.\"\"\"\n    # Download MNIST\n    MNIST(self.raw_dir, train=True, download=True)\n    MNIST(self.raw_dir, train=False, download=True)\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.get","title":"<code>get(idx)</code>","text":"<p>Get a graph by index.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.get--parameters","title":"Parameters","text":"<p>idx : int     Index of the graph</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.get--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph data object</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def get(self, idx):\n    \"\"\"\n    Get a graph by index.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the graph\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph data object\n    \"\"\"\n    data = Data()\n    for key in self.data.keys:\n        item, slices = self.data[key], self.slices[key]\n        start, end = slices[idx].item(), slices[idx + 1].item()\n        if torch.is_tensor(item):\n            s = list(item.size())\n            if len(s) &gt; 0:\n                s[0] = end - start\n                data[key] = item[start:end].view(s)\n            else:\n                data[key] = item[start:end]\n        elif start &lt; end:\n            data[key] = item[start:end]\n    return data\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.get_train_test_split","title":"<code>get_train_test_split(train_ratio=0.8, random_seed=42)</code>","text":"<p>Split the dataset into train and test sets.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.get_train_test_split--parameters","title":"Parameters","text":"<p>train_ratio : float, optional     Ratio of training samples, by default 0.8 random_seed : int, optional     Random seed, by default 42</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.get_train_test_split--returns","title":"Returns","text":"<p>tuple     (train_dataset, test_dataset)</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def get_train_test_split(self, train_ratio=0.8, random_seed=42):\n    \"\"\"\n    Split the dataset into train and test sets.\n\n    Parameters\n    ----------\n    train_ratio : float, optional\n        Ratio of training samples, by default 0.8\n    random_seed : int, optional\n        Random seed, by default 42\n\n    Returns\n    -------\n    tuple\n        (train_dataset, test_dataset)\n    \"\"\"\n    from torch_geometric.data import Dataset\n\n    # Set random seed\n    np.random.seed(random_seed)\n\n    # Get indices\n    train_indices = np.arange(60000)\n    test_indices = np.arange(60000, len(self))\n\n    # Create datasets\n    train_dataset = torch.utils.data.Subset(self, train_indices)\n    test_dataset = torch.utils.data.Subset(self, test_indices)\n\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.len","title":"<code>len()</code>","text":"<p>Get the number of graphs in the dataset.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.len--returns","title":"Returns","text":"<p>int     Number of graphs</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def len(self):\n    \"\"\"\n    Get the number of graphs in the dataset.\n\n    Returns\n    -------\n    int\n        Number of graphs\n    \"\"\"\n    return self.data.y.size(0)\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.process","title":"<code>process()</code>","text":"<p>Process the MNIST dataset into graphs.</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def process(self):\n    \"\"\"Process the MNIST dataset into graphs.\"\"\"\n    # Check if processed file exists and force_reload is False\n    if os.path.exists(self.processed_paths[0]) and not self.force_reload:\n        return\n\n    # Load MNIST\n    train_dataset = MNIST(self.raw_dir, train=True, download=True)\n    test_dataset = MNIST(self.raw_dir, train=False, download=True)\n\n    # Combine train and test datasets\n    data_list = []\n\n    # Process training data\n    print(\"Processing MNIST training data...\")\n    for img, label in tqdm(train_dataset):\n        # Convert to numpy array\n        img_np = np.array(img)\n\n        # Add channel dimension for grayscale\n        img_np = np.stack([img_np, img_np, img_np], axis=2)\n\n        # Pre-transform if available\n        if self.pre_transform is not None:\n            img_np = self.pre_transform(img_np)\n\n        # Convert to graph\n        try:\n            graph = self.graph_builder(img_np)\n\n            # Add label\n            graph.y = torch.tensor(label, dtype=torch.long)\n\n            # Add to data list\n            data_list.append(graph)\n        except Exception as e:\n            print(f\"Error processing image: {e}\")\n\n    # Process test data\n    print(\"Processing MNIST test data...\")\n    for img, label in tqdm(test_dataset):\n        # Convert to numpy array\n        img_np = np.array(img)\n\n        # Add channel dimension for grayscale\n        img_np = np.stack([img_np, img_np, img_np], axis=2)\n\n        # Pre-transform if available\n        if self.pre_transform is not None:\n            img_np = self.pre_transform(img_np)\n\n        # Convert to graph\n        try:\n            graph = self.graph_builder(img_np)\n\n            # Add label\n            graph.y = torch.tensor(label, dtype=torch.long)\n\n            # Add to data list\n            data_list.append(graph)\n        except Exception as e:\n            print(f\"Error processing image: {e}\")\n\n    # Save processed data\n    data, slices = self.collate(data_list)\n    torch.save((data, slices), self.processed_paths[0])\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.visualize","title":"<code>visualize(idx, show_graph=True)</code>","text":"<p>Visualize a graph.</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.visualize--parameters","title":"Parameters","text":"<p>idx : int     Index of the graph to visualize show_graph : bool, optional     Whether to show the graph representation, by default True</p>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.MNISTGraphDataset.visualize--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def visualize(self, idx, show_graph=True):\n    \"\"\"\n    Visualize a graph.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the graph to visualize\n    show_graph : bool, optional\n        Whether to show the graph representation, by default True\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Get graph\n    graph = self.get(idx)\n\n    # Get original image from MNIST\n    mnist = MNIST(self.raw_dir, train=idx &lt; 60000, download=False)\n    img, label = mnist[idx % 60000]\n\n    # Convert to numpy array\n    img_np = np.array(img)\n\n    # Add channel dimension for grayscale\n    img_np = np.stack([img_np, img_np, img_np], axis=2)\n\n    # Create figure\n    if show_graph:\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n        # Display original image\n        ax1.imshow(img_np)\n        ax1.set_title(f\"Original Image (Class: {label})\")\n        ax1.axis('off')\n\n        # Display graph\n        if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n            node_positions = graph.node_info['centroids']\n\n            # Display image in background\n            ax2.imshow(img_np)\n\n            # Plot nodes\n            ax2.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n            # Plot edges\n            edge_index = graph.edge_index.cpu().numpy()\n            for i in range(edge_index.shape[1]):\n                src_idx = edge_index[0, i]\n                dst_idx = edge_index[1, i]\n\n                src_pos = node_positions[src_idx]\n                dst_pos = node_positions[dst_idx]\n\n                ax2.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n            ax2.set_title(f\"Graph: {graph.num_nodes} nodes, {graph.edge_index.shape[1]} edges\")\n            ax2.axis('off')\n        else:\n            ax2.text(0.5, 0.5, \"Graph does not contain node positions\", ha='center', va='center')\n            ax2.axis('off')\n    else:\n        fig, ax = plt.subplots(figsize=(5, 5))\n\n        # Display original image\n        ax.imshow(img_np)\n        ax.set_title(f\"Original Image (Class: {label})\")\n        ax.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/datasets/mnist_dataset/#imgraph.datasets.mnist_dataset.get_mnist_dataset","title":"<code>get_mnist_dataset(root='./data', train=True, transform=None)</code>","text":"<p>Get the MNIST dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset should be stored</p> <code>'./data'</code> <code>train</code> <code>bool</code> <p>If True, creates dataset from training set, otherwise creates from test set</p> <code>True</code> <code>transform</code> <code>callable</code> <p>A function/transform that takes in an PIL image and returns a transformed version</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MNISTGraphDataset</code> <p>The MNIST dataset</p> Source code in <code>imgraph/datasets/mnist_dataset.py</code> <pre><code>def get_mnist_dataset(root='./data', train=True, transform=None):\n    \"\"\"\n    Get the MNIST dataset.\n\n    Args:\n        root (str): Root directory where the dataset should be stored\n        train (bool): If True, creates dataset from training set, otherwise creates from test set\n        transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version\n\n    Returns:\n        MNISTGraphDataset: The MNIST dataset\n    \"\"\"\n    return MNISTGraphDataset(root=root, train=train, transform=transform)\n</code></pre>"},{"location":"imgraph/datasets/pneumonia_dataset/","title":"Pneumonia Dataset","text":""},{"location":"imgraph/datasets/pneumonia_dataset/#imgraphdatasetspneumonia_dataset","title":"<code>imgraph.datasets.pneumonia_dataset</code>","text":""},{"location":"imgraph/datasets/pneumonia_dataset/#imgraph.datasets.pneumonia_dataset.get_pneumonia_dataset","title":"<code>get_pneumonia_dataset(super_pixels, feature_extractor)</code>","text":"<p>Download the pneumonia datasets Args:     super_pixels (int): The number of super pixels to be used in the graph     feature_extractor (str): The feature extractor to be used to create the graph. Currently only 'resnet18/efficientnet/densenet121' is supported</p> <p>Returns:</p> Type Description <code>list</code> <p>Pneumonia dataset</p> Source code in <code>imgraph/datasets/pneumonia_dataset.py</code> <pre><code>def get_pneumonia_dataset(super_pixels : int, feature_extractor : str) -&gt; list:\n    \"\"\"\n    Download the pneumonia datasets\n    Args:\n        super_pixels (int): The number of super pixels to be used in the graph\n        feature_extractor (str): The feature extractor to be used to create the graph. Currently only 'resnet18/efficientnet/densenet121' is supported\n\n    Returns:\n        Pneumonia dataset\n    \"\"\"\n\n\n    train_loader_url = pneumonia_dataset_url[f\"train_dataloader_{super_pixels}_{feature_extractor}\"]\n    test_loader_url = pneumonia_dataset_url[f\"test_dataloader_{super_pixels}_{feature_extractor}\"]\n\n\n    path = osp.join(DEFAULT_CACHE_DIR, 'output')\n    if os.environ.get(ENV_IMGRAPH_HOME):\n        path = osp.join(os.environ.get(ENV_IMGRAPH_HOME), 'output')\n\n    path = osp.expanduser(path)\n\n    train_filename = osp.expanduser(osp.join(path, f'train_dataloader_{super_pixels}_{feature_extractor}.pkl'))\n    test_filename = osp.expanduser(osp.join(path, f'test_dataloader_{super_pixels}_{feature_extractor}.pkl'))\n\n\n    download_from_url(train_loader_url, path, f'train_dataloader_{super_pixels}_{feature_extractor}.pkl')\n    download_from_url(test_loader_url, path, f'test_dataloader_{super_pixels}_{feature_extractor}.pkl')\n    #filepath = osp.join(path, filename)\n\n    train_loader = None\n    test_loader = None\n\n    print(\"Loading Pneumonia dataset\")\n    with open(train_filename, 'rb') as f:\n        train_loader = pickle.load(f)\n\n    with open(test_filename, 'rb') as f:\n        test_loader = pickle.load(f)\n\n    return train_loader.dataset,test_loader.dataset\n</code></pre>"},{"location":"imgraph/datasets/standard/","title":"Standard Dataset","text":""},{"location":"imgraph/datasets/standard/#imgraphdatasetsstandard","title":"<code>imgraph.datasets.standard</code>","text":"<p>Standard dataset wrappers for common image datasets like CIFAR.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.CIFAR100GraphDataset","title":"<code>CIFAR100GraphDataset</code>","text":"<p>               Bases: <code>StandardGraphDataset</code></p> <p>Graph dataset for CIFAR-100.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.CIFAR100GraphDataset--parameters","title":"Parameters","text":"<p>root : str     Root directory for dataset storage preset : str or callable, optional     Graph preset or custom graph builder function, by default 'slic_mean_color' transform : callable, optional     Transform function applied to graphs, by default None pre_transform : callable, optional     Pre-transform function applied to images before creating graphs, by default None force_reload : bool, optional     Whether to force reload the dataset, by default False</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>class CIFAR100GraphDataset(StandardGraphDataset):\n    \"\"\"\n    Graph dataset for CIFAR-100.\n\n    Parameters\n    ----------\n    root : str\n        Root directory for dataset storage\n    preset : str or callable, optional\n        Graph preset or custom graph builder function, by default 'slic_mean_color'\n    transform : callable, optional\n        Transform function applied to graphs, by default None\n    pre_transform : callable, optional\n        Pre-transform function applied to images before creating graphs, by default None\n    force_reload : bool, optional\n        Whether to force reload the dataset, by default False\n    \"\"\"\n\n    def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n        \"\"\"Initialize the CIFAR-100 graph dataset.\"\"\"\n        super(CIFAR100GraphDataset, self).__init__(\n            root, CIFAR100, preset, transform, pre_transform, force_reload\n        )\n\n        # Set class names from CIFAR-100\n        cifar = CIFAR100(root, download=False)\n        self.classes = cifar.classes\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.CIFAR100GraphDataset.__init__","title":"<code>__init__(root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False)</code>","text":"<p>Initialize the CIFAR-100 graph dataset.</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n    \"\"\"Initialize the CIFAR-100 graph dataset.\"\"\"\n    super(CIFAR100GraphDataset, self).__init__(\n        root, CIFAR100, preset, transform, pre_transform, force_reload\n    )\n\n    # Set class names from CIFAR-100\n    cifar = CIFAR100(root, download=False)\n    self.classes = cifar.classes\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.CIFAR10GraphDataset","title":"<code>CIFAR10GraphDataset</code>","text":"<p>               Bases: <code>StandardGraphDataset</code></p> <p>Graph dataset for CIFAR-10.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.CIFAR10GraphDataset--parameters","title":"Parameters","text":"<p>root : str     Root directory for dataset storage preset : str or callable, optional     Graph preset or custom graph builder function, by default 'slic_mean_color' transform : callable, optional     Transform function applied to graphs, by default None pre_transform : callable, optional     Pre-transform function applied to images before creating graphs, by default None force_reload : bool, optional     Whether to force reload the dataset, by default False</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>class CIFAR10GraphDataset(StandardGraphDataset):\n    \"\"\"\n    Graph dataset for CIFAR-10.\n\n    Parameters\n    ----------\n    root : str\n        Root directory for dataset storage\n    preset : str or callable, optional\n        Graph preset or custom graph builder function, by default 'slic_mean_color'\n    transform : callable, optional\n        Transform function applied to graphs, by default None\n    pre_transform : callable, optional\n        Pre-transform function applied to images before creating graphs, by default None\n    force_reload : bool, optional\n        Whether to force reload the dataset, by default False\n    \"\"\"\n\n    def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n        \"\"\"Initialize the CIFAR-10 graph dataset.\"\"\"\n        super(CIFAR10GraphDataset, self).__init__(\n            root, CIFAR10, preset, transform, pre_transform, force_reload\n        )\n\n        # Set class names\n        self.classes = [\n            'airplane', 'automobile', 'bird', 'cat', 'deer',\n            'dog', 'frog', 'horse', 'ship', 'truck'\n        ]\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.CIFAR10GraphDataset.__init__","title":"<code>__init__(root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False)</code>","text":"<p>Initialize the CIFAR-10 graph dataset.</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def __init__(self, root, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n    \"\"\"Initialize the CIFAR-10 graph dataset.\"\"\"\n    super(CIFAR10GraphDataset, self).__init__(\n        root, CIFAR10, preset, transform, pre_transform, force_reload\n    )\n\n    # Set class names\n    self.classes = [\n        'airplane', 'automobile', 'bird', 'cat', 'deer',\n        'dog', 'frog', 'horse', 'ship', 'truck'\n    ]\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset","title":"<code>StandardGraphDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for standard graph datasets.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset--parameters","title":"Parameters","text":"<p>root : str     Root directory for dataset storage dataset_cls : class     Dataset class from torchvision preset : str or callable, optional     Graph preset or custom graph builder function, by default 'slic_mean_color' transform : callable, optional     Transform function applied to graphs, by default None pre_transform : callable, optional     Pre-transform function applied to images before creating graphs, by default None force_reload : bool, optional     Whether to force reload the dataset, by default False</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>class StandardGraphDataset(Dataset):\n    \"\"\"\n    Base class for standard graph datasets.\n\n    Parameters\n    ----------\n    root : str\n        Root directory for dataset storage\n    dataset_cls : class\n        Dataset class from torchvision\n    preset : str or callable, optional\n        Graph preset or custom graph builder function, by default 'slic_mean_color'\n    transform : callable, optional\n        Transform function applied to graphs, by default None\n    pre_transform : callable, optional\n        Pre-transform function applied to images before creating graphs, by default None\n    force_reload : bool, optional\n        Whether to force reload the dataset, by default False\n    \"\"\"\n\n    def __init__(self, root, dataset_cls, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n        \"\"\"Initialize the standard graph dataset.\"\"\"\n        self.dataset_cls = dataset_cls\n        self.preset = preset\n        self.force_reload = force_reload\n\n        # Create graph builder\n        if isinstance(preset, str):\n            if preset == 'slic_mean_color':\n                self.graph_builder = GraphPresets.slic_mean_color()\n            elif preset == 'slic_color_position':\n                self.graph_builder = GraphPresets.slic_color_position()\n            elif preset == 'patches_color':\n                self.graph_builder = GraphPresets.patches_color()\n            elif preset == 'tiny_graph':\n                self.graph_builder = GraphPresets.tiny_graph()\n            elif preset == 'superpixel_comprehensive':\n                self.graph_builder = GraphPresets.superpixel_comprehensive()\n            else:\n                raise ValueError(f\"Unknown preset: {preset}\")\n        else:\n            # Custom graph builder function\n            self.graph_builder = preset\n\n        # Initialize dataset\n        super(StandardGraphDataset, self).__init__(root, transform, pre_transform)\n\n        # Load processed data\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        \"\"\"List of raw file names.\"\"\"\n        return []\n\n    @property\n    def processed_file_names(self):\n        \"\"\"List of processed file names.\"\"\"\n        dataset_name = self.dataset_cls.__name__.lower()\n        return [f'{dataset_name}_graphs_{self.preset}.pt']\n\n    def download(self):\n        \"\"\"Download the dataset.\"\"\"\n        # Download using torchvision\n        self.dataset_cls(self.raw_dir, train=True, download=True)\n        self.dataset_cls(self.raw_dir, train=False, download=True)\n\n    def process(self):\n        \"\"\"Process the dataset into graphs.\"\"\"\n        # Check if processed file exists and force_reload is False\n        if os.path.exists(self.processed_paths[0]) and not self.force_reload:\n            return\n\n        # Load dataset\n        train_dataset = self.dataset_cls(self.raw_dir, train=True, download=True)\n        test_dataset = self.dataset_cls(self.raw_dir, train=False, download=True)\n\n        # Store class names\n        if hasattr(train_dataset, 'classes'):\n            self.classes = train_dataset.classes\n\n        # Combine train and test datasets\n        data_list = []\n\n        # Process training data\n        print(f\"Processing {self.dataset_cls.__name__} training data...\")\n        for img, label in tqdm(train_dataset):\n            # Convert to numpy array\n            img_np = np.array(img)\n\n            # Pre-transform if available\n            if self.pre_transform is not None:\n                img_np = self.pre_transform(img_np)\n\n            # Convert to graph\n            try:\n                graph = self.graph_builder(img_np)\n\n                # Add label\n                graph.y = torch.tensor(label, dtype=torch.long)\n\n                # Add to data list\n                data_list.append(graph)\n            except Exception as e:\n                print(f\"Error processing image: {e}\")\n\n        # Process test data\n        print(f\"Processing {self.dataset_cls.__name__} test data...\")\n        for img, label in tqdm(test_dataset):\n            # Convert to numpy array\n            img_np = np.array(img)\n\n            # Pre-transform if available\n            if self.pre_transform is not None:\n                img_np = self.pre_transform(img_np)\n\n            # Convert to graph\n            try:\n                graph = self.graph_builder(img_np)\n\n                # Add label\n                graph.y = torch.tensor(label, dtype=torch.long)\n\n                # Add metadata for train/test split\n                graph.is_test = torch.tensor([True], dtype=torch.bool)\n\n                # Add to data list\n                data_list.append(graph)\n            except Exception as e:\n                print(f\"Error processing image: {e}\")\n\n        # Save processed data\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])\n\n    def len(self):\n        \"\"\"\n        Get the number of graphs in the dataset.\n\n        Returns\n        -------\n        int\n            Number of graphs\n        \"\"\"\n        return self.data.y.size(0)\n\n    def get(self, idx):\n        \"\"\"\n        Get a graph by index.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the graph\n\n        Returns\n        -------\n        torch_geometric.data.Data\n            Graph data object\n        \"\"\"\n        data = Data()\n        for key in self.data.keys:\n            item, slices = self.data[key], self.slices[key]\n            start, end = slices[idx].item(), slices[idx + 1].item()\n            if torch.is_tensor(item):\n                s = list(item.size())\n                if len(s) &gt; 0:\n                    s[0] = end - start\n                    data[key] = item[start:end].view(s)\n                else:\n                    data[key] = item[start:end]\n            elif start &lt; end:\n                data[key] = item[start:end]\n        return data\n\n    def visualize(self, idx):\n        \"\"\"\n        Visualize a graph.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the graph to visualize\n\n        Returns\n        -------\n        matplotlib.figure.Figure\n            Figure with the visualization\n        \"\"\"\n        # Get graph\n        graph = self.get(idx)\n\n        # Get image from dataset\n        is_train = not hasattr(graph, 'is_test') or not graph.is_test.item()\n        dataset = self.dataset_cls(self.raw_dir, train=is_train, download=False)\n\n        # Calculate index in the original dataset\n        if is_train:\n            orig_idx = idx\n        else:\n            # Count training samples\n            train_dataset = self.dataset_cls(self.raw_dir, train=True, download=False)\n            orig_idx = idx - len(train_dataset)\n\n        # Get image\n        img, label = dataset[orig_idx]\n        img_np = np.array(img)\n\n        # Create figure\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n        # Display original image\n        ax1.imshow(img_np)\n        class_name = self.classes[label] if hasattr(self, 'classes') else str(label)\n        ax1.set_title(f\"Original Image (Class: {class_name})\")\n        ax1.axis('off')\n\n        # Display graph\n        if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n            node_positions = graph.node_info['centroids']\n\n            # Display image in background\n            ax2.imshow(img_np)\n\n            # Plot nodes\n            ax2.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n            # Plot edges\n            edge_index = graph.edge_index.cpu().numpy()\n            for i in range(edge_index.shape[1]):\n                src_idx = edge_index[0, i]\n                dst_idx = edge_index[1, i]\n\n                src_pos = node_positions[src_idx]\n                dst_pos = node_positions[dst_idx]\n\n                ax2.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n            ax2.set_title(f\"Graph: {graph.num_nodes} nodes, {graph.edge_index.shape[1]} edges\")\n            ax2.axis('off')\n        else:\n            ax2.text(0.5, 0.5, \"Graph does not contain node positions\", ha='center', va='center')\n            ax2.axis('off')\n\n        return fig\n\n    def get_train_test_split(self):\n        \"\"\"\n        Split the dataset into train and test sets.\n\n        Returns\n        -------\n        tuple\n            (train_dataset, test_dataset)\n        \"\"\"\n        from torch.utils.data import Subset\n\n        # Get indices\n        train_indices = []\n        test_indices = []\n\n        for i in range(len(self)):\n            graph = self.get(i)\n            if hasattr(graph, 'is_test') and graph.is_test.item():\n                test_indices.append(i)\n            else:\n                train_indices.append(i)\n\n        # Create datasets\n        train_dataset = Subset(self, train_indices)\n        test_dataset = Subset(self, test_indices)\n\n        return train_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.processed_file_names","title":"<code>processed_file_names</code>  <code>property</code>","text":"<p>List of processed file names.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.raw_file_names","title":"<code>raw_file_names</code>  <code>property</code>","text":"<p>List of raw file names.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.__init__","title":"<code>__init__(root, dataset_cls, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False)</code>","text":"<p>Initialize the standard graph dataset.</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def __init__(self, root, dataset_cls, preset='slic_mean_color', transform=None, pre_transform=None, force_reload=False):\n    \"\"\"Initialize the standard graph dataset.\"\"\"\n    self.dataset_cls = dataset_cls\n    self.preset = preset\n    self.force_reload = force_reload\n\n    # Create graph builder\n    if isinstance(preset, str):\n        if preset == 'slic_mean_color':\n            self.graph_builder = GraphPresets.slic_mean_color()\n        elif preset == 'slic_color_position':\n            self.graph_builder = GraphPresets.slic_color_position()\n        elif preset == 'patches_color':\n            self.graph_builder = GraphPresets.patches_color()\n        elif preset == 'tiny_graph':\n            self.graph_builder = GraphPresets.tiny_graph()\n        elif preset == 'superpixel_comprehensive':\n            self.graph_builder = GraphPresets.superpixel_comprehensive()\n        else:\n            raise ValueError(f\"Unknown preset: {preset}\")\n    else:\n        # Custom graph builder function\n        self.graph_builder = preset\n\n    # Initialize dataset\n    super(StandardGraphDataset, self).__init__(root, transform, pre_transform)\n\n    # Load processed data\n    self.data, self.slices = torch.load(self.processed_paths[0])\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.download","title":"<code>download()</code>","text":"<p>Download the dataset.</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def download(self):\n    \"\"\"Download the dataset.\"\"\"\n    # Download using torchvision\n    self.dataset_cls(self.raw_dir, train=True, download=True)\n    self.dataset_cls(self.raw_dir, train=False, download=True)\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.get","title":"<code>get(idx)</code>","text":"<p>Get a graph by index.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.get--parameters","title":"Parameters","text":"<p>idx : int     Index of the graph</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.get--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph data object</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def get(self, idx):\n    \"\"\"\n    Get a graph by index.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the graph\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph data object\n    \"\"\"\n    data = Data()\n    for key in self.data.keys:\n        item, slices = self.data[key], self.slices[key]\n        start, end = slices[idx].item(), slices[idx + 1].item()\n        if torch.is_tensor(item):\n            s = list(item.size())\n            if len(s) &gt; 0:\n                s[0] = end - start\n                data[key] = item[start:end].view(s)\n            else:\n                data[key] = item[start:end]\n        elif start &lt; end:\n            data[key] = item[start:end]\n    return data\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.get_train_test_split","title":"<code>get_train_test_split()</code>","text":"<p>Split the dataset into train and test sets.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.get_train_test_split--returns","title":"Returns","text":"<p>tuple     (train_dataset, test_dataset)</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def get_train_test_split(self):\n    \"\"\"\n    Split the dataset into train and test sets.\n\n    Returns\n    -------\n    tuple\n        (train_dataset, test_dataset)\n    \"\"\"\n    from torch.utils.data import Subset\n\n    # Get indices\n    train_indices = []\n    test_indices = []\n\n    for i in range(len(self)):\n        graph = self.get(i)\n        if hasattr(graph, 'is_test') and graph.is_test.item():\n            test_indices.append(i)\n        else:\n            train_indices.append(i)\n\n    # Create datasets\n    train_dataset = Subset(self, train_indices)\n    test_dataset = Subset(self, test_indices)\n\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.len","title":"<code>len()</code>","text":"<p>Get the number of graphs in the dataset.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.len--returns","title":"Returns","text":"<p>int     Number of graphs</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def len(self):\n    \"\"\"\n    Get the number of graphs in the dataset.\n\n    Returns\n    -------\n    int\n        Number of graphs\n    \"\"\"\n    return self.data.y.size(0)\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.process","title":"<code>process()</code>","text":"<p>Process the dataset into graphs.</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def process(self):\n    \"\"\"Process the dataset into graphs.\"\"\"\n    # Check if processed file exists and force_reload is False\n    if os.path.exists(self.processed_paths[0]) and not self.force_reload:\n        return\n\n    # Load dataset\n    train_dataset = self.dataset_cls(self.raw_dir, train=True, download=True)\n    test_dataset = self.dataset_cls(self.raw_dir, train=False, download=True)\n\n    # Store class names\n    if hasattr(train_dataset, 'classes'):\n        self.classes = train_dataset.classes\n\n    # Combine train and test datasets\n    data_list = []\n\n    # Process training data\n    print(f\"Processing {self.dataset_cls.__name__} training data...\")\n    for img, label in tqdm(train_dataset):\n        # Convert to numpy array\n        img_np = np.array(img)\n\n        # Pre-transform if available\n        if self.pre_transform is not None:\n            img_np = self.pre_transform(img_np)\n\n        # Convert to graph\n        try:\n            graph = self.graph_builder(img_np)\n\n            # Add label\n            graph.y = torch.tensor(label, dtype=torch.long)\n\n            # Add to data list\n            data_list.append(graph)\n        except Exception as e:\n            print(f\"Error processing image: {e}\")\n\n    # Process test data\n    print(f\"Processing {self.dataset_cls.__name__} test data...\")\n    for img, label in tqdm(test_dataset):\n        # Convert to numpy array\n        img_np = np.array(img)\n\n        # Pre-transform if available\n        if self.pre_transform is not None:\n            img_np = self.pre_transform(img_np)\n\n        # Convert to graph\n        try:\n            graph = self.graph_builder(img_np)\n\n            # Add label\n            graph.y = torch.tensor(label, dtype=torch.long)\n\n            # Add metadata for train/test split\n            graph.is_test = torch.tensor([True], dtype=torch.bool)\n\n            # Add to data list\n            data_list.append(graph)\n        except Exception as e:\n            print(f\"Error processing image: {e}\")\n\n    # Save processed data\n    data, slices = self.collate(data_list)\n    torch.save((data, slices), self.processed_paths[0])\n</code></pre>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.visualize","title":"<code>visualize(idx)</code>","text":"<p>Visualize a graph.</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.visualize--parameters","title":"Parameters","text":"<p>idx : int     Index of the graph to visualize</p>"},{"location":"imgraph/datasets/standard/#imgraph.datasets.standard.StandardGraphDataset.visualize--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/datasets/standard.py</code> <pre><code>def visualize(self, idx):\n    \"\"\"\n    Visualize a graph.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the graph to visualize\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Get graph\n    graph = self.get(idx)\n\n    # Get image from dataset\n    is_train = not hasattr(graph, 'is_test') or not graph.is_test.item()\n    dataset = self.dataset_cls(self.raw_dir, train=is_train, download=False)\n\n    # Calculate index in the original dataset\n    if is_train:\n        orig_idx = idx\n    else:\n        # Count training samples\n        train_dataset = self.dataset_cls(self.raw_dir, train=True, download=False)\n        orig_idx = idx - len(train_dataset)\n\n    # Get image\n    img, label = dataset[orig_idx]\n    img_np = np.array(img)\n\n    # Create figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Display original image\n    ax1.imshow(img_np)\n    class_name = self.classes[label] if hasattr(self, 'classes') else str(label)\n    ax1.set_title(f\"Original Image (Class: {class_name})\")\n    ax1.axis('off')\n\n    # Display graph\n    if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n        node_positions = graph.node_info['centroids']\n\n        # Display image in background\n        ax2.imshow(img_np)\n\n        # Plot nodes\n        ax2.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=10, alpha=0.7)\n\n        # Plot edges\n        edge_index = graph.edge_index.cpu().numpy()\n        for i in range(edge_index.shape[1]):\n            src_idx = edge_index[0, i]\n            dst_idx = edge_index[1, i]\n\n            src_pos = node_positions[src_idx]\n            dst_pos = node_positions[dst_idx]\n\n            ax2.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n        ax2.set_title(f\"Graph: {graph.num_nodes} nodes, {graph.edge_index.shape[1]} edges\")\n        ax2.axis('off')\n    else:\n        ax2.text(0.5, 0.5, \"Graph does not contain node positions\", ha='center', va='center')\n        ax2.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/datasets/url_config/","title":"URL Config","text":""},{"location":"imgraph/datasets/url_config/#imgraphdatasetsurl_config","title":"<code>imgraph.datasets.url_config</code>","text":""},{"location":"imgraph/examples/basic_graph_creation/","title":"Basic Graph Creation","text":""},{"location":"imgraph/examples/basic_graph_creation/#imgraphexamplesbasic_graph_creation","title":"<code>imgraph.examples.basic_graph_creation</code>","text":"<p>Example script for basic graph creation and visualization with robust error handling.</p>"},{"location":"imgraph/examples/basic_graph_creation/#imgraph.examples.basic_graph_creation.create_test_image","title":"<code>create_test_image(width=300, height=300)</code>","text":"<p>Create a synthetic test image with various shapes.</p> Source code in <code>imgraph/examples/basic_graph_creation.py</code> <pre><code>def create_test_image(width=300, height=300):\n    \"\"\"Create a synthetic test image with various shapes.\"\"\"\n    image = np.ones((height, width, 3), dtype=np.uint8) * 255\n\n    # Draw some shapes\n    cv2.circle(image, (width//2, height//2), min(width, height)//3, (255, 0, 0), -1)\n    cv2.rectangle(image, (width//6, height//6), (width*5//6, height*5//6), (0, 255, 0), 5)\n    cv2.line(image, (0, 0), (width, height), (0, 0, 255), 3)\n\n    return image\n</code></pre>"},{"location":"imgraph/examples/comprehensive_graph_creation/","title":"Comprehensive Graph Creation","text":""},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraphexamplescomprehensive_graph_creation","title":"<code>imgraph.examples.comprehensive_graph_creation</code>","text":"<p>Comprehensive example for creating and analyzing graphs from images with custom configurations.</p>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.analyze_graphs","title":"<code>analyze_graphs(image, graphs, names)</code>","text":"<p>Analyzes and visualizes graphs.</p>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.analyze_graphs--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image graphs : list     List of graph data names : list     List of graph names</p> Source code in <code>imgraph/examples/comprehensive_graph_creation.py</code> <pre><code>def analyze_graphs(image, graphs, names):\n    \"\"\"\n    Analyzes and visualizes graphs.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    graphs : list\n        List of graph data\n    names : list\n        List of graph names\n    \"\"\"\n    os.makedirs(\"outputs/analysis\", exist_ok=True)\n\n    for i, (graph, name) in enumerate(zip(graphs, names)):\n        print(f\"Analyzing {name} graph...\")\n\n        try:\n            # Visualize graph with node feature heatmap\n            if graph.x is not None and graph.x.shape[1] &gt; 0:\n                for feature_idx in range(min(3, graph.x.shape[1])):\n                    fig = visualize_graph_with_features(image, graph, node_feature_idx=feature_idx)\n                    fig.suptitle(f\"{name} - Node Feature {feature_idx}\")\n                    fig.savefig(f\"outputs/analysis/{name}_node_feature_{feature_idx}.png\")\n                    plt.close(fig)  # Close figure to free memory\n\n            # Visualize graph with edge feature heatmap\n            if hasattr(graph, 'edge_attr') and graph.edge_attr is not None and graph.edge_attr.shape[1] &gt; 0:\n                for feature_idx in range(min(2, graph.edge_attr.shape[1])):\n                    fig = visualize_graph_with_features(image, graph, edge_feature_idx=feature_idx)\n                    fig.suptitle(f\"{name} - Edge Feature {feature_idx}\")\n                    fig.savefig(f\"outputs/analysis/{name}_edge_feature_{feature_idx}.png\")\n                    plt.close(fig)  # Close figure to free memory\n\n            # Plot node feature distributions\n            if graph.x is not None and graph.x.shape[1] &gt; 0:\n                fig = plot_node_feature_distribution(graph)\n                fig.suptitle(f\"{name} - Node Feature Distribution\")\n                fig.savefig(f\"outputs/analysis/{name}_node_feature_distribution.png\")\n                plt.close(fig)  # Close figure to free memory\n        except Exception as e:\n            print(f\"  Warning: Error visualizing {name} graph: {e}\")\n\n        # Print graph statistics\n        print(f\"  Nodes: {graph.num_nodes}\")\n        print(f\"  Edges: {graph.edge_index.shape[1]}\")\n        print(f\"  Node feature dimensions: {graph.x.shape[1]}\")\n        if hasattr(graph, 'edge_attr') and graph.edge_attr is not None:\n            print(f\"  Edge feature dimensions: {graph.edge_attr.shape[1]}\")\n        print()\n</code></pre>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.create_custom_graph","title":"<code>create_custom_graph(image)</code>","text":"<p>Creates a custom graph with specific configuration.</p>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.create_custom_graph--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image</p>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.create_custom_graph--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph data</p> Source code in <code>imgraph/examples/comprehensive_graph_creation.py</code> <pre><code>def create_custom_graph(image):\n    \"\"\"\n    Creates a custom graph with specific configuration.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph data\n    \"\"\"\n    # Create custom configuration\n    config = {\n        'node_creation': {\n            'method': 'slic_superpixel_nodes',\n            'params': {\n                'n_segments': 75,\n                'compactness': 15,\n                'sigma': 1.0\n            }\n        },\n        'node_features': {\n            'method': 'mean_std_color_features',\n            'params': {\n                'color_space': 'hsv',\n                'normalize': True\n            }\n        },\n        'edge_creation': {\n            'method': 'region_adjacency_edges',\n            'params': {\n                'connectivity': 2\n            }\n        },\n        'edge_features': {\n            'method': 'color_difference',\n            'params': {\n                'color_space': 'hsv',\n                'normalize': True\n            }\n        }\n    }\n\n    # Create graph pipeline\n    pipeline = GraphPipeline(config)\n\n    # Process image\n    return pipeline.process(image)\n</code></pre>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.create_multi_feature_graph","title":"<code>create_multi_feature_graph(image)</code>","text":"<p>Creates a graph with multiple node and edge features.</p>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.create_multi_feature_graph--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image</p>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.create_multi_feature_graph--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph data</p> Source code in <code>imgraph/examples/comprehensive_graph_creation.py</code> <pre><code>def create_multi_feature_graph(image):\n    \"\"\"\n    Creates a graph with multiple node and edge features.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph data\n    \"\"\"\n    # Create nodes\n    node_info = slic_superpixel_nodes(image, n_segments=50, compactness=10)\n\n    # Extract multiple node features\n    color_features = mean_std_color_features(image, node_info)\n    texture_features = lbp_features(image, node_info)\n    position_features = normalized_position_features(image, node_info)\n\n    # Combine node features\n    node_features = torch.cat([color_features, texture_features, position_features], dim=1)\n\n    # Create edges\n    edge_index = region_adjacency_edges(node_info)\n\n    # Extract edge features\n    edge_feat1 = color_difference(image, node_info, edge_index)\n    edge_feat2 = boundary_strength(image, node_info, edge_index)\n\n    # Combine edge features\n    edge_attr = torch.cat([edge_feat1, edge_feat2], dim=1)\n\n    # Create graph data\n    from torch_geometric.data import Data\n\n    graph_data = Data(\n        x=node_features,\n        edge_index=edge_index,\n        edge_attr=edge_attr,\n        num_nodes=len(node_features),\n        image_size=torch.tensor(image.shape[:2])\n    )\n\n    # Store node info for visualization\n    graph_data.node_info = node_info\n\n    return graph_data\n</code></pre>"},{"location":"imgraph/examples/comprehensive_graph_creation/#imgraph.examples.comprehensive_graph_creation.create_test_image","title":"<code>create_test_image(width=300, height=300)</code>","text":"<p>Create a synthetic test image with various shapes.</p> Source code in <code>imgraph/examples/comprehensive_graph_creation.py</code> <pre><code>def create_test_image(width=300, height=300):\n    \"\"\"Create a synthetic test image with various shapes.\"\"\"\n    image = np.ones((height, width, 3), dtype=np.uint8) * 255\n\n    # Draw some shapes\n    cv2.rectangle(image, (50, 50), (100, 100), (255, 0, 0), -1)\n    cv2.rectangle(image, (150, 50), (200, 100), (0, 255, 0), -1)\n    cv2.rectangle(image, (100, 150), (200, 200), (0, 0, 255), -1)\n    cv2.circle(image, (75, 200), 25, (255, 255, 0), -1)\n    cv2.circle(image, (200, 75), 25, (255, 0, 255), -1)\n\n    return image\n</code></pre>"},{"location":"imgraph/examples/training_image_folder_example/","title":"Training on Folder","text":""},{"location":"imgraph/examples/training_image_folder_example/#imgraphexamplestraining_image_folder_example","title":"<code>imgraph.examples.training_image_folder_example</code>","text":"<p>Example script for training GNN models on a folder of images.</p> <p>This script demonstrates: 1. How to use ImageFolderGraphDataset to create graphs from images 2. How to train and evaluate different GNN models (GCN, GAT, GIN, GraphSAGE)</p> Usage <p>python training_image_folder_example.py --input_dir  [options] Options <p>--input_dir: Directory containing images organized in class folders --output_dir: Directory for saving processed graphs --results_dir: Directory for saving training results --model: GNN model to train (gcn, gat, gin, sage) --preset: Graph representation preset (see choices below) --batch_size: Batch size for training --epochs: Number of training epochs --lr: Learning rate --patience: Early stopping patience --random_seed: Random seed for reproducibility</p>"},{"location":"imgraph/examples/training_image_folder_example/#imgraph.examples.training_image_folder_example.prepare_image_folder_structure","title":"<code>prepare_image_folder_structure(input_dir, dataset_dir)</code>","text":"<p>Prepares the dataset directory structure.</p> Source code in <code>imgraph/examples/training_image_folder_example.py</code> <pre><code>def prepare_image_folder_structure(input_dir, dataset_dir):\n    \"\"\"Prepares the dataset directory structure.\"\"\"\n    # Create raw and processed directories\n    raw_dir = os.path.join(dataset_dir, 'raw')\n    processed_dir = os.path.join(dataset_dir, 'processed')\n    os.makedirs(raw_dir, exist_ok=True)\n    os.makedirs(processed_dir, exist_ok=True)\n\n    # Check if input has train/test/val structure\n    if os.path.isdir(os.path.join(input_dir, 'train')):\n        # Find all class names\n        class_names = set()\n        for split in ['train', 'test', 'val']:\n            split_dir = os.path.join(input_dir, split)\n            if os.path.isdir(split_dir):\n                for cls in os.listdir(split_dir):\n                    cls_dir = os.path.join(split_dir, cls)\n                    if os.path.isdir(cls_dir):\n                        class_names.add(cls)\n\n        # Create class directories\n        for cls in class_names:\n            os.makedirs(os.path.join(raw_dir, cls), exist_ok=True)\n\n        # Create symbolic links or copy files\n        for split in ['train', 'test', 'val']:\n            split_dir = os.path.join(input_dir, split)\n            if os.path.isdir(split_dir):\n                for cls in class_names:\n                    cls_dir = os.path.join(split_dir, cls)\n                    if os.path.isdir(cls_dir):\n                        for img in os.listdir(cls_dir):\n                            src_path = os.path.join(cls_dir, img)\n                            if os.path.isfile(src_path):\n                                # Create a unique filename to avoid collisions\n                                dst_name = f\"{split}_{img}\"\n                                dst_path = os.path.join(raw_dir, cls, dst_name)\n\n                                if not os.path.exists(dst_path):\n                                    try:\n                                        os.symlink(os.path.abspath(src_path), dst_path)\n                                    except OSError:\n                                        shutil.copy2(src_path, dst_path)\n\n        print(f\"Prepared dataset with {len(class_names)} classes\")\n    else:\n        # Direct copy/link of the directory structure\n        for item in os.listdir(input_dir):\n            src_path = os.path.join(input_dir, item)\n            if os.path.isdir(src_path):\n                dst_path = os.path.join(raw_dir, item)\n                if not os.path.exists(dst_path):\n                    try:\n                        os.symlink(os.path.abspath(src_path), dst_path)\n                    except OSError:\n                        # Create directory and copy files\n                        os.makedirs(dst_path, exist_ok=True)\n                        for file in os.listdir(src_path):\n                            file_path = os.path.join(src_path, file)\n                            if os.path.isfile(file_path):\n                                shutil.copy2(file_path, os.path.join(dst_path, file))\n</code></pre>"},{"location":"imgraph/examples/training_synthetic_example/","title":"Training on Synthetic","text":""},{"location":"imgraph/examples/training_synthetic_example/#imgraphexamplestraining_synthetic_example","title":"<code>imgraph.examples.training_synthetic_example</code>","text":"<p>Example script for training GNN models on synthetic data.</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.create_graph_dataset","title":"<code>create_graph_dataset(images, labels, preset='slic_mean_color')</code>","text":"<p>Creates a graph dataset from images.</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.create_graph_dataset--parameters","title":"Parameters","text":"<p>images : numpy.ndarray     Array of images labels : numpy.ndarray     Array of labels preset : str, optional     Graph preset to use, by default 'slic_mean_color'</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.create_graph_dataset--returns","title":"Returns","text":"<p>list     List of graph data objects</p> Source code in <code>imgraph/examples/training_synthetic_example.py</code> <pre><code>def create_graph_dataset(images, labels, preset='slic_mean_color'):\n    \"\"\"\n    Creates a graph dataset from images.\n\n    Parameters\n    ----------\n    images : numpy.ndarray\n        Array of images\n    labels : numpy.ndarray\n        Array of labels\n    preset : str, optional\n        Graph preset to use, by default 'slic_mean_color'\n\n    Returns\n    -------\n    list\n        List of graph data objects\n    \"\"\"\n    # Create graph builder\n    if preset == 'slic_mean_color':\n        graph_builder = GraphPresets.slic_mean_color()\n    elif preset == 'slic_color_position':\n        graph_builder = GraphPresets.slic_color_position()\n    elif preset == 'patches_color':\n        graph_builder = GraphPresets.patches_color()\n    elif preset == 'tiny_graph':\n        graph_builder = GraphPresets.tiny_graph()\n    elif preset == 'superpixel_comprehensive':\n        graph_builder = GraphPresets.superpixel_comprehensive()\n    else:\n        raise ValueError(f\"Unknown preset: {preset}\")\n\n    # Create graphs\n    graphs = []\n    for i, (image, label) in enumerate(tqdm(zip(images, labels), total=len(images), desc=\"Creating graphs\")):\n        # Convert image to graph\n        graph = graph_builder(image)\n\n        # Add label\n        graph.y = torch.tensor(label, dtype=torch.long)\n\n        graphs.append(graph)\n\n    return graphs\n</code></pre>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.create_synthetic_dataset","title":"<code>create_synthetic_dataset(n_samples=100, image_size=64, shapes=None, colors=None)</code>","text":"<p>Creates a synthetic dataset of images with different shapes.</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.create_synthetic_dataset--parameters","title":"Parameters","text":"<p>n_samples : int, optional     Number of samples to generate, by default 100 image_size : int, optional     Size of the images, by default 64 shapes : list, optional     List of shapes to use, by default None     Options: 'circle', 'rectangle', 'triangle', 'cross', 'ellipse' colors : list, optional     List of colors to use, by default None</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.create_synthetic_dataset--returns","title":"Returns","text":"<p>tuple     (images, labels, class_names)</p> Source code in <code>imgraph/examples/training_synthetic_example.py</code> <pre><code>def create_synthetic_dataset(n_samples=100, image_size=64, shapes=None, colors=None):\n    \"\"\"\n    Creates a synthetic dataset of images with different shapes.\n\n    Parameters\n    ----------\n    n_samples : int, optional\n        Number of samples to generate, by default 100\n    image_size : int, optional\n        Size of the images, by default 64\n    shapes : list, optional\n        List of shapes to use, by default None\n        Options: 'circle', 'rectangle', 'triangle', 'cross', 'ellipse'\n    colors : list, optional\n        List of colors to use, by default None\n\n    Returns\n    -------\n    tuple\n        (images, labels, class_names)\n    \"\"\"\n    # Default shapes and colors\n    if shapes is None:\n        shapes = ['circle', 'rectangle', 'triangle', 'cross', 'ellipse']\n\n    if colors is None:\n        colors = [\n            (255, 0, 0),    # Red\n            (0, 255, 0),    # Green\n            (0, 0, 255),    # Blue\n            (255, 255, 0),  # Yellow\n            (255, 0, 255)   # Magenta\n        ]\n\n    # Generate samples\n    images = []\n    labels = []\n\n    for i in range(n_samples):\n        # Create blank image\n        img = np.ones((image_size, image_size, 3), dtype=np.uint8) * 255\n\n        # Choose a random shape\n        shape_idx = np.random.randint(0, len(shapes))\n        shape = shapes[shape_idx]\n\n        # Choose a random color\n        color = colors[np.random.randint(0, len(colors))]\n\n        # Calculate center and size\n        center = (np.random.randint(image_size // 4, 3 * image_size // 4),\n                  np.random.randint(image_size // 4, 3 * image_size // 4))\n        size = np.random.randint(image_size // 6, image_size // 3)\n\n        # Draw shape\n        if shape == 'circle':\n            cv2.circle(img, center, size, color, -1)\n        elif shape == 'rectangle':\n            top_left = (center[0] - size, center[1] - size)\n            bottom_right = (center[0] + size, center[1] + size)\n            cv2.rectangle(img, top_left, bottom_right, color, -1)\n        elif shape == 'triangle':\n            points = np.array([\n                [center[0], center[1] - size],\n                [center[0] - size, center[1] + size],\n                [center[0] + size, center[1] + size]\n            ], np.int32)\n            cv2.fillPoly(img, [points], color)\n        elif shape == 'cross':\n            thickness = size // 3\n            cv2.line(img, (center[0] - size, center[1]), (center[0] + size, center[1]), color, thickness)\n            cv2.line(img, (center[0], center[1] - size), (center[0], center[1] + size), color, thickness)\n        elif shape == 'ellipse':\n            axes = (size, size // 2)\n            angle = np.random.randint(0, 180)\n            cv2.ellipse(img, center, axes, angle, 0, 360, color, -1)\n\n        # Add noise\n        noise = np.random.randint(0, 20, img.shape).astype(np.uint8)\n        img = cv2.add(img, noise)\n\n        # Add to dataset\n        images.append(img)\n        labels.append(shape_idx)\n\n    return np.array(images), np.array(labels), shapes\n</code></pre>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.evaluate_model","title":"<code>evaluate_model(model, loader, device)</code>","text":"<p>Evaluates the model on the given data loader.</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.evaluate_model--parameters","title":"Parameters","text":"<p>model : torch.nn.Module     Model to evaluate loader : torch_geometric.loader.DataLoader     Data loader device : torch.device     Device to evaluate on</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.evaluate_model--returns","title":"Returns","text":"<p>tuple     (accuracy, predictions, targets)</p> Source code in <code>imgraph/examples/training_synthetic_example.py</code> <pre><code>def evaluate_model(model, loader, device):\n    \"\"\"\n    Evaluates the model on the given data loader.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to evaluate\n    loader : torch_geometric.loader.DataLoader\n        Data loader\n    device : torch.device\n        Device to evaluate on\n\n    Returns\n    -------\n    tuple\n        (accuracy, predictions, targets)\n    \"\"\"\n    model.eval()\n    predictions = []\n    targets = []\n\n    with torch.no_grad():\n        for data in loader:\n            data = data.to(device)\n            out = model(data)\n            pred = out.argmax(dim=1)\n\n            predictions.extend(pred.cpu().numpy())\n            targets.extend(data.y.cpu().numpy())\n\n    accuracy = accuracy_score(targets, predictions)\n\n    return accuracy, predictions, targets\n</code></pre>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.plot_results","title":"<code>plot_results(history, predictions, targets, class_names, output_dir='results')</code>","text":"<p>Plots the training history and confusion matrix.</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.plot_results--parameters","title":"Parameters","text":"<p>history : dict     Training history predictions : list     Model predictions targets : list     Ground truth targets class_names : list     List of class names output_dir : str, optional     Output directory, by default 'results'</p> Source code in <code>imgraph/examples/training_synthetic_example.py</code> <pre><code>def plot_results(history, predictions, targets, class_names, output_dir='results'):\n    \"\"\"\n    Plots the training history and confusion matrix.\n\n    Parameters\n    ----------\n    history : dict\n        Training history\n    predictions : list\n        Model predictions\n    targets : list\n        Ground truth targets\n    class_names : list\n        List of class names\n    output_dir : str, optional\n        Output directory, by default 'results'\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Plot training history\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train')\n    plt.plot(history['val_loss'], label='Validation')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_acc'], label='Train')\n    plt.plot(history['val_acc'], label='Validation')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, 'training_history.png'))\n\n    # Plot confusion matrix\n    cm = confusion_matrix(targets, predictions)\n    plt.figure(figsize=(10, 8))\n\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n\n    # Print classification report\n    report = classification_report(targets, predictions, target_names=class_names)\n    print(\"\\nClassification Report:\")\n    print(report)\n\n    # Save report to file\n    with open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n        f.write(report)\n</code></pre>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.train_model","title":"<code>train_model(model_name, train_loader, val_loader, num_features, num_classes, device, lr=0.001, epochs=100, patience=10)</code>","text":"<p>Trains a GNN model.</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.train_model--parameters","title":"Parameters","text":"<p>model_name : str     Name of the model to train train_loader : torch_geometric.loader.DataLoader     Training data loader val_loader : torch_geometric.loader.DataLoader     Validation data loader num_features : int     Number of input features num_classes : int     Number of output classes device : torch.device     Device to train on lr : float, optional     Learning rate, by default 0.001 epochs : int, optional     Number of epochs, by default 100 patience : int, optional     Patience for early stopping, by default 10</p>"},{"location":"imgraph/examples/training_synthetic_example/#imgraph.examples.training_synthetic_example.train_model--returns","title":"Returns","text":"<p>tuple     (model, trainer, history)</p> Source code in <code>imgraph/examples/training_synthetic_example.py</code> <pre><code>def train_model(model_name, train_loader, val_loader, num_features, num_classes, \n                device, lr=0.001, epochs=100, patience=10):\n    \"\"\"\n    Trains a GNN model.\n\n    Parameters\n    ----------\n    model_name : str\n        Name of the model to train\n    train_loader : torch_geometric.loader.DataLoader\n        Training data loader\n    val_loader : torch_geometric.loader.DataLoader\n        Validation data loader\n    num_features : int\n        Number of input features\n    num_classes : int\n        Number of output classes\n    device : torch.device\n        Device to train on\n    lr : float, optional\n        Learning rate, by default 0.001\n    epochs : int, optional\n        Number of epochs, by default 100\n    patience : int, optional\n        Patience for early stopping, by default 10\n\n    Returns\n    -------\n    tuple\n        (model, trainer, history)\n    \"\"\"\n    # Create model\n    hidden_dim = 64\n    if model_name.lower() == 'gcn':\n        model = GCN(num_features, hidden_dim, num_classes, num_layers=3)\n    elif model_name.lower() == 'gat':\n        model = GAT(num_features, hidden_dim, num_classes, num_layers=2, heads=4)\n    elif model_name.lower() == 'gin':\n        model = GIN(num_features, hidden_dim, num_classes, num_layers=3)\n    elif model_name.lower() == 'sage':\n        model = GraphSAGE(num_features, hidden_dim, num_classes, num_layers=3)\n    else:\n        raise ValueError(f\"Unknown model: {model_name}\")\n\n    # Move model to device\n    model = model.to(device)\n\n    # Create optimizer and loss function\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    # Create early stopping\n    early_stopping = EarlyStopping(patience=patience, verbose=True)\n\n    # Create trainer\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        criterion=criterion,\n        device=device,\n        early_stopping=early_stopping\n    )\n\n    # Train model\n    history = trainer.fit(train_loader, val_loader, epochs=epochs)\n\n    return model, trainer, history\n</code></pre>"},{"location":"imgraph/models/base/","title":"Base Model","text":""},{"location":"imgraph/models/base/#imgraphmodelsbase","title":"<code>imgraph.models.base</code>","text":"<p>Base model class for graph neural networks.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN","title":"<code>BaseGNN</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for Graph Neural Networks.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean'</p> Source code in <code>imgraph/models/base.py</code> <pre><code>class BaseGNN(nn.Module):\n    \"\"\"\n    Base class for Graph Neural Networks.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean'):\n        \"\"\"Initialize the BaseGNN.\"\"\"\n        super(BaseGNN, self).__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        # Set pooling function\n        if pool == 'mean':\n            self.pool = global_mean_pool\n        elif pool == 'max':\n            self.pool = global_max_pool\n        elif pool == 'sum':\n            self.pool = global_add_pool\n        else:\n            raise ValueError(f\"Unknown pooling method: {pool}\")\n\n        # Initialize layers (to be implemented by subclasses)\n        self.init_layers()\n\n        # Readout layers for graph classification\n        self.readout = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def init_layers(self):\n        \"\"\"Initialize layers (to be implemented by subclasses).\"\"\"\n        raise NotImplementedError(\"Subclasses must implement init_layers method\")\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement forward method\")\n\n    def predict(self, data):\n        \"\"\"\n        Make predictions.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Predicted class probabilities or logits\n        \"\"\"\n        self.eval()\n        with torch.no_grad():\n            return self.forward(data)\n\n    def reset_parameters(self):\n        \"\"\"Reset model parameters.\"\"\"\n        # Reset parameters of all layers\n        for layer in self.modules():\n            if hasattr(layer, 'reset_parameters'):\n                layer.reset_parameters()\n\n    def create_explanations(self, data):\n        \"\"\"\n        Create explanations for model predictions.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        dict\n            Explanation data\n        \"\"\"\n        # Basic implementation - subclasses can override for model-specific explanations\n        try:\n            from torch_geometric.explain import Explainer, ModelConfig\n\n            # Try using PyG's explainer\n            model_config = ModelConfig(\n                mode=\"classification\",\n                task_level=\"graph\",\n                return_type=\"log_probs\"\n            )\n\n            explainer = Explainer(\n                model=self,\n                algorithm=None,  # Will be set during explanation\n                model_config=model_config,\n            )\n\n            # Use GNNExplainer algorithm\n            from torch_geometric.explain.algorithm import GNNExplainer\n            explanation = explainer(\n                data.x, \n                data.edge_index, \n                algorithm=GNNExplainer(), \n                index=0  # Explain first node by default\n            )\n\n            return {\n                \"node_importances\": explanation.node_mask.detach(),\n                \"edge_importances\": explanation.edge_mask.detach(),\n                \"explained_prediction\": explanation.prediction.detach()\n            }\n\n        except (ImportError, Exception) as e:\n            # Fallback to simpler approach if PyG explainer is not available\n            print(f\"Warning: GNN explanation failed ({str(e)}). Using fallback.\")\n\n            # Get node importances using a simple gradient-based approach\n            data = data.clone()\n            data.x.requires_grad_(True)\n            out = self.forward(data)\n\n            if out.shape[0] &gt; 1:  # Graph has multiple nodes or batched\n                # For graph classification, use pooled output\n                pred_class = out.argmax(dim=1)\n                out[range(out.shape[0]), pred_class].sum().backward()\n            else:\n                # For single prediction\n                out.max().backward()\n\n            node_importances = data.x.grad.abs().sum(dim=1)\n\n            return {\n                \"node_importances\": node_importances.detach(),\n                \"explained_prediction\": out.detach()\n            }\n\n    def save(self, path):\n        \"\"\"\n        Save model to file.\n\n        Parameters\n        ----------\n        path : str\n            Path to save model\n        \"\"\"\n        torch.save(self.state_dict(), path)\n\n    def load(self, path):\n        \"\"\"\n        Load model from file.\n\n        Parameters\n        ----------\n        path : str\n            Path to load model from\n        \"\"\"\n        self.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n        self.eval()\n\n    @staticmethod\n    def get_optimizer(model, lr=0.01, weight_decay=5e-4, optimizer='adam'):\n        \"\"\"\n        Get optimizer for the model.\n\n        Parameters\n        ----------\n        model : nn.Module\n            Model to optimize\n        lr : float, optional\n            Learning rate, by default 0.01\n        weight_decay : float, optional\n            Weight decay, by default 5e-4\n        optimizer : str, optional\n            Optimizer type, by default 'adam'\n\n        Returns\n        -------\n        torch.optim.Optimizer\n            Optimizer\n        \"\"\"\n        if optimizer == 'adam':\n            return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n        elif optimizer == 'sgd':\n            return torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n        elif optimizer == 'adamw':\n            return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n        else:\n            raise ValueError(f\"Unknown optimizer: {optimizer}\")\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean')</code>","text":"<p>Initialize the BaseGNN.</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean'):\n    \"\"\"Initialize the BaseGNN.\"\"\"\n    super(BaseGNN, self).__init__()\n\n    self.input_dim = input_dim\n    self.hidden_dim = hidden_dim\n    self.output_dim = output_dim\n    self.num_layers = num_layers\n    self.dropout = dropout\n\n    # Set pooling function\n    if pool == 'mean':\n        self.pool = global_mean_pool\n    elif pool == 'max':\n        self.pool = global_max_pool\n    elif pool == 'sum':\n        self.pool = global_add_pool\n    else:\n        raise ValueError(f\"Unknown pooling method: {pool}\")\n\n    # Initialize layers (to be implemented by subclasses)\n    self.init_layers()\n\n    # Readout layers for graph classification\n    self.readout = nn.Sequential(\n        nn.Linear(hidden_dim, hidden_dim),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim, output_dim)\n    )\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.create_explanations","title":"<code>create_explanations(data)</code>","text":"<p>Create explanations for model predictions.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.create_explanations--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.create_explanations--returns","title":"Returns","text":"<p>dict     Explanation data</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def create_explanations(self, data):\n    \"\"\"\n    Create explanations for model predictions.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    dict\n        Explanation data\n    \"\"\"\n    # Basic implementation - subclasses can override for model-specific explanations\n    try:\n        from torch_geometric.explain import Explainer, ModelConfig\n\n        # Try using PyG's explainer\n        model_config = ModelConfig(\n            mode=\"classification\",\n            task_level=\"graph\",\n            return_type=\"log_probs\"\n        )\n\n        explainer = Explainer(\n            model=self,\n            algorithm=None,  # Will be set during explanation\n            model_config=model_config,\n        )\n\n        # Use GNNExplainer algorithm\n        from torch_geometric.explain.algorithm import GNNExplainer\n        explanation = explainer(\n            data.x, \n            data.edge_index, \n            algorithm=GNNExplainer(), \n            index=0  # Explain first node by default\n        )\n\n        return {\n            \"node_importances\": explanation.node_mask.detach(),\n            \"edge_importances\": explanation.edge_mask.detach(),\n            \"explained_prediction\": explanation.prediction.detach()\n        }\n\n    except (ImportError, Exception) as e:\n        # Fallback to simpler approach if PyG explainer is not available\n        print(f\"Warning: GNN explanation failed ({str(e)}). Using fallback.\")\n\n        # Get node importances using a simple gradient-based approach\n        data = data.clone()\n        data.x.requires_grad_(True)\n        out = self.forward(data)\n\n        if out.shape[0] &gt; 1:  # Graph has multiple nodes or batched\n            # For graph classification, use pooled output\n            pred_class = out.argmax(dim=1)\n            out[range(out.shape[0]), pred_class].sum().backward()\n        else:\n            # For single prediction\n            out.max().backward()\n\n        node_importances = data.x.grad.abs().sum(dim=1)\n\n        return {\n            \"node_importances\": node_importances.detach(),\n            \"explained_prediction\": out.detach()\n        }\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement forward method\")\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.get_optimizer","title":"<code>get_optimizer(model, lr=0.01, weight_decay=0.0005, optimizer='adam')</code>  <code>staticmethod</code>","text":"<p>Get optimizer for the model.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.get_optimizer--parameters","title":"Parameters","text":"<p>model : nn.Module     Model to optimize lr : float, optional     Learning rate, by default 0.01 weight_decay : float, optional     Weight decay, by default 5e-4 optimizer : str, optional     Optimizer type, by default 'adam'</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.get_optimizer--returns","title":"Returns","text":"<p>torch.optim.Optimizer     Optimizer</p> Source code in <code>imgraph/models/base.py</code> <pre><code>@staticmethod\ndef get_optimizer(model, lr=0.01, weight_decay=5e-4, optimizer='adam'):\n    \"\"\"\n    Get optimizer for the model.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Model to optimize\n    lr : float, optional\n        Learning rate, by default 0.01\n    weight_decay : float, optional\n        Weight decay, by default 5e-4\n    optimizer : str, optional\n        Optimizer type, by default 'adam'\n\n    Returns\n    -------\n    torch.optim.Optimizer\n        Optimizer\n    \"\"\"\n    if optimizer == 'adam':\n        return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optimizer == 'sgd':\n        return torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optimizer == 'adamw':\n        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer}\")\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize layers (to be implemented by subclasses).</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize layers (to be implemented by subclasses).\"\"\"\n    raise NotImplementedError(\"Subclasses must implement init_layers method\")\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.load","title":"<code>load(path)</code>","text":"<p>Load model from file.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.load--parameters","title":"Parameters","text":"<p>path : str     Path to load model from</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def load(self, path):\n    \"\"\"\n    Load model from file.\n\n    Parameters\n    ----------\n    path : str\n        Path to load model from\n    \"\"\"\n    self.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n    self.eval()\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.predict","title":"<code>predict(data)</code>","text":"<p>Make predictions.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.predict--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.predict--returns","title":"Returns","text":"<p>torch.Tensor     Predicted class probabilities or logits</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def predict(self, data):\n    \"\"\"\n    Make predictions.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Predicted class probabilities or logits\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        return self.forward(data)\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset model parameters.</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def reset_parameters(self):\n    \"\"\"Reset model parameters.\"\"\"\n    # Reset parameters of all layers\n    for layer in self.modules():\n        if hasattr(layer, 'reset_parameters'):\n            layer.reset_parameters()\n</code></pre>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.save","title":"<code>save(path)</code>","text":"<p>Save model to file.</p>"},{"location":"imgraph/models/base/#imgraph.models.base.BaseGNN.save--parameters","title":"Parameters","text":"<p>path : str     Path to save model</p> Source code in <code>imgraph/models/base.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save model to file.\n\n    Parameters\n    ----------\n    path : str\n        Path to save model\n    \"\"\"\n    torch.save(self.state_dict(), path)\n</code></pre>"},{"location":"imgraph/models/custom_gnn/","title":"CustomGNN","text":""},{"location":"imgraph/models/custom_gnn/#imgraphmodelscustom_gnn","title":"<code>imgraph.models.custom_gnn</code>","text":""},{"location":"imgraph/models/custom_gnn/#imgraph.models.custom_gnn.CustomGNN","title":"<code>CustomGNN</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>A flexible GNN model that accepts arbitrary PyG GNN layers, pooling methods, and custom layers.</p> <p>Compatible with the ImGraph training pipeline and follows the structure of other GNN models in the library.</p>"},{"location":"imgraph/models/custom_gnn/#imgraph.models.custom_gnn.CustomGNN--parameters","title":"Parameters:","text":"<p>num_features : int     Number of input features hidden_dim : int     Number of hidden features num_classes : int     Number of output classes/values num_layers : int     Number of GNN layers gnn_layer_cls : Union[Callable, List[Callable]]     GNN layer class(es) from PyG or custom implementation pooling_method : Union[str, Callable]     Method to pool node features to graph representation     Supported string values: 'mean', 'max', 'sum' dropout : float     Dropout probability activation : Callable     Activation function to use between layers kwargs : Dict     Additional keyword arguments to pass to the GNN layers</p> Source code in <code>imgraph/models/custom_gnn.py</code> <pre><code>class CustomGNN(BaseGNN):\n    \"\"\"\n    A flexible GNN model that accepts arbitrary PyG GNN layers, pooling methods, and custom layers.\n\n    Compatible with the ImGraph training pipeline and follows the structure of other GNN models in the library.\n\n    Parameters:\n    -----------\n    num_features : int\n        Number of input features\n    hidden_dim : int\n        Number of hidden features\n    num_classes : int\n        Number of output classes/values\n    num_layers : int\n        Number of GNN layers\n    gnn_layer_cls : Union[Callable, List[Callable]]\n        GNN layer class(es) from PyG or custom implementation\n    pooling_method : Union[str, Callable]\n        Method to pool node features to graph representation\n        Supported string values: 'mean', 'max', 'sum'\n    dropout : float\n        Dropout probability\n    activation : Callable\n        Activation function to use between layers\n    kwargs : Dict\n        Additional keyword arguments to pass to the GNN layers\n    \"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        hidden_dim: int,\n        num_classes: int,\n        num_layers: int = 3,\n        gnn_layer_cls: Union[Callable, List[Callable]] = None,\n        pooling_method: Union[str, Callable] = 'mean',\n        dropout: float = 0.5,\n        activation: Callable = F.relu,\n        **kwargs\n    ):\n        # Call the parent class constructor with required parameters\n        super().__init__(\n            input_dim=num_features,\n            hidden_dim=hidden_dim,\n            output_dim=num_classes,\n            num_layers=num_layers,\n            dropout=dropout,\n            pool=pooling_method if isinstance(pooling_method, str) else 'mean'\n        )\n\n        self.num_features = num_features\n        self.hidden_dim = hidden_dim\n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.activation = activation\n\n        # Identify if we're using a special pooling layer (TopK, SAG, etc.)\n        self.is_special_pooling = not isinstance(pooling_method, str)\n        self.custom_pooling = None\n        if self.is_special_pooling:\n            self.custom_pooling = pooling_method\n\n        # Handle single layer class or list of layer classes\n        if gnn_layer_cls is None:\n            raise ValueError(\"gnn_layer_cls must be provided\")\n\n        # Convert single layer class to list of the same class\n        if not isinstance(gnn_layer_cls, list):\n            gnn_layer_cls = [gnn_layer_cls] * num_layers\n\n        # Ensure we have enough layer classes\n        if len(gnn_layer_cls) &lt; num_layers:\n            gnn_layer_cls.extend([gnn_layer_cls[-1]] * (num_layers - len(gnn_layer_cls)))\n\n        # Create GNN layers\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # First layer: num_features to hidden_dim\n        self.convs.append(gnn_layer_cls[0](num_features, hidden_dim, **kwargs))\n        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n\n        # Hidden layers: hidden_dim to hidden_dim\n        for i in range(1, num_layers - 1):\n            self.convs.append(gnn_layer_cls[i](hidden_dim, hidden_dim, **kwargs))\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n\n        # Last layer: hidden_dim to hidden_dim (not to num_classes yet)\n        # We'll use the base class's readout layers for the final classification\n        if num_layers &gt; 1:\n            self.convs.append(gnn_layer_cls[-1](hidden_dim, hidden_dim, **kwargs))\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n\n    def init_layers(self):\n        \"\"\"\n        Initialize layers - this is already handled in __init__ so we provide an empty implementation\n        to satisfy the abstract method requirement.\n        \"\"\"\n        pass\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass of the CustomGNN model.\n\n        Parameters:\n        -----------\n        data : torch_geometric.data.Data\n            The input graph data\n\n        Returns:\n        --------\n        torch.Tensor\n            Output predictions (logits)\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n\n        # Process through GNN layers\n        for i, conv in enumerate(self.convs):\n            # Check if the layer supports edge attributes\n            if edge_attr is not None and hasattr(conv, 'supports_edge_attr') and conv.supports_edge_attr:\n                x = conv(x, edge_index, edge_attr)\n            else:\n                x = conv(x, edge_index)\n\n            x = self.batch_norms[i](x)\n            x = self.activation(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Apply pooling if batch information is available\n        if batch is not None:\n            if self.is_special_pooling:\n                # Handle special pooling layers like TopKPooling\n                if hasattr(self.custom_pooling, 'forward'):\n                    # TopKPooling and similar layers need edge_index\n                    x, edge_index, _, batch, _, _ = self.custom_pooling(x, edge_index, None, batch)\n                    # After special pooling, apply global mean pooling\n                    x = global_mean_pool(x, batch)\n                else:\n                    # Custom pooling function that takes (x, batch)\n                    x = self.custom_pooling(x, batch)\n            else:\n                # Standard pooling from parent class\n                x = self.pool(x, batch)\n\n        # Use the base class's readout layers for final classification\n        out = self.readout(x)\n\n        return out\n</code></pre>"},{"location":"imgraph/models/custom_gnn/#imgraph.models.custom_gnn.CustomGNN.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass of the CustomGNN model.</p>"},{"location":"imgraph/models/custom_gnn/#imgraph.models.custom_gnn.CustomGNN.forward--parameters","title":"Parameters:","text":"<p>data : torch_geometric.data.Data     The input graph data</p>"},{"location":"imgraph/models/custom_gnn/#imgraph.models.custom_gnn.CustomGNN.forward--returns","title":"Returns:","text":"<p>torch.Tensor     Output predictions (logits)</p> Source code in <code>imgraph/models/custom_gnn.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass of the CustomGNN model.\n\n    Parameters:\n    -----------\n    data : torch_geometric.data.Data\n        The input graph data\n\n    Returns:\n    --------\n    torch.Tensor\n        Output predictions (logits)\n    \"\"\"\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n    edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n\n    # Process through GNN layers\n    for i, conv in enumerate(self.convs):\n        # Check if the layer supports edge attributes\n        if edge_attr is not None and hasattr(conv, 'supports_edge_attr') and conv.supports_edge_attr:\n            x = conv(x, edge_index, edge_attr)\n        else:\n            x = conv(x, edge_index)\n\n        x = self.batch_norms[i](x)\n        x = self.activation(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Apply pooling if batch information is available\n    if batch is not None:\n        if self.is_special_pooling:\n            # Handle special pooling layers like TopKPooling\n            if hasattr(self.custom_pooling, 'forward'):\n                # TopKPooling and similar layers need edge_index\n                x, edge_index, _, batch, _, _ = self.custom_pooling(x, edge_index, None, batch)\n                # After special pooling, apply global mean pooling\n                x = global_mean_pool(x, batch)\n            else:\n                # Custom pooling function that takes (x, batch)\n                x = self.custom_pooling(x, batch)\n        else:\n            # Standard pooling from parent class\n            x = self.pool(x, batch)\n\n    # Use the base class's readout layers for final classification\n    out = self.readout(x)\n\n    return out\n</code></pre>"},{"location":"imgraph/models/custom_gnn/#imgraph.models.custom_gnn.CustomGNN.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize layers - this is already handled in init so we provide an empty implementation to satisfy the abstract method requirement.</p> Source code in <code>imgraph/models/custom_gnn.py</code> <pre><code>def init_layers(self):\n    \"\"\"\n    Initialize layers - this is already handled in __init__ so we provide an empty implementation\n    to satisfy the abstract method requirement.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"imgraph/models/custom_gnn/#imgraph.models.custom_gnn.CustomGNNWithEdgeFeatures","title":"<code>CustomGNNWithEdgeFeatures</code>","text":"<p>               Bases: <code>CustomGNN</code></p> <p>A version of CustomGNN that explicitly handles edge features.</p> <p>This class is provided for API consistency with other models in the package.</p> Source code in <code>imgraph/models/custom_gnn.py</code> <pre><code>class CustomGNNWithEdgeFeatures(CustomGNN):\n    \"\"\"\n    A version of CustomGNN that explicitly handles edge features.\n\n    This class is provided for API consistency with other models in the package.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        hidden_dim: int,\n        num_classes: int,\n        edge_dim: int,\n        num_layers: int = 3,\n        gnn_layer_cls: Union[Callable, List[Callable]] = None,\n        pooling_method: Union[str, Callable] = 'mean',\n        dropout: float = 0.5,\n        activation: Callable = F.relu,\n        **kwargs\n    ):\n        super().__init__(\n            num_features=num_features,\n            hidden_dim=hidden_dim,\n            num_classes=num_classes,\n            num_layers=num_layers,\n            gnn_layer_cls=gnn_layer_cls,\n            pooling_method=pooling_method,\n            dropout=dropout,\n            activation=activation,\n            edge_dim=edge_dim,\n            **kwargs\n        )\n\n        # Mark edge features as supported for layers that don't explicitly declare it\n        for conv in self.convs:\n            if hasattr(conv, 'supports_edge_attr'):\n                continue\n            setattr(conv, 'supports_edge_attr', True)\n</code></pre>"},{"location":"imgraph/models/gat/","title":"GAT","text":""},{"location":"imgraph/models/gat/#imgraphmodelsgat","title":"<code>imgraph.models.gat</code>","text":"<p>Graph Attention Network (GAT) model.</p>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GAT","title":"<code>GAT</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>Graph Attention Network (GAT) model.</p>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GAT--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean' heads : int, optional     Number of attention heads, by default 8 concat : bool, optional     Whether to concatenate attention heads, by default True use_v2 : bool, optional     Whether to use GATv2, by default False</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>class GAT(BaseGNN):\n    \"\"\"\n    Graph Attention Network (GAT) model.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    heads : int, optional\n        Number of attention heads, by default 8\n    concat : bool, optional\n        Whether to concatenate attention heads, by default True\n    use_v2 : bool, optional\n        Whether to use GATv2, by default False\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, \n                 pool='mean', heads=8, concat=True, use_v2=False):\n        \"\"\"Initialize the GAT model.\"\"\"\n        self.heads = heads\n        self.concat = concat\n        self.use_v2 = use_v2\n        super(GAT, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GAT layers.\"\"\"\n        self.convs = nn.ModuleList()\n\n        # Choose GAT implementation\n        GATLayer = GATv2Conv if self.use_v2 else GATConv\n\n        # Input layer\n        self.convs.append(GATLayer(\n            in_channels=self.input_dim,\n            out_channels=self.hidden_dim // self.heads if self.concat else self.hidden_dim,\n            heads=self.heads,\n            concat=self.concat,\n            dropout=self.dropout\n        ))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            if i &lt; self.num_layers - 2:\n                # Hidden layer with multiple heads\n                self.convs.append(GATLayer(\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim // self.heads if self.concat else self.hidden_dim,\n                    heads=self.heads,\n                    concat=self.concat,\n                    dropout=self.dropout\n                ))\n            else:\n                # Last layer often uses a single head\n                self.convs.append(GATLayer(\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    heads=1,\n                    concat=False,\n                    dropout=self.dropout\n                ))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Convolutional layers\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GAT.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', heads=8, concat=True, use_v2=False)</code>","text":"<p>Initialize the GAT model.</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, \n             pool='mean', heads=8, concat=True, use_v2=False):\n    \"\"\"Initialize the GAT model.\"\"\"\n    self.heads = heads\n    self.concat = concat\n    self.use_v2 = use_v2\n    super(GAT, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GAT.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GAT.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GAT.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Convolutional layers\n    for i in range(self.num_layers):\n        x = self.convs[i](x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GAT.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GAT layers.</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GAT layers.\"\"\"\n    self.convs = nn.ModuleList()\n\n    # Choose GAT implementation\n    GATLayer = GATv2Conv if self.use_v2 else GATConv\n\n    # Input layer\n    self.convs.append(GATLayer(\n        in_channels=self.input_dim,\n        out_channels=self.hidden_dim // self.heads if self.concat else self.hidden_dim,\n        heads=self.heads,\n        concat=self.concat,\n        dropout=self.dropout\n    ))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        if i &lt; self.num_layers - 2:\n            # Hidden layer with multiple heads\n            self.convs.append(GATLayer(\n                in_channels=self.hidden_dim,\n                out_channels=self.hidden_dim // self.heads if self.concat else self.hidden_dim,\n                heads=self.heads,\n                concat=self.concat,\n                dropout=self.dropout\n            ))\n        else:\n            # Last layer often uses a single head\n            self.convs.append(GATLayer(\n                in_channels=self.hidden_dim,\n                out_channels=self.hidden_dim,\n                heads=1,\n                concat=False,\n                dropout=self.dropout\n            ))\n</code></pre>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GATWithEdgeFeatures","title":"<code>GATWithEdgeFeatures</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>Graph Attention Network (GAT) with edge features.</p>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GATWithEdgeFeatures--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension edge_dim : int     Edge feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean' heads : int, optional     Number of attention heads, by default 8</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>class GATWithEdgeFeatures(BaseGNN):\n    \"\"\"\n    Graph Attention Network (GAT) with edge features.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    edge_dim : int\n        Edge feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    heads : int, optional\n        Number of attention heads, by default 8\n    \"\"\"\n\n    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, \n                 dropout=0.0, pool='mean', heads=8):\n        \"\"\"Initialize the GAT model with edge features.\"\"\"\n        self.edge_dim = edge_dim\n        self.heads = heads\n        super(GATWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GAT layers with edge features.\"\"\"\n        self.convs = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(GATConv(\n            in_channels=self.input_dim,\n            out_channels=self.hidden_dim // self.heads,\n            heads=self.heads,\n            concat=True,\n            dropout=self.dropout,\n            edge_dim=self.edge_dim\n        ))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            if i &lt; self.num_layers - 2:\n                # Hidden layer with multiple heads\n                self.convs.append(GATConv(\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim // self.heads,\n                    heads=self.heads,\n                    concat=True,\n                    dropout=self.dropout,\n                    edge_dim=self.edge_dim\n                ))\n            else:\n                # Last layer often uses a single head\n                self.convs.append(GATConv(\n                    in_channels=self.hidden_dim,\n                    out_channels=self.hidden_dim,\n                    heads=1,\n                    concat=False,\n                    dropout=self.dropout,\n                    edge_dim=self.edge_dim\n                ))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Convolutional layers\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index, edge_attr=edge_attr)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GATWithEdgeFeatures.__init__","title":"<code>__init__(input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', heads=8)</code>","text":"<p>Initialize the GAT model with edge features.</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, \n             dropout=0.0, pool='mean', heads=8):\n    \"\"\"Initialize the GAT model with edge features.\"\"\"\n    self.edge_dim = edge_dim\n    self.heads = heads\n    super(GATWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GATWithEdgeFeatures.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GATWithEdgeFeatures.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GATWithEdgeFeatures.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Convolutional layers\n    for i in range(self.num_layers):\n        x = self.convs[i](x, edge_index, edge_attr=edge_attr)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/gat/#imgraph.models.gat.GATWithEdgeFeatures.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GAT layers with edge features.</p> Source code in <code>imgraph/models/gat.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GAT layers with edge features.\"\"\"\n    self.convs = nn.ModuleList()\n\n    # Input layer\n    self.convs.append(GATConv(\n        in_channels=self.input_dim,\n        out_channels=self.hidden_dim // self.heads,\n        heads=self.heads,\n        concat=True,\n        dropout=self.dropout,\n        edge_dim=self.edge_dim\n    ))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        if i &lt; self.num_layers - 2:\n            # Hidden layer with multiple heads\n            self.convs.append(GATConv(\n                in_channels=self.hidden_dim,\n                out_channels=self.hidden_dim // self.heads,\n                heads=self.heads,\n                concat=True,\n                dropout=self.dropout,\n                edge_dim=self.edge_dim\n            ))\n        else:\n            # Last layer often uses a single head\n            self.convs.append(GATConv(\n                in_channels=self.hidden_dim,\n                out_channels=self.hidden_dim,\n                heads=1,\n                concat=False,\n                dropout=self.dropout,\n                edge_dim=self.edge_dim\n            ))\n</code></pre>"},{"location":"imgraph/models/gcn/","title":"GCN","text":""},{"location":"imgraph/models/gcn/#imgraphmodelsgcn","title":"<code>imgraph.models.gcn</code>","text":"<p>Graph Convolutional Network (GCN) model.</p>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCN","title":"<code>GCN</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>Graph Convolutional Network (GCN) model.</p>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCN--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean' use_batch_norm : bool, optional     Whether to use batch normalization, by default True</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>class GCN(BaseGNN):\n    \"\"\"\n    Graph Convolutional Network (GCN) model.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', use_batch_norm=True):\n        \"\"\"Initialize the GCN model.\"\"\"\n        self.use_batch_norm = use_batch_norm\n        super(GCN, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GCN layers.\"\"\"\n        self.convs = nn.ModuleList()\n        self.bns = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(GCNConv(self.input_dim, self.hidden_dim))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            self.convs.append(GCNConv(self.hidden_dim, self.hidden_dim))\n            if self.use_batch_norm:\n                self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Convolutional layers\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n\n            if self.use_batch_norm:\n                x = self.bns[i](x)\n\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCN.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', use_batch_norm=True)</code>","text":"<p>Initialize the GCN model.</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', use_batch_norm=True):\n    \"\"\"Initialize the GCN model.\"\"\"\n    self.use_batch_norm = use_batch_norm\n    super(GCN, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCN.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCN.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCN.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Convolutional layers\n    for i in range(self.num_layers):\n        x = self.convs[i](x, edge_index)\n\n        if self.use_batch_norm:\n            x = self.bns[i](x)\n\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCN.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GCN layers.</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GCN layers.\"\"\"\n    self.convs = nn.ModuleList()\n    self.bns = nn.ModuleList()\n\n    # Input layer\n    self.convs.append(GCNConv(self.input_dim, self.hidden_dim))\n    if self.use_batch_norm:\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        self.convs.append(GCNConv(self.hidden_dim, self.hidden_dim))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n</code></pre>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCNWithEdgeFeatures","title":"<code>GCNWithEdgeFeatures</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>Graph Convolutional Network (GCN) with edge features.</p>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCNWithEdgeFeatures--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension edge_dim : int     Edge feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean'</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>class GCNWithEdgeFeatures(BaseGNN):\n    \"\"\"\n    Graph Convolutional Network (GCN) with edge features.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    edge_dim : int\n        Edge feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    \"\"\"\n\n    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean'):\n        \"\"\"Initialize the GCN model with edge features.\"\"\"\n        self.edge_dim = edge_dim\n        super(GCNWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GCN layers with edge features.\"\"\"\n        from torch_geometric.nn import GCNConv\n\n        self.convs = nn.ModuleList()\n        self.edge_embeddings = nn.ModuleList()\n        self.bns = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(GCNConv(self.input_dim, self.hidden_dim))\n        self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            self.convs.append(GCNConv(self.hidden_dim, self.hidden_dim))\n            self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Convolutional layers\n        for i in range(self.num_layers):\n            # Compute edge weights from edge features\n            if edge_attr is not None:\n                edge_weights = torch.sigmoid(self.edge_embeddings[i](edge_attr)).squeeze(-1)\n            else:\n                edge_weights = None\n\n            # Apply convolution\n            x = self.convs[i](x, edge_index, edge_weight=edge_weights)\n            x = self.bns[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCNWithEdgeFeatures.__init__","title":"<code>__init__(input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean')</code>","text":"<p>Initialize the GCN model with edge features.</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean'):\n    \"\"\"Initialize the GCN model with edge features.\"\"\"\n    self.edge_dim = edge_dim\n    super(GCNWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCNWithEdgeFeatures.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCNWithEdgeFeatures.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCNWithEdgeFeatures.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Convolutional layers\n    for i in range(self.num_layers):\n        # Compute edge weights from edge features\n        if edge_attr is not None:\n            edge_weights = torch.sigmoid(self.edge_embeddings[i](edge_attr)).squeeze(-1)\n        else:\n            edge_weights = None\n\n        # Apply convolution\n        x = self.convs[i](x, edge_index, edge_weight=edge_weights)\n        x = self.bns[i](x)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/gcn/#imgraph.models.gcn.GCNWithEdgeFeatures.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GCN layers with edge features.</p> Source code in <code>imgraph/models/gcn.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GCN layers with edge features.\"\"\"\n    from torch_geometric.nn import GCNConv\n\n    self.convs = nn.ModuleList()\n    self.edge_embeddings = nn.ModuleList()\n    self.bns = nn.ModuleList()\n\n    # Input layer\n    self.convs.append(GCNConv(self.input_dim, self.hidden_dim))\n    self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n    self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        self.convs.append(GCNConv(self.hidden_dim, self.hidden_dim))\n        self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n</code></pre>"},{"location":"imgraph/models/gin/","title":"GIN","text":""},{"location":"imgraph/models/gin/#imgraphmodelsgin","title":"<code>imgraph.models.gin</code>","text":"<p>Graph Isomorphism Network (GIN) model.</p>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GIN","title":"<code>GIN</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>Graph Isomorphism Network (GIN) model.</p>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GIN--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean' eps : float, optional     Epsilon value for GIN, by default 0.0 train_eps : bool, optional     Whether to train epsilon, by default False use_batch_norm : bool, optional     Whether to use batch normalization, by default True</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>class GIN(BaseGNN):\n    \"\"\"\n    Graph Isomorphism Network (GIN) model.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    eps : float, optional\n        Epsilon value for GIN, by default 0.0\n    train_eps : bool, optional\n        Whether to train epsilon, by default False\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, \n                 pool='mean', eps=0.0, train_eps=False, use_batch_norm=True):\n        \"\"\"Initialize the GIN model.\"\"\"\n        self.eps = eps\n        self.train_eps = train_eps\n        self.use_batch_norm = use_batch_norm\n        super(GIN, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GIN layers.\"\"\"\n        self.convs = nn.ModuleList()\n        self.bns = nn.ModuleList() if self.use_batch_norm else None\n\n        # Input layer\n        nn1 = nn.Sequential(\n            nn.Linear(self.input_dim, self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(self.hidden_dim, self.hidden_dim)\n        )\n        self.convs.append(GINConv(nn1, eps=self.eps, train_eps=self.train_eps))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            nn_layer = nn.Sequential(\n                nn.Linear(self.hidden_dim, self.hidden_dim),\n                nn.ReLU(),\n                nn.Linear(self.hidden_dim, self.hidden_dim)\n            )\n            self.convs.append(GINConv(nn_layer, eps=self.eps, train_eps=self.train_eps))\n            if self.use_batch_norm:\n                self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Convolutional layers\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n\n            if self.use_batch_norm:\n                x = self.bns[i](x)\n\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GIN.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', eps=0.0, train_eps=False, use_batch_norm=True)</code>","text":"<p>Initialize the GIN model.</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, \n             pool='mean', eps=0.0, train_eps=False, use_batch_norm=True):\n    \"\"\"Initialize the GIN model.\"\"\"\n    self.eps = eps\n    self.train_eps = train_eps\n    self.use_batch_norm = use_batch_norm\n    super(GIN, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GIN.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GIN.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GIN.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Convolutional layers\n    for i in range(self.num_layers):\n        x = self.convs[i](x, edge_index)\n\n        if self.use_batch_norm:\n            x = self.bns[i](x)\n\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GIN.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GIN layers.</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GIN layers.\"\"\"\n    self.convs = nn.ModuleList()\n    self.bns = nn.ModuleList() if self.use_batch_norm else None\n\n    # Input layer\n    nn1 = nn.Sequential(\n        nn.Linear(self.input_dim, self.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(self.hidden_dim, self.hidden_dim)\n    )\n    self.convs.append(GINConv(nn1, eps=self.eps, train_eps=self.train_eps))\n    if self.use_batch_norm:\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        nn_layer = nn.Sequential(\n            nn.Linear(self.hidden_dim, self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(self.hidden_dim, self.hidden_dim)\n        )\n        self.convs.append(GINConv(nn_layer, eps=self.eps, train_eps=self.train_eps))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n</code></pre>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GINWithEdgeFeatures","title":"<code>GINWithEdgeFeatures</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>Graph Isomorphism Network (GIN) with edge features (GINE).</p>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GINWithEdgeFeatures--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension edge_dim : int     Edge feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean' eps : float, optional     Epsilon value for GIN, by default 0.0 train_eps : bool, optional     Whether to train epsilon, by default False use_batch_norm : bool, optional     Whether to use batch normalization, by default True</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>class GINWithEdgeFeatures(BaseGNN):\n    \"\"\"\n    Graph Isomorphism Network (GIN) with edge features (GINE).\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    edge_dim : int\n        Edge feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    eps : float, optional\n        Epsilon value for GIN, by default 0.0\n    train_eps : bool, optional\n        Whether to train epsilon, by default False\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True\n    \"\"\"\n\n    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0,\n                 pool='mean', eps=0.0, train_eps=False, use_batch_norm=True):\n        \"\"\"Initialize the GINE model.\"\"\"\n        self.edge_dim = edge_dim\n        self.eps = eps\n        self.train_eps = train_eps\n        self.use_batch_norm = use_batch_norm\n        super(GINWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GINE layers.\"\"\"\n        self.convs = nn.ModuleList()\n        self.bns = nn.ModuleList() if self.use_batch_norm else None\n\n        # Input layer\n        nn1 = nn.Sequential(\n            nn.Linear(self.input_dim, self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(self.hidden_dim, self.hidden_dim)\n        )\n        self.convs.append(GINEConv(nn1, eps=self.eps, train_eps=self.train_eps, edge_dim=self.edge_dim))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            nn_layer = nn.Sequential(\n                nn.Linear(self.hidden_dim, self.hidden_dim),\n                nn.ReLU(),\n                nn.Linear(self.hidden_dim, self.hidden_dim)\n            )\n            self.convs.append(GINEConv(nn_layer, eps=self.eps, train_eps=self.train_eps, edge_dim=self.edge_dim))\n            if self.use_batch_norm:\n                self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Convolutional layers\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index, edge_attr)\n\n            if self.use_batch_norm:\n                x = self.bns[i](x)\n\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GINWithEdgeFeatures.__init__","title":"<code>__init__(input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', eps=0.0, train_eps=False, use_batch_norm=True)</code>","text":"<p>Initialize the GINE model.</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0,\n             pool='mean', eps=0.0, train_eps=False, use_batch_norm=True):\n    \"\"\"Initialize the GINE model.\"\"\"\n    self.edge_dim = edge_dim\n    self.eps = eps\n    self.train_eps = train_eps\n    self.use_batch_norm = use_batch_norm\n    super(GINWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GINWithEdgeFeatures.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GINWithEdgeFeatures.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GINWithEdgeFeatures.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Convolutional layers\n    for i in range(self.num_layers):\n        x = self.convs[i](x, edge_index, edge_attr)\n\n        if self.use_batch_norm:\n            x = self.bns[i](x)\n\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/gin/#imgraph.models.gin.GINWithEdgeFeatures.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GINE layers.</p> Source code in <code>imgraph/models/gin.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GINE layers.\"\"\"\n    self.convs = nn.ModuleList()\n    self.bns = nn.ModuleList() if self.use_batch_norm else None\n\n    # Input layer\n    nn1 = nn.Sequential(\n        nn.Linear(self.input_dim, self.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(self.hidden_dim, self.hidden_dim)\n    )\n    self.convs.append(GINEConv(nn1, eps=self.eps, train_eps=self.train_eps, edge_dim=self.edge_dim))\n    if self.use_batch_norm:\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        nn_layer = nn.Sequential(\n            nn.Linear(self.hidden_dim, self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(self.hidden_dim, self.hidden_dim)\n        )\n        self.convs.append(GINEConv(nn_layer, eps=self.eps, train_eps=self.train_eps, edge_dim=self.edge_dim))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n</code></pre>"},{"location":"imgraph/models/sage/","title":"GraphSAGE","text":""},{"location":"imgraph/models/sage/#imgraphmodelssage","title":"<code>imgraph.models.sage</code>","text":"<p>GraphSAGE model implementation.</p>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGE","title":"<code>GraphSAGE</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>GraphSAGE model.</p>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGE--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean' aggr : str, optional     Aggregation method, by default 'mean' use_batch_norm : bool, optional     Whether to use batch normalization, by default True</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>class GraphSAGE(BaseGNN):\n    \"\"\"\n    GraphSAGE model.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    aggr : str, optional\n        Aggregation method, by default 'mean'\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, \n                 pool='mean', aggr='mean', use_batch_norm=True):\n        \"\"\"Initialize the GraphSAGE model.\"\"\"\n        self.aggr = aggr\n        self.use_batch_norm = use_batch_norm\n        super(GraphSAGE, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GraphSAGE layers.\"\"\"\n        self.convs = nn.ModuleList()\n        self.bns = nn.ModuleList() if self.use_batch_norm else None\n\n        # Input layer\n        self.convs.append(SAGEConv(self.input_dim, self.hidden_dim, aggr=self.aggr))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            self.convs.append(SAGEConv(self.hidden_dim, self.hidden_dim, aggr=self.aggr))\n            if self.use_batch_norm:\n                self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Convolutional layers\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n\n            if self.use_batch_norm:\n                x = self.bns[i](x)\n\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGE.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', aggr='mean', use_batch_norm=True)</code>","text":"<p>Initialize the GraphSAGE model.</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, \n             pool='mean', aggr='mean', use_batch_norm=True):\n    \"\"\"Initialize the GraphSAGE model.\"\"\"\n    self.aggr = aggr\n    self.use_batch_norm = use_batch_norm\n    super(GraphSAGE, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGE.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGE.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGE.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Convolutional layers\n    for i in range(self.num_layers):\n        x = self.convs[i](x, edge_index)\n\n        if self.use_batch_norm:\n            x = self.bns[i](x)\n\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGE.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GraphSAGE layers.</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GraphSAGE layers.\"\"\"\n    self.convs = nn.ModuleList()\n    self.bns = nn.ModuleList() if self.use_batch_norm else None\n\n    # Input layer\n    self.convs.append(SAGEConv(self.input_dim, self.hidden_dim, aggr=self.aggr))\n    if self.use_batch_norm:\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        self.convs.append(SAGEConv(self.hidden_dim, self.hidden_dim, aggr=self.aggr))\n        if self.use_batch_norm:\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n</code></pre>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGEWithEdgeFeatures","title":"<code>GraphSAGEWithEdgeFeatures</code>","text":"<p>               Bases: <code>BaseGNN</code></p> <p>GraphSAGE model with edge features.</p>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGEWithEdgeFeatures--parameters","title":"Parameters","text":"<p>input_dim : int     Input feature dimension edge_dim : int     Edge feature dimension hidden_dim : int     Hidden layer dimension output_dim : int     Output dimension num_layers : int, optional     Number of layers, by default 2 dropout : float, optional     Dropout rate, by default 0.0 pool : str, optional     Global pooling method, by default 'mean' aggr : str, optional     Aggregation method, by default 'mean'</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>class GraphSAGEWithEdgeFeatures(BaseGNN):\n    \"\"\"\n    GraphSAGE model with edge features.\n\n    Parameters\n    ----------\n    input_dim : int\n        Input feature dimension\n    edge_dim : int\n        Edge feature dimension\n    hidden_dim : int\n        Hidden layer dimension\n    output_dim : int\n        Output dimension\n    num_layers : int, optional\n        Number of layers, by default 2\n    dropout : float, optional\n        Dropout rate, by default 0.0\n    pool : str, optional\n        Global pooling method, by default 'mean'\n    aggr : str, optional\n        Aggregation method, by default 'mean'\n    \"\"\"\n\n    def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, \n                 dropout=0.0, pool='mean', aggr='mean'):\n        \"\"\"Initialize the GraphSAGE model with edge features.\"\"\"\n        self.edge_dim = edge_dim\n        self.aggr = aggr\n        super(GraphSAGEWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n\n    def init_layers(self):\n        \"\"\"Initialize GraphSAGE layers with edge features.\"\"\"\n        self.convs = nn.ModuleList()\n        self.edge_embeddings = nn.ModuleList()\n        self.bns = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(SAGEConv(self.input_dim, self.hidden_dim, aggr=self.aggr))\n        self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n        # Hidden layers\n        for i in range(self.num_layers - 1):\n            self.convs.append(SAGEConv(self.hidden_dim, self.hidden_dim, aggr=self.aggr))\n            self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n            self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    def forward(self, data):\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        data : torch_geometric.data.Data\n            Input graph data\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor\n        \"\"\"\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n        # If batch is None, assume a single graph\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n        # Since SAGEConv doesn't support edge features directly, we'll use edge weights instead\n        for i in range(self.num_layers):\n            # Compute edge weights from edge features\n            if edge_attr is not None:\n                edge_weights = torch.sigmoid(self.edge_embeddings[i](edge_attr)).squeeze(-1)\n\n                # Apply weighted message passing (this is a simplification, since SAGEConv\n                # doesn't directly support edge weights, we apply the weights to the features)\n                src, dst = edge_index\n                src_features = x[src] * edge_weights.unsqueeze(-1)\n\n                # Store original features\n                original_x = x.clone()\n\n                # Update source features in x (for weighted aggregation)\n                for j in range(len(src)):\n                    x[src[j]] = src_features[j]\n\n                # Apply convolution\n                x = self.convs[i](x, edge_index)\n\n                # Restore original features\n                x = x + original_x\n            else:\n                # Standard convolution without edge features\n                x = self.convs[i](x, edge_index)\n\n            x = self.bns[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Global pooling\n        x = self.pool(x, batch)\n\n        # Readout\n        x = self.readout(x)\n\n        return x\n</code></pre>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGEWithEdgeFeatures.__init__","title":"<code>__init__(input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, pool='mean', aggr='mean')</code>","text":"<p>Initialize the GraphSAGE model with edge features.</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>def __init__(self, input_dim, edge_dim, hidden_dim, output_dim, num_layers=2, \n             dropout=0.0, pool='mean', aggr='mean'):\n    \"\"\"Initialize the GraphSAGE model with edge features.\"\"\"\n    self.edge_dim = edge_dim\n    self.aggr = aggr\n    super(GraphSAGEWithEdgeFeatures, self).__init__(input_dim, hidden_dim, output_dim, num_layers, dropout, pool)\n</code></pre>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGEWithEdgeFeatures.forward","title":"<code>forward(data)</code>","text":"<p>Forward pass.</p>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGEWithEdgeFeatures.forward--parameters","title":"Parameters","text":"<p>data : torch_geometric.data.Data     Input graph data</p>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGEWithEdgeFeatures.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    data : torch_geometric.data.Data\n        Input graph data\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor\n    \"\"\"\n    x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n    # If batch is None, assume a single graph\n    if batch is None:\n        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n\n    # Since SAGEConv doesn't support edge features directly, we'll use edge weights instead\n    for i in range(self.num_layers):\n        # Compute edge weights from edge features\n        if edge_attr is not None:\n            edge_weights = torch.sigmoid(self.edge_embeddings[i](edge_attr)).squeeze(-1)\n\n            # Apply weighted message passing (this is a simplification, since SAGEConv\n            # doesn't directly support edge weights, we apply the weights to the features)\n            src, dst = edge_index\n            src_features = x[src] * edge_weights.unsqueeze(-1)\n\n            # Store original features\n            original_x = x.clone()\n\n            # Update source features in x (for weighted aggregation)\n            for j in range(len(src)):\n                x[src[j]] = src_features[j]\n\n            # Apply convolution\n            x = self.convs[i](x, edge_index)\n\n            # Restore original features\n            x = x + original_x\n        else:\n            # Standard convolution without edge features\n            x = self.convs[i](x, edge_index)\n\n        x = self.bns[i](x)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n    # Global pooling\n    x = self.pool(x, batch)\n\n    # Readout\n    x = self.readout(x)\n\n    return x\n</code></pre>"},{"location":"imgraph/models/sage/#imgraph.models.sage.GraphSAGEWithEdgeFeatures.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize GraphSAGE layers with edge features.</p> Source code in <code>imgraph/models/sage.py</code> <pre><code>def init_layers(self):\n    \"\"\"Initialize GraphSAGE layers with edge features.\"\"\"\n    self.convs = nn.ModuleList()\n    self.edge_embeddings = nn.ModuleList()\n    self.bns = nn.ModuleList()\n\n    # Input layer\n    self.convs.append(SAGEConv(self.input_dim, self.hidden_dim, aggr=self.aggr))\n    self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n    self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n\n    # Hidden layers\n    for i in range(self.num_layers - 1):\n        self.convs.append(SAGEConv(self.hidden_dim, self.hidden_dim, aggr=self.aggr))\n        self.edge_embeddings.append(nn.Linear(self.edge_dim, 1))\n        self.bns.append(nn.BatchNorm1d(self.hidden_dim))\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/","title":"Config Pipeline","text":""},{"location":"imgraph/pipeline/config_pipeline/#imgraphpipelineconfig_pipeline","title":"<code>imgraph.pipeline.config_pipeline</code>","text":"<p>Pipeline for creating graphs from images using a configuration.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline","title":"<code>GraphPipeline</code>","text":"<p>A pipeline for processing images into graphs using a configuration.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline--parameters","title":"Parameters","text":"<p>config : dict or str     Configuration dictionary or path to configuration file</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>class GraphPipeline:\n    \"\"\"\n    A pipeline for processing images into graphs using a configuration.\n\n    Parameters\n    ----------\n    config : dict or str\n        Configuration dictionary or path to configuration file\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"Initialize the GraphPipeline.\"\"\"\n        if isinstance(config, str):\n            # Load config from file\n            with open(config, 'r') as f:\n                self.config = json.load(f)\n        else:\n            self.config = config\n\n        # Create graph builder from config\n        self.graph_builder = GraphPresets.custom_preset(self.config)\n\n    def process(self, image):\n        \"\"\"\n        Process an image into a graph.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            Input image with shape (H, W, C)\n\n        Returns\n        -------\n        torch_geometric.data.Data\n            Graph representation of the image\n        \"\"\"\n        return self.graph_builder(image)\n\n    def batch_process(self, images):\n        \"\"\"\n        Process multiple images into graphs.\n\n        Parameters\n        ----------\n        images : list of numpy.ndarray\n            List of input images\n\n        Returns\n        -------\n        list of torch_geometric.data.Data\n            List of graph representations\n        \"\"\"\n        return [self.process(image) for image in images]\n\n    def save_config(self, path):\n        \"\"\"\n        Save the configuration to a file.\n\n        Parameters\n        ----------\n        path : str\n            Path to save the configuration\n        \"\"\"\n        with open(path, 'w') as f:\n            json.dump(self.config, f, indent=4)\n\n    @classmethod\n    def from_preset(cls, preset_name, **kwargs):\n        \"\"\"\n        Create a pipeline from a preset.\n\n        Parameters\n        ----------\n        preset_name : str\n            Name of the preset to use\n        **kwargs\n            Additional arguments to override preset parameters\n\n        Returns\n        -------\n        GraphPipeline\n            Pipeline with the specified preset\n        \"\"\"\n        # Create base config based on preset\n        if preset_name == 'slic_mean_color':\n            config = {\n                'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 100, 'compactness': 10}},\n                'node_features': {'method': 'mean_color_features', 'params': {}},\n                'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n                'edge_features': {'method': None, 'params': {}}\n            }\n        elif preset_name == 'slic_color_position':\n            config = {\n                'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 100, 'compactness': 10}},\n                'node_features': {'method': 'custom', 'params': {\n                    'features': [\n                        {'method': 'mean_std_color_features', 'params': {}},\n                        {'method': 'normalized_position_features', 'params': {}}\n                    ]\n                }},\n                'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n                'edge_features': {'method': 'color_difference', 'params': {}}\n            }\n        elif preset_name == 'patches_color':\n            config = {\n                'node_creation': {'method': 'regular_patch_nodes', 'params': {'patch_size': 16}},\n                'node_features': {'method': 'mean_std_color_features', 'params': {}},\n                'edge_creation': {'method': 'grid_4_edges', 'params': {}},\n                'edge_features': {'method': None, 'params': {}}\n            }\n        elif preset_name == 'patches_cnn':\n            config = {\n                'node_creation': {'method': 'regular_patch_nodes', 'params': {'patch_size': 32}},\n                'node_features': {'method': 'pretrained_cnn_features', 'params': {'model_name': 'resnet18'}},\n                'edge_creation': {'method': 'grid_4_edges', 'params': {}},\n                'edge_features': {'method': None, 'params': {}}\n            }\n        elif preset_name == 'tiny_graph':\n            config = {\n                'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 20, 'compactness': 10}},\n                'node_features': {'method': 'mean_std_color_features', 'params': {}},\n                'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n                'edge_features': {'method': None, 'params': {}}\n            }\n        elif preset_name == 'superpixel_comprehensive':\n            config = {\n                'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 150, 'compactness': 15}},\n                'node_features': {'method': 'custom', 'params': {\n                    'features': [\n                        {'method': 'mean_std_color_features', 'params': {}},\n                        {'method': 'lbp_features', 'params': {}},\n                        {'method': 'normalized_position_features', 'params': {}}\n                    ]\n                }},\n                'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n                'edge_features': {'method': 'custom', 'params': {\n                    'features': [\n                        {'method': 'color_difference', 'params': {}},\n                        {'method': 'distance_features', 'params': {}},\n                        {'method': 'boundary_strength', 'params': {}}\n                    ]\n                }}\n            }\n        else:\n            raise ValueError(f\"Unknown preset: {preset_name}\")\n\n        # Override with kwargs\n        for section in kwargs:\n            if section in config:\n                for param, value in kwargs[section].items():\n                    config[section]['params'][param] = value\n\n        return cls(config)\n\n    @staticmethod\n    def list_presets():\n        \"\"\"\n        List available presets.\n\n        Returns\n        -------\n        list\n            List of preset names\n        \"\"\"\n        return [\n            'slic_mean_color',\n            'slic_color_position',\n            'patches_color',\n            'patches_cnn',\n            'tiny_graph',\n            'superpixel_comprehensive'\n        ]\n\n    @staticmethod\n    def create_default_config():\n        \"\"\"\n        Create a default configuration.\n\n        Returns\n        -------\n        dict\n            Default configuration\n        \"\"\"\n        return {\n            'node_creation': {\n                'method': 'slic_superpixel_nodes',\n                'params': {\n                    'n_segments': 100,\n                    'compactness': 10,\n                    'sigma': 0\n                }\n            },\n            'node_features': {\n                'method': 'mean_std_color_features',\n                'params': {\n                    'color_space': 'rgb',\n                    'normalize': True\n                }\n            },\n            'edge_creation': {\n                'method': 'region_adjacency_edges',\n                'params': {\n                    'connectivity': 2\n                }\n            },\n            'edge_features': {\n                'method': 'color_difference',\n                'params': {\n                    'color_space': 'rgb',\n                    'normalize': True\n                }\n            }\n        }\n\n    def optimize_config(self, metric_fn, param_grid, images, labels=None):\n        \"\"\"\n        Optimize configuration parameters using grid search.\n\n        Parameters\n        ----------\n        metric_fn : callable\n            Function to evaluate graph quality\n        param_grid : dict\n            Dictionary of parameters to search\n        images : list\n            List of images to use for optimization\n        labels : list, optional\n            List of labels if needed for evaluation, by default None\n\n        Returns\n        -------\n        dict\n            Optimized configuration\n        \"\"\"\n        best_score = float('-inf')\n        best_config = self.config.copy()\n\n        # Implement a simple grid search\n        import itertools\n\n        # Generate parameter combinations\n        param_names = list(param_grid.keys())\n        param_values = list(param_grid.values())\n        param_combinations = list(itertools.product(*param_values))\n\n        for combination in param_combinations:\n            # Update configuration\n            config = self.config.copy()\n\n            for i, param_name in enumerate(param_names):\n                # Parse parameter path (e.g., \"node_creation.params.n_segments\")\n                parts = param_name.split('.')\n                target = config\n\n                for part in parts[:-1]:\n                    if part not in target:\n                        target[part] = {}\n                    target = target[part]\n\n                # Set parameter value\n                target[parts[-1]] = combination[i]\n\n            # Create graph builder with updated config\n            pipeline = GraphPipeline(config)\n\n            # Process images\n            graphs = pipeline.batch_process(images)\n\n            # Evaluate\n            score = metric_fn(graphs, labels)\n\n            # Update best config if score is better\n            if score &gt; best_score:\n                best_score = score\n                best_config = config\n\n        # Update current config\n        self.config = best_config\n        self.graph_builder = GraphPresets.custom_preset(self.config)\n\n        return best_config\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the GraphPipeline.</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>def __init__(self, config):\n    \"\"\"Initialize the GraphPipeline.\"\"\"\n    if isinstance(config, str):\n        # Load config from file\n        with open(config, 'r') as f:\n            self.config = json.load(f)\n    else:\n        self.config = config\n\n    # Create graph builder from config\n    self.graph_builder = GraphPresets.custom_preset(self.config)\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.batch_process","title":"<code>batch_process(images)</code>","text":"<p>Process multiple images into graphs.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.batch_process--parameters","title":"Parameters","text":"<p>images : list of numpy.ndarray     List of input images</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.batch_process--returns","title":"Returns","text":"<p>list of torch_geometric.data.Data     List of graph representations</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>def batch_process(self, images):\n    \"\"\"\n    Process multiple images into graphs.\n\n    Parameters\n    ----------\n    images : list of numpy.ndarray\n        List of input images\n\n    Returns\n    -------\n    list of torch_geometric.data.Data\n        List of graph representations\n    \"\"\"\n    return [self.process(image) for image in images]\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.create_default_config","title":"<code>create_default_config()</code>  <code>staticmethod</code>","text":"<p>Create a default configuration.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.create_default_config--returns","title":"Returns","text":"<p>dict     Default configuration</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>@staticmethod\ndef create_default_config():\n    \"\"\"\n    Create a default configuration.\n\n    Returns\n    -------\n    dict\n        Default configuration\n    \"\"\"\n    return {\n        'node_creation': {\n            'method': 'slic_superpixel_nodes',\n            'params': {\n                'n_segments': 100,\n                'compactness': 10,\n                'sigma': 0\n            }\n        },\n        'node_features': {\n            'method': 'mean_std_color_features',\n            'params': {\n                'color_space': 'rgb',\n                'normalize': True\n            }\n        },\n        'edge_creation': {\n            'method': 'region_adjacency_edges',\n            'params': {\n                'connectivity': 2\n            }\n        },\n        'edge_features': {\n            'method': 'color_difference',\n            'params': {\n                'color_space': 'rgb',\n                'normalize': True\n            }\n        }\n    }\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.from_preset","title":"<code>from_preset(preset_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a pipeline from a preset.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.from_preset--parameters","title":"Parameters","text":"<p>preset_name : str     Name of the preset to use **kwargs     Additional arguments to override preset parameters</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.from_preset--returns","title":"Returns","text":"<p>GraphPipeline     Pipeline with the specified preset</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>@classmethod\ndef from_preset(cls, preset_name, **kwargs):\n    \"\"\"\n    Create a pipeline from a preset.\n\n    Parameters\n    ----------\n    preset_name : str\n        Name of the preset to use\n    **kwargs\n        Additional arguments to override preset parameters\n\n    Returns\n    -------\n    GraphPipeline\n        Pipeline with the specified preset\n    \"\"\"\n    # Create base config based on preset\n    if preset_name == 'slic_mean_color':\n        config = {\n            'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 100, 'compactness': 10}},\n            'node_features': {'method': 'mean_color_features', 'params': {}},\n            'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n            'edge_features': {'method': None, 'params': {}}\n        }\n    elif preset_name == 'slic_color_position':\n        config = {\n            'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 100, 'compactness': 10}},\n            'node_features': {'method': 'custom', 'params': {\n                'features': [\n                    {'method': 'mean_std_color_features', 'params': {}},\n                    {'method': 'normalized_position_features', 'params': {}}\n                ]\n            }},\n            'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n            'edge_features': {'method': 'color_difference', 'params': {}}\n        }\n    elif preset_name == 'patches_color':\n        config = {\n            'node_creation': {'method': 'regular_patch_nodes', 'params': {'patch_size': 16}},\n            'node_features': {'method': 'mean_std_color_features', 'params': {}},\n            'edge_creation': {'method': 'grid_4_edges', 'params': {}},\n            'edge_features': {'method': None, 'params': {}}\n        }\n    elif preset_name == 'patches_cnn':\n        config = {\n            'node_creation': {'method': 'regular_patch_nodes', 'params': {'patch_size': 32}},\n            'node_features': {'method': 'pretrained_cnn_features', 'params': {'model_name': 'resnet18'}},\n            'edge_creation': {'method': 'grid_4_edges', 'params': {}},\n            'edge_features': {'method': None, 'params': {}}\n        }\n    elif preset_name == 'tiny_graph':\n        config = {\n            'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 20, 'compactness': 10}},\n            'node_features': {'method': 'mean_std_color_features', 'params': {}},\n            'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n            'edge_features': {'method': None, 'params': {}}\n        }\n    elif preset_name == 'superpixel_comprehensive':\n        config = {\n            'node_creation': {'method': 'slic_superpixel_nodes', 'params': {'n_segments': 150, 'compactness': 15}},\n            'node_features': {'method': 'custom', 'params': {\n                'features': [\n                    {'method': 'mean_std_color_features', 'params': {}},\n                    {'method': 'lbp_features', 'params': {}},\n                    {'method': 'normalized_position_features', 'params': {}}\n                ]\n            }},\n            'edge_creation': {'method': 'region_adjacency_edges', 'params': {}},\n            'edge_features': {'method': 'custom', 'params': {\n                'features': [\n                    {'method': 'color_difference', 'params': {}},\n                    {'method': 'distance_features', 'params': {}},\n                    {'method': 'boundary_strength', 'params': {}}\n                ]\n            }}\n        }\n    else:\n        raise ValueError(f\"Unknown preset: {preset_name}\")\n\n    # Override with kwargs\n    for section in kwargs:\n        if section in config:\n            for param, value in kwargs[section].items():\n                config[section]['params'][param] = value\n\n    return cls(config)\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.list_presets","title":"<code>list_presets()</code>  <code>staticmethod</code>","text":"<p>List available presets.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.list_presets--returns","title":"Returns","text":"<p>list     List of preset names</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>@staticmethod\ndef list_presets():\n    \"\"\"\n    List available presets.\n\n    Returns\n    -------\n    list\n        List of preset names\n    \"\"\"\n    return [\n        'slic_mean_color',\n        'slic_color_position',\n        'patches_color',\n        'patches_cnn',\n        'tiny_graph',\n        'superpixel_comprehensive'\n    ]\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.optimize_config","title":"<code>optimize_config(metric_fn, param_grid, images, labels=None)</code>","text":"<p>Optimize configuration parameters using grid search.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.optimize_config--parameters","title":"Parameters","text":"<p>metric_fn : callable     Function to evaluate graph quality param_grid : dict     Dictionary of parameters to search images : list     List of images to use for optimization labels : list, optional     List of labels if needed for evaluation, by default None</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.optimize_config--returns","title":"Returns","text":"<p>dict     Optimized configuration</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>def optimize_config(self, metric_fn, param_grid, images, labels=None):\n    \"\"\"\n    Optimize configuration parameters using grid search.\n\n    Parameters\n    ----------\n    metric_fn : callable\n        Function to evaluate graph quality\n    param_grid : dict\n        Dictionary of parameters to search\n    images : list\n        List of images to use for optimization\n    labels : list, optional\n        List of labels if needed for evaluation, by default None\n\n    Returns\n    -------\n    dict\n        Optimized configuration\n    \"\"\"\n    best_score = float('-inf')\n    best_config = self.config.copy()\n\n    # Implement a simple grid search\n    import itertools\n\n    # Generate parameter combinations\n    param_names = list(param_grid.keys())\n    param_values = list(param_grid.values())\n    param_combinations = list(itertools.product(*param_values))\n\n    for combination in param_combinations:\n        # Update configuration\n        config = self.config.copy()\n\n        for i, param_name in enumerate(param_names):\n            # Parse parameter path (e.g., \"node_creation.params.n_segments\")\n            parts = param_name.split('.')\n            target = config\n\n            for part in parts[:-1]:\n                if part not in target:\n                    target[part] = {}\n                target = target[part]\n\n            # Set parameter value\n            target[parts[-1]] = combination[i]\n\n        # Create graph builder with updated config\n        pipeline = GraphPipeline(config)\n\n        # Process images\n        graphs = pipeline.batch_process(images)\n\n        # Evaluate\n        score = metric_fn(graphs, labels)\n\n        # Update best config if score is better\n        if score &gt; best_score:\n            best_score = score\n            best_config = config\n\n    # Update current config\n    self.config = best_config\n    self.graph_builder = GraphPresets.custom_preset(self.config)\n\n    return best_config\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.process","title":"<code>process(image)</code>","text":"<p>Process an image into a graph.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.process--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image with shape (H, W, C)</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.process--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph representation of the image</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>def process(self, image):\n    \"\"\"\n    Process an image into a graph.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image with shape (H, W, C)\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph representation of the image\n    \"\"\"\n    return self.graph_builder(image)\n</code></pre>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.save_config","title":"<code>save_config(path)</code>","text":"<p>Save the configuration to a file.</p>"},{"location":"imgraph/pipeline/config_pipeline/#imgraph.pipeline.config_pipeline.GraphPipeline.save_config--parameters","title":"Parameters","text":"<p>path : str     Path to save the configuration</p> Source code in <code>imgraph/pipeline/config_pipeline.py</code> <pre><code>def save_config(self, path):\n    \"\"\"\n    Save the configuration to a file.\n\n    Parameters\n    ----------\n    path : str\n        Path to save the configuration\n    \"\"\"\n    with open(path, 'w') as f:\n        json.dump(self.config, f, indent=4)\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/","title":"Folder Pipeline","text":""},{"location":"imgraph/pipeline/folder_pipeline/#imgraphpipelinefolder_pipeline","title":"<code>imgraph.pipeline.folder_pipeline</code>","text":"<p>Pipeline for processing folders of images into graphs.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset","title":"<code>FolderGraphDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch Geometric dataset for graph data stored in a folder.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset--parameters","title":"Parameters","text":"<p>root : str     Root directory where the processed graphs are stored with_labels : bool, optional     Whether to include labels, by default False label_fn : callable, optional     Function to extract labels from file paths, by default None transform : callable, optional     Transform function applied to each data object, by default None pre_transform : callable, optional     Pre-transform function applied to each data object, by default None</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>class FolderGraphDataset(Dataset):\n    \"\"\"\n    PyTorch Geometric dataset for graph data stored in a folder.\n\n    Parameters\n    ----------\n    root : str\n        Root directory where the processed graphs are stored\n    with_labels : bool, optional\n        Whether to include labels, by default False\n    label_fn : callable, optional\n        Function to extract labels from file paths, by default None\n    transform : callable, optional\n        Transform function applied to each data object, by default None\n    pre_transform : callable, optional\n        Pre-transform function applied to each data object, by default None\n    \"\"\"\n\n    def __init__(self, root, with_labels=False, label_fn=None, transform=None, pre_transform=None):\n        \"\"\"Initialize the FolderGraphDataset.\"\"\"\n        self.root = root\n        self.with_labels = with_labels\n        self.label_fn = label_fn\n        self.graph_files = glob.glob(os.path.join(root, '**/*.pt'), recursive=True)\n\n        # Ensure deterministic order\n        self.graph_files = sorted(self.graph_files)\n\n        super(FolderGraphDataset, self).__init__(root, transform, pre_transform)\n\n    @property\n    def raw_file_names(self):\n        \"\"\"List of raw file names.\"\"\"\n        return []\n\n    @property\n    def processed_file_names(self):\n        \"\"\"List of processed file names.\"\"\"\n        return [os.path.basename(f) for f in self.graph_files]\n\n    def download(self):\n        \"\"\"Download method (not used).\"\"\"\n        pass\n\n    def process(self):\n        \"\"\"Process method (not used).\"\"\"\n        pass\n\n    def len(self):\n        \"\"\"\n        Get the number of graphs in the dataset.\n\n        Returns\n        -------\n        int\n            Number of graphs\n        \"\"\"\n        return len(self.graph_files)\n\n    def get(self, idx):\n        \"\"\"\n        Get a graph by index.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the graph\n\n        Returns\n        -------\n        torch_geometric.data.Data\n            Graph data object\n        \"\"\"\n        # Load graph\n        graph = torch.load(self.graph_files[idx])\n\n        # Add label if required\n        if self.with_labels and not hasattr(graph, 'y'):\n            if self.label_fn is not None:\n                label = self.label_fn(self.graph_files[idx])\n                graph.y = torch.tensor(label, dtype=torch.long)\n            else:\n                # Try to extract label from directory name\n                parent_dir = os.path.basename(os.path.dirname(self.graph_files[idx]))\n                try:\n                    label = int(parent_dir)\n                    graph.y = torch.tensor(label, dtype=torch.long)\n                except ValueError:\n                    # If parent directory name is not an integer, use it as a string label\n                    graph.y = parent_dir\n\n        return graph\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.processed_file_names","title":"<code>processed_file_names</code>  <code>property</code>","text":"<p>List of processed file names.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.raw_file_names","title":"<code>raw_file_names</code>  <code>property</code>","text":"<p>List of raw file names.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.__init__","title":"<code>__init__(root, with_labels=False, label_fn=None, transform=None, pre_transform=None)</code>","text":"<p>Initialize the FolderGraphDataset.</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def __init__(self, root, with_labels=False, label_fn=None, transform=None, pre_transform=None):\n    \"\"\"Initialize the FolderGraphDataset.\"\"\"\n    self.root = root\n    self.with_labels = with_labels\n    self.label_fn = label_fn\n    self.graph_files = glob.glob(os.path.join(root, '**/*.pt'), recursive=True)\n\n    # Ensure deterministic order\n    self.graph_files = sorted(self.graph_files)\n\n    super(FolderGraphDataset, self).__init__(root, transform, pre_transform)\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.download","title":"<code>download()</code>","text":"<p>Download method (not used).</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def download(self):\n    \"\"\"Download method (not used).\"\"\"\n    pass\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.get","title":"<code>get(idx)</code>","text":"<p>Get a graph by index.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.get--parameters","title":"Parameters","text":"<p>idx : int     Index of the graph</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.get--returns","title":"Returns","text":"<p>torch_geometric.data.Data     Graph data object</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def get(self, idx):\n    \"\"\"\n    Get a graph by index.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the graph\n\n    Returns\n    -------\n    torch_geometric.data.Data\n        Graph data object\n    \"\"\"\n    # Load graph\n    graph = torch.load(self.graph_files[idx])\n\n    # Add label if required\n    if self.with_labels and not hasattr(graph, 'y'):\n        if self.label_fn is not None:\n            label = self.label_fn(self.graph_files[idx])\n            graph.y = torch.tensor(label, dtype=torch.long)\n        else:\n            # Try to extract label from directory name\n            parent_dir = os.path.basename(os.path.dirname(self.graph_files[idx]))\n            try:\n                label = int(parent_dir)\n                graph.y = torch.tensor(label, dtype=torch.long)\n            except ValueError:\n                # If parent directory name is not an integer, use it as a string label\n                graph.y = parent_dir\n\n    return graph\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.len","title":"<code>len()</code>","text":"<p>Get the number of graphs in the dataset.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.len--returns","title":"Returns","text":"<p>int     Number of graphs</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def len(self):\n    \"\"\"\n    Get the number of graphs in the dataset.\n\n    Returns\n    -------\n    int\n        Number of graphs\n    \"\"\"\n    return len(self.graph_files)\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphDataset.process","title":"<code>process()</code>","text":"<p>Process method (not used).</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def process(self):\n    \"\"\"Process method (not used).\"\"\"\n    pass\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline","title":"<code>FolderGraphPipeline</code>","text":"<p>A pipeline for processing folders of images into graphs.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline--parameters","title":"Parameters","text":"<p>input_dir : str     Path to input directory containing images output_dir : str     Path to output directory for saving graphs config : dict or str, optional     Configuration dictionary or path to configuration file, by default None (uses default config) image_extensions : list, optional     List of image file extensions to process, by default ['.jpg', '.jpeg', '.png'] load_fn : callable, optional     Function to load images, by default None (uses default loader)</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>class FolderGraphPipeline:\n    \"\"\"\n    A pipeline for processing folders of images into graphs.\n\n    Parameters\n    ----------\n    input_dir : str\n        Path to input directory containing images\n    output_dir : str\n        Path to output directory for saving graphs\n    config : dict or str, optional\n        Configuration dictionary or path to configuration file, by default None (uses default config)\n    image_extensions : list, optional\n        List of image file extensions to process, by default ['.jpg', '.jpeg', '.png']\n    load_fn : callable, optional\n        Function to load images, by default None (uses default loader)\n    \"\"\"\n\n    def __init__(self, input_dir, output_dir, config=None, image_extensions=None, load_fn=None):\n        \"\"\"Initialize the FolderGraphPipeline.\"\"\"\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n\n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Set image extensions\n        self.image_extensions = image_extensions or ['.jpg', '.jpeg', '.png']\n\n        # Set image loader\n        self.load_fn = load_fn or self._default_loader\n\n        # Create graph pipeline\n        if config is None:\n            self.graph_pipeline = GraphPipeline(GraphPipeline.create_default_config())\n        else:\n            self.graph_pipeline = GraphPipeline(config)\n\n        # Save config to output directory\n        self.graph_pipeline.save_config(os.path.join(output_dir, 'graph_config.json'))\n\n    def _default_loader(self, path):\n        \"\"\"\n        Default image loader.\n\n        Parameters\n        ----------\n        path : str\n            Path to image file\n\n        Returns\n        -------\n        numpy.ndarray\n            Loaded image with shape (H, W, C)\n        \"\"\"\n        try:\n            import cv2\n            # Load image in BGR format\n            img = cv2.imread(path)\n            # Convert to RGB\n            if img is not None:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            return img\n        except ImportError:\n            try:\n                from PIL import Image\n                img = Image.open(path).convert('RGB')\n                return np.array(img)\n            except ImportError:\n                raise ImportError(\"Either OpenCV or PIL is required for loading images\")\n\n    def get_image_files(self):\n        \"\"\"\n        Get list of image files in the input directory.\n\n        Returns\n        -------\n        list\n            List of image file paths\n        \"\"\"\n        image_files = []\n\n        for ext in self.image_extensions:\n            image_files.extend(glob.glob(os.path.join(self.input_dir, f'**/*{ext}'), recursive=True))\n\n        return sorted(image_files)\n\n    def process(self, verbose=True):\n        \"\"\"\n        Process all images in the input directory.\n\n        Parameters\n        ----------\n        verbose : bool, optional\n            Whether to display progress bar, by default True\n\n        Returns\n        -------\n        list\n            List of output file paths\n        \"\"\"\n        # Get image files\n        image_files = self.get_image_files()\n\n        # Check if there are any image files\n        if not image_files:\n            print(f\"No images found in {self.input_dir} with extensions {self.image_extensions}\")\n            return []\n\n        # Process images\n        output_files = []\n\n        # Create progress bar if verbose\n        if verbose:\n            pbar = tqdm(total=len(image_files), desc=\"Processing images\")\n\n        for image_path in image_files:\n            # Get relative path\n            rel_path = os.path.relpath(image_path, self.input_dir)\n\n            # Create output path\n            output_path = os.path.join(self.output_dir, os.path.splitext(rel_path)[0] + '.pt')\n\n            # Create output directory if it doesn't exist\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n            # Load image\n            try:\n                image = self.load_fn(image_path)\n\n                # Skip if image loading failed\n                if image is None:\n                    print(f\"Failed to load image: {image_path}\")\n                    if verbose:\n                        pbar.update(1)\n                    continue\n\n                # Process image\n                graph = self.graph_pipeline.process(image)\n\n                # Save graph\n                torch.save(graph, output_path)\n\n                # Add to output files\n                output_files.append(output_path)\n\n            except Exception as e:\n                print(f\"Error processing {image_path}: {e}\")\n\n            # Update progress bar\n            if verbose:\n                pbar.update(1)\n\n        # Close progress bar\n        if verbose:\n            pbar.close()\n\n        return output_files\n\n    def create_dataset(self, with_labels=False, label_fn=None):\n        \"\"\"\n        Create a PyTorch Geometric dataset from the processed graphs.\n\n        Parameters\n        ----------\n        with_labels : bool, optional\n            Whether to include labels, by default False\n        label_fn : callable, optional\n            Function to extract labels from file paths, by default None\n\n        Returns\n        -------\n        torch_geometric.data.Dataset\n            Dataset of graphs\n        \"\"\"\n        return FolderGraphDataset(\n            self.output_dir,\n            with_labels=with_labels,\n            label_fn=label_fn,\n            transform=None,\n            pre_transform=None\n        )\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.__init__","title":"<code>__init__(input_dir, output_dir, config=None, image_extensions=None, load_fn=None)</code>","text":"<p>Initialize the FolderGraphPipeline.</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def __init__(self, input_dir, output_dir, config=None, image_extensions=None, load_fn=None):\n    \"\"\"Initialize the FolderGraphPipeline.\"\"\"\n    self.input_dir = input_dir\n    self.output_dir = output_dir\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Set image extensions\n    self.image_extensions = image_extensions or ['.jpg', '.jpeg', '.png']\n\n    # Set image loader\n    self.load_fn = load_fn or self._default_loader\n\n    # Create graph pipeline\n    if config is None:\n        self.graph_pipeline = GraphPipeline(GraphPipeline.create_default_config())\n    else:\n        self.graph_pipeline = GraphPipeline(config)\n\n    # Save config to output directory\n    self.graph_pipeline.save_config(os.path.join(output_dir, 'graph_config.json'))\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.create_dataset","title":"<code>create_dataset(with_labels=False, label_fn=None)</code>","text":"<p>Create a PyTorch Geometric dataset from the processed graphs.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.create_dataset--parameters","title":"Parameters","text":"<p>with_labels : bool, optional     Whether to include labels, by default False label_fn : callable, optional     Function to extract labels from file paths, by default None</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.create_dataset--returns","title":"Returns","text":"<p>torch_geometric.data.Dataset     Dataset of graphs</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def create_dataset(self, with_labels=False, label_fn=None):\n    \"\"\"\n    Create a PyTorch Geometric dataset from the processed graphs.\n\n    Parameters\n    ----------\n    with_labels : bool, optional\n        Whether to include labels, by default False\n    label_fn : callable, optional\n        Function to extract labels from file paths, by default None\n\n    Returns\n    -------\n    torch_geometric.data.Dataset\n        Dataset of graphs\n    \"\"\"\n    return FolderGraphDataset(\n        self.output_dir,\n        with_labels=with_labels,\n        label_fn=label_fn,\n        transform=None,\n        pre_transform=None\n    )\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.get_image_files","title":"<code>get_image_files()</code>","text":"<p>Get list of image files in the input directory.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.get_image_files--returns","title":"Returns","text":"<p>list     List of image file paths</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def get_image_files(self):\n    \"\"\"\n    Get list of image files in the input directory.\n\n    Returns\n    -------\n    list\n        List of image file paths\n    \"\"\"\n    image_files = []\n\n    for ext in self.image_extensions:\n        image_files.extend(glob.glob(os.path.join(self.input_dir, f'**/*{ext}'), recursive=True))\n\n    return sorted(image_files)\n</code></pre>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.process","title":"<code>process(verbose=True)</code>","text":"<p>Process all images in the input directory.</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.process--parameters","title":"Parameters","text":"<p>verbose : bool, optional     Whether to display progress bar, by default True</p>"},{"location":"imgraph/pipeline/folder_pipeline/#imgraph.pipeline.folder_pipeline.FolderGraphPipeline.process--returns","title":"Returns","text":"<p>list     List of output file paths</p> Source code in <code>imgraph/pipeline/folder_pipeline.py</code> <pre><code>def process(self, verbose=True):\n    \"\"\"\n    Process all images in the input directory.\n\n    Parameters\n    ----------\n    verbose : bool, optional\n        Whether to display progress bar, by default True\n\n    Returns\n    -------\n    list\n        List of output file paths\n    \"\"\"\n    # Get image files\n    image_files = self.get_image_files()\n\n    # Check if there are any image files\n    if not image_files:\n        print(f\"No images found in {self.input_dir} with extensions {self.image_extensions}\")\n        return []\n\n    # Process images\n    output_files = []\n\n    # Create progress bar if verbose\n    if verbose:\n        pbar = tqdm(total=len(image_files), desc=\"Processing images\")\n\n    for image_path in image_files:\n        # Get relative path\n        rel_path = os.path.relpath(image_path, self.input_dir)\n\n        # Create output path\n        output_path = os.path.join(self.output_dir, os.path.splitext(rel_path)[0] + '.pt')\n\n        # Create output directory if it doesn't exist\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n        # Load image\n        try:\n            image = self.load_fn(image_path)\n\n            # Skip if image loading failed\n            if image is None:\n                print(f\"Failed to load image: {image_path}\")\n                if verbose:\n                    pbar.update(1)\n                continue\n\n            # Process image\n            graph = self.graph_pipeline.process(image)\n\n            # Save graph\n            torch.save(graph, output_path)\n\n            # Add to output files\n            output_files.append(output_path)\n\n        except Exception as e:\n            print(f\"Error processing {image_path}: {e}\")\n\n        # Update progress bar\n        if verbose:\n            pbar.update(1)\n\n    # Close progress bar\n    if verbose:\n        pbar.close()\n\n    return output_files\n</code></pre>"},{"location":"imgraph/pipeline/from_dataset/","title":"From Dataset","text":""},{"location":"imgraph/pipeline/from_dataset/#imgraphpipelinefrom_dataset","title":"<code>imgraph.pipeline.from_dataset</code>","text":""},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.create_graph_pipeline","title":"<code>create_graph_pipeline(config=None)</code>","text":"<p>Create a graph pipeline.</p>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.create_graph_pipeline--parameters","title":"Parameters","text":"<p>config : dict or str, optional     Configuration dictionary or path to configuration file, by default None</p>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.create_graph_pipeline--returns","title":"Returns","text":"<p>GraphPipeline     Graph pipeline</p> Source code in <code>imgraph/pipeline/from_dataset.py</code> <pre><code>def create_graph_pipeline(config=None):\n    \"\"\"\n    Create a graph pipeline.\n\n    Parameters\n    ----------\n    config : dict or str, optional\n        Configuration dictionary or path to configuration file, by default None\n\n    Returns\n    -------\n    GraphPipeline\n        Graph pipeline\n    \"\"\"\n    if config is None:\n        return GraphPipeline(GraphPipeline.create_default_config())\n    else:\n        return GraphPipeline(config)\n</code></pre>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.load_saved_datasets","title":"<code>load_saved_datasets(dataset_dir)</code>","text":"<p>Load saved graph datasets.</p>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.load_saved_datasets--parameters","title":"Parameters","text":"<p>dataset_dir : str     Directory containing saved graph datasets</p>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.load_saved_datasets--returns","title":"Returns","text":"<p>dict     Dictionary of datasets</p> Source code in <code>imgraph/pipeline/from_dataset.py</code> <pre><code>def load_saved_datasets(dataset_dir):\n    \"\"\"\n    Load saved graph datasets.\n\n    Parameters\n    ----------\n    dataset_dir : str\n        Directory containing saved graph datasets\n\n    Returns\n    -------\n    dict\n        Dictionary of datasets\n    \"\"\"\n    datasets = {}\n\n    # List all files in the directory\n    for root, dirs, files in os.walk(dataset_dir):\n        for file in files:\n            if file.endswith('.pt'):\n                # Load dataset\n                filepath = os.path.join(root, file)\n                dataset_name = os.path.splitext(file)[0]\n\n                try:\n                    dataset = torch.load(filepath)\n                    datasets[dataset_name] = dataset\n                    print(f\"Loaded dataset: {dataset_name}\")\n                except Exception as e:\n                    print(f\"Error loading dataset {dataset_name}: {e}\")\n\n    return datasets\n</code></pre>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.process_dataset","title":"<code>process_dataset(dataset, graph_pipeline, verbose=True)</code>","text":"<p>Process a dataset into graphs.</p>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.process_dataset--parameters","title":"Parameters","text":"<p>dataset : torch.utils.data.Dataset     Dataset to process graph_pipeline : GraphPipeline     Graph pipeline to use for processing verbose : bool, optional     Whether to display progress bar, by default True</p>"},{"location":"imgraph/pipeline/from_dataset/#imgraph.pipeline.from_dataset.process_dataset--returns","title":"Returns","text":"<p>list     List of graph data objects</p> Source code in <code>imgraph/pipeline/from_dataset.py</code> <pre><code>def process_dataset(dataset, graph_pipeline, verbose=True):\n    \"\"\"\n    Process a dataset into graphs.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset to process\n    graph_pipeline : GraphPipeline\n        Graph pipeline to use for processing\n    verbose : bool, optional\n        Whether to display progress bar, by default True\n\n    Returns\n    -------\n    list\n        List of graph data objects\n    \"\"\"\n    graphs = []\n\n    # Create progress bar if verbose\n    if verbose:\n        pbar = tqdm(total=len(dataset), desc=\"Processing dataset\")\n\n    for i in range(len(dataset)):\n        # Get data\n        data = dataset[i]\n\n        # Extract image and label\n        if isinstance(data, tuple):\n            image, label = data\n        else:\n            image = data\n            label = None\n\n        # Convert to numpy array if needed\n        if isinstance(image, torch.Tensor):\n            image = image.permute(1, 2, 0).numpy()\n\n        # Process image\n        try:\n            graph = graph_pipeline.process(image)\n\n            # Add label if available\n            if label is not None:\n                if isinstance(label, torch.Tensor):\n                    graph.y = label\n                else:\n                    graph.y = torch.tensor(label, dtype=torch.long)\n\n            # Add to list\n            graphs.append(graph)\n\n        except Exception as e:\n            print(f\"Error processing image {i}: {e}\")\n\n        # Update progress bar\n        if verbose:\n            pbar.update(1)\n\n    # Close progress bar\n    if verbose:\n        pbar.close()\n\n    return graphs\n</code></pre>"},{"location":"imgraph/reader/read_directory/","title":"Read Directory","text":""},{"location":"imgraph/reader/read_directory/#imgraphreaderread_directory","title":"<code>imgraph.reader.read_directory</code>","text":""},{"location":"imgraph/reader/read_directory/#imgraph.reader.read_directory.get_directories_from_path","title":"<code>get_directories_from_path(path)</code>","text":"<p>Get all directories from a path. Args:     path (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs  Returns:     A list of directories. The reruned list is sorted used to iterate through each folder.</p> Source code in <code>imgraph/reader/read_directory.py</code> <pre><code>def get_directories_from_path(path):\n    \"\"\"Get all directories from a path.\n    Args:\n        path (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs \n    Returns:\n        A list of directories. The reruned list is sorted used to iterate through each folder.\n    \"\"\"\n    return [os.path.join(path, f) for f in os.listdir(path) if os.path.isdir(os.path.join(path, f)) and not f.startswith('__')]\n</code></pre>"},{"location":"imgraph/reader/read_directory/#imgraph.reader.read_directory.get_file_categoryies_from_path","title":"<code>get_file_categoryies_from_path(path_list)</code>","text":"<p>Get all directories from a path. Args:     path (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs  Returns:     A list of directories. Returns available catgory names, test, train, val, or class names.</p> Source code in <code>imgraph/reader/read_directory.py</code> <pre><code>def get_file_categoryies_from_path(path_list):\n    \"\"\"Get all directories from a path.\n    Args:\n        path (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs \n    Returns:\n        A list of directories. Returns available catgory names, test, train, val, or class names.\n    \"\"\"\n    return [f.split('/')[-1] for f in path_list]\n</code></pre>"},{"location":"imgraph/reader/read_directory/#imgraph.reader.read_directory.get_files_from_path","title":"<code>get_files_from_path(folderpath)</code>","text":"<p>Get all files from a path. Args:     folderpath (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs  Returns:     A list of files. The reruned list is sorted used to iterate through each folder.</p> Source code in <code>imgraph/reader/read_directory.py</code> <pre><code>def get_files_from_path(folderpath):\n    \"\"\"Get all files from a path.\n    Args:\n        folderpath (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs \n    Returns:\n        A list of files. The reruned list is sorted used to iterate through each folder.\n    \"\"\"\n    return [os.path.join(folderpath, f) for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f))]\n</code></pre>"},{"location":"imgraph/reader/read_directory/#imgraph.reader.read_directory.get_files_from_path_with_extension","title":"<code>get_files_from_path_with_extension(folderpath, extension)</code>","text":"<p>Get all files from a path. Args:     folderpath (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs  Returns:     A list of files. The reruned list is sorted used to iterate through each folder.</p> Source code in <code>imgraph/reader/read_directory.py</code> <pre><code>def get_files_from_path_with_extension(folderpath, extension):\n    \"\"\"Get all files from a path.\n    Args:\n        folderpath (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs \n    Returns:\n        A list of files. The reruned list is sorted used to iterate through each folder.\n    \"\"\"\n    return [os.path.join(folderpath, f) for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f)) and f.endswith(extension)]\n</code></pre>"},{"location":"imgraph/reader/read_directory/#imgraph.reader.read_directory.get_files_from_path_with_extension_and_prefix","title":"<code>get_files_from_path_with_extension_and_prefix(folderpath, extension, prefix)</code>","text":"<p>Get all files from a path. Args:     folderpath (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs  Returns:     A list of files. The reruned list is sorted used to iterate through each folder.</p> Source code in <code>imgraph/reader/read_directory.py</code> <pre><code>def get_files_from_path_with_extension_and_prefix(folderpath, extension, prefix):\n    \"\"\"Get all files from a path.\n    Args:\n        folderpath (str): The path to a local folder. this folder should contain train, test, validation folders of images of graphs \n    Returns:\n        A list of files. The reruned list is sorted used to iterate through each folder.\n    \"\"\"\n    return [os.path.join(folderpath, f) for f in os.listdir(folderpath) if os.path.isfile(os.path.join(folderpath, f)) and f.endswith(extension) and f.startswith(prefix)]\n</code></pre>"},{"location":"imgraph/reader/read_files/","title":"Read Files","text":""},{"location":"imgraph/reader/read_files/#imgraphreaderread_files","title":"<code>imgraph.reader.read_files</code>","text":""},{"location":"imgraph/reader/read_files/#imgraph.reader.read_files.read_csv_file","title":"<code>read_csv_file(path)</code>","text":"<p>Reads a csv file from a file. Args:     path (str): The path to a local csv file. Returns:     A csv file.</p> Source code in <code>imgraph/reader/read_files.py</code> <pre><code>def read_csv_file(path):\n    \"\"\"Reads a csv file from a file.\n    Args:\n        path (str): The path to a local csv file.\n    Returns:\n        A csv file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"imgraph/reader/read_files/#imgraph.reader.read_files.read_graph","title":"<code>read_graph(path)</code>","text":"<p>Reads a graph from a file. Args:     path (str): The path to a local graph file. Returns:     A networkx graph.</p> Source code in <code>imgraph/reader/read_files.py</code> <pre><code>def read_graph(path):\n    \"\"\"Reads a graph from a file.\n    Args:\n        path (str): The path to a local graph file.\n    Returns:\n        A networkx graph.\n    \"\"\"\n    return nx.read_gpickle(path)\n</code></pre>"},{"location":"imgraph/reader/read_files/#imgraph.reader.read_files.read_image","title":"<code>read_image(path, name, backend='PIL', **kwargs)</code>","text":"<p>Reads an image from a file. Args:     path (str): The path to a local image file. Returns:     A PIL image.</p> Source code in <code>imgraph/reader/read_files.py</code> <pre><code>def read_image(path, name : str ,backend='PIL', **kwargs):\n    \"\"\"Reads an image from a file.\n    Args:\n        path (str): The path to a local image file.\n    Returns:\n        A PIL image.\n    \"\"\"\n    image = imread(path)\n    height, widht = 0,0\n    if len(image.shape) &gt;= 3:\n        height, width, channel = image.shape\n    else:\n        height,width = image.shape\n    #height, width = image.shape\n    # image = image[0:height, 10:width-10]\n    try:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (500, 500))\n    except Exception as e:\n        print(\"Error in resizing image: \", name, \" with error: \", e)\n    # image = img_as_float(image)\n    return image,name\n</code></pre>"},{"location":"imgraph/reader/read_files/#imgraph.reader.read_files.read_pickle_file","title":"<code>read_pickle_file(path)</code>","text":"<p>Reads a pickle file from a file. Args:     path (str): The path to a local pickle file. Returns:     A pickle file.</p> Source code in <code>imgraph/reader/read_files.py</code> <pre><code>def read_pickle_file(path):\n    \"\"\"Reads a pickle file from a file.\n    Args:\n        path (str): The path to a local pickle file.\n    Returns:\n        A pickle file.\n    \"\"\"\n\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"imgraph/training/trainer/","title":"Trainer","text":""},{"location":"imgraph/training/trainer/#imgraphtrainingtrainer","title":"<code>imgraph.training.trainer</code>","text":"<p>Trainer class for graph neural networks.</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer","title":"<code>Trainer</code>","text":"<p>Trainer class for graph neural networks.</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer--parameters","title":"Parameters","text":"<p>model : torch.nn.Module     Model to train optimizer : torch.optim.Optimizer     Optimizer for training criterion : callable     Loss function device : torch.device     Device to train on early_stopping : EarlyStopping, optional     Early stopping object, by default None</p> Source code in <code>imgraph/training/trainer.py</code> <pre><code>class Trainer:\n    \"\"\"\n    Trainer class for graph neural networks.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to train\n    optimizer : torch.optim.Optimizer\n        Optimizer for training\n    criterion : callable\n        Loss function\n    device : torch.device\n        Device to train on\n    early_stopping : EarlyStopping, optional\n        Early stopping object, by default None\n    \"\"\"\n\n    def __init__(self, model, optimizer, criterion, device, early_stopping=None):\n        \"\"\"Initialize the Trainer.\"\"\"\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.device = device\n        self.early_stopping = early_stopping\n\n    def train_epoch(self, loader):\n        \"\"\"\n        Trains the model for one epoch.\n\n        Parameters\n        ----------\n        loader : torch_geometric.loader.DataLoader\n            Data loader\n\n        Returns\n        -------\n        tuple\n            (loss, accuracy)\n        \"\"\"\n        self.model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        # Progress bar\n        pbar = tqdm(loader, desc=\"Training\", leave=False)\n\n        # Iterate over batches\n        for data in pbar:\n            # Move data to device\n            data = data.to(self.device)\n\n            # Zero gradients\n            self.optimizer.zero_grad()\n\n            # Forward pass\n            out = self.model(data)\n\n            # Compute loss\n            loss = self.criterion(out, data.y)\n\n            # Backward pass\n            loss.backward()\n\n            # Update weights\n            self.optimizer.step()\n\n            # Update statistics\n            total_loss += loss.item() * data.num_graphs\n\n            # Compute accuracy\n            pred = out.argmax(dim=1)\n            correct += pred.eq(data.y).sum().item()\n            total += data.num_graphs\n\n            # Update progress bar\n            pbar.set_postfix(loss=loss.item(), acc=correct/total)\n\n        # Close progress bar\n        pbar.close()\n\n        # Compute epoch statistics\n        epoch_loss = total_loss / len(loader.dataset)\n        epoch_acc = correct / total\n\n        return epoch_loss, epoch_acc\n\n    def validate(self, loader):\n        \"\"\"\n        Validates the model on the given data loader.\n\n        Parameters\n        ----------\n        loader : torch_geometric.loader.DataLoader\n            Data loader\n\n        Returns\n        -------\n        tuple\n            (loss, accuracy)\n        \"\"\"\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        # Disable gradients\n        with torch.no_grad():\n            # Iterate over batches\n            for data in loader:\n                # Move data to device\n                data = data.to(self.device)\n\n                # Forward pass\n                out = self.model(data)\n\n                # Compute loss\n                loss = self.criterion(out, data.y)\n\n                # Update statistics\n                total_loss += loss.item() * data.num_graphs\n\n                # Compute accuracy\n                pred = out.argmax(dim=1)\n                correct += pred.eq(data.y).sum().item()\n                total += data.num_graphs\n\n        # Compute epoch statistics\n        epoch_loss = total_loss / len(loader.dataset)\n        epoch_acc = correct / total\n\n        return epoch_loss, epoch_acc\n\n    def fit(self, train_loader, val_loader, epochs=100):\n        \"\"\"\n        Trains the model for the given number of epochs.\n\n        Parameters\n        ----------\n        train_loader : torch_geometric.loader.DataLoader\n            Training data loader\n        val_loader : torch_geometric.loader.DataLoader\n            Validation data loader\n        epochs : int, optional\n            Number of epochs, by default 100\n\n        Returns\n        -------\n        dict\n            Training history\n        \"\"\"\n        # Initialize history\n        history = {\n            'train_loss': [],\n            'val_loss': [],\n            'train_acc': [],\n            'val_acc': []\n        }\n\n        # Initialize best model state\n        best_val_loss = float('inf')\n        best_model_state = None\n\n        # Start timer\n        start_time = time.time()\n\n        # Training loop\n        for epoch in range(epochs):\n            # Train epoch\n            train_loss, train_acc = self.train_epoch(train_loader)\n\n            # Validate\n            val_loss, val_acc = self.validate(val_loader)\n\n            # Update history\n            history['train_loss'].append(train_loss)\n            history['val_loss'].append(val_loss)\n            history['train_acc'].append(train_acc)\n            history['val_acc'].append(val_acc)\n\n            # Print progress\n            print(f\"Epoch {epoch+1}/{epochs} - \"\n                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n            # Check if this is the best model\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model_state = self.model.state_dict().copy()\n\n            # Early stopping\n            if self.early_stopping is not None:\n                self.early_stopping(val_loss)\n                if self.early_stopping.early_stop:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n\n        # End timer\n        end_time = time.time()\n\n        # Print training time\n        training_time = end_time - start_time\n        print(f\"Training completed in {training_time:.2f} seconds\")\n\n        # Load best model\n        if best_model_state is not None:\n            self.model.load_state_dict(best_model_state)\n\n        return history\n\n    def predict(self, loader):\n        \"\"\"\n        Makes predictions with the model.\n\n        Parameters\n        ----------\n        loader : torch_geometric.loader.DataLoader\n            Data loader\n\n        Returns\n        -------\n        tuple\n            (predictions, targets)\n        \"\"\"\n        self.model.eval()\n        predictions = []\n        targets = []\n\n        # Disable gradients\n        with torch.no_grad():\n            # Iterate over batches\n            for data in loader:\n                # Move data to device\n                data = data.to(self.device)\n\n                # Forward pass\n                out = self.model(data)\n\n                # Get predictions\n                pred = out.argmax(dim=1)\n\n                # Add to lists\n                predictions.extend(pred.cpu().numpy())\n                targets.extend(data.y.cpu().numpy())\n\n        return np.array(predictions), np.array(targets)\n\n    def evaluate(self, loader):\n        \"\"\"\n        Evaluates the model on the given data loader.\n\n        Parameters\n        ----------\n        loader : torch_geometric.loader.DataLoader\n            Data loader\n\n        Returns\n        -------\n        dict\n            Evaluation metrics\n        \"\"\"\n        # Get predictions\n        predictions, targets = self.predict(loader)\n\n        # Compute accuracy\n        accuracy = np.mean(predictions == targets)\n\n        # Return metrics\n        return {\n            'accuracy': accuracy,\n            'predictions': predictions,\n            'targets': targets\n        }\n</code></pre>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.__init__","title":"<code>__init__(model, optimizer, criterion, device, early_stopping=None)</code>","text":"<p>Initialize the Trainer.</p> Source code in <code>imgraph/training/trainer.py</code> <pre><code>def __init__(self, model, optimizer, criterion, device, early_stopping=None):\n    \"\"\"Initialize the Trainer.\"\"\"\n    self.model = model\n    self.optimizer = optimizer\n    self.criterion = criterion\n    self.device = device\n    self.early_stopping = early_stopping\n</code></pre>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.evaluate","title":"<code>evaluate(loader)</code>","text":"<p>Evaluates the model on the given data loader.</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.evaluate--parameters","title":"Parameters","text":"<p>loader : torch_geometric.loader.DataLoader     Data loader</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.evaluate--returns","title":"Returns","text":"<p>dict     Evaluation metrics</p> Source code in <code>imgraph/training/trainer.py</code> <pre><code>def evaluate(self, loader):\n    \"\"\"\n    Evaluates the model on the given data loader.\n\n    Parameters\n    ----------\n    loader : torch_geometric.loader.DataLoader\n        Data loader\n\n    Returns\n    -------\n    dict\n        Evaluation metrics\n    \"\"\"\n    # Get predictions\n    predictions, targets = self.predict(loader)\n\n    # Compute accuracy\n    accuracy = np.mean(predictions == targets)\n\n    # Return metrics\n    return {\n        'accuracy': accuracy,\n        'predictions': predictions,\n        'targets': targets\n    }\n</code></pre>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.fit","title":"<code>fit(train_loader, val_loader, epochs=100)</code>","text":"<p>Trains the model for the given number of epochs.</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.fit--parameters","title":"Parameters","text":"<p>train_loader : torch_geometric.loader.DataLoader     Training data loader val_loader : torch_geometric.loader.DataLoader     Validation data loader epochs : int, optional     Number of epochs, by default 100</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.fit--returns","title":"Returns","text":"<p>dict     Training history</p> Source code in <code>imgraph/training/trainer.py</code> <pre><code>def fit(self, train_loader, val_loader, epochs=100):\n    \"\"\"\n    Trains the model for the given number of epochs.\n\n    Parameters\n    ----------\n    train_loader : torch_geometric.loader.DataLoader\n        Training data loader\n    val_loader : torch_geometric.loader.DataLoader\n        Validation data loader\n    epochs : int, optional\n        Number of epochs, by default 100\n\n    Returns\n    -------\n    dict\n        Training history\n    \"\"\"\n    # Initialize history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_acc': [],\n        'val_acc': []\n    }\n\n    # Initialize best model state\n    best_val_loss = float('inf')\n    best_model_state = None\n\n    # Start timer\n    start_time = time.time()\n\n    # Training loop\n    for epoch in range(epochs):\n        # Train epoch\n        train_loss, train_acc = self.train_epoch(train_loader)\n\n        # Validate\n        val_loss, val_acc = self.validate(val_loader)\n\n        # Update history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n\n        # Print progress\n        print(f\"Epoch {epoch+1}/{epochs} - \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        # Check if this is the best model\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = self.model.state_dict().copy()\n\n        # Early stopping\n        if self.early_stopping is not None:\n            self.early_stopping(val_loss)\n            if self.early_stopping.early_stop:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    # End timer\n    end_time = time.time()\n\n    # Print training time\n    training_time = end_time - start_time\n    print(f\"Training completed in {training_time:.2f} seconds\")\n\n    # Load best model\n    if best_model_state is not None:\n        self.model.load_state_dict(best_model_state)\n\n    return history\n</code></pre>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.predict","title":"<code>predict(loader)</code>","text":"<p>Makes predictions with the model.</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.predict--parameters","title":"Parameters","text":"<p>loader : torch_geometric.loader.DataLoader     Data loader</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.predict--returns","title":"Returns","text":"<p>tuple     (predictions, targets)</p> Source code in <code>imgraph/training/trainer.py</code> <pre><code>def predict(self, loader):\n    \"\"\"\n    Makes predictions with the model.\n\n    Parameters\n    ----------\n    loader : torch_geometric.loader.DataLoader\n        Data loader\n\n    Returns\n    -------\n    tuple\n        (predictions, targets)\n    \"\"\"\n    self.model.eval()\n    predictions = []\n    targets = []\n\n    # Disable gradients\n    with torch.no_grad():\n        # Iterate over batches\n        for data in loader:\n            # Move data to device\n            data = data.to(self.device)\n\n            # Forward pass\n            out = self.model(data)\n\n            # Get predictions\n            pred = out.argmax(dim=1)\n\n            # Add to lists\n            predictions.extend(pred.cpu().numpy())\n            targets.extend(data.y.cpu().numpy())\n\n    return np.array(predictions), np.array(targets)\n</code></pre>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.train_epoch","title":"<code>train_epoch(loader)</code>","text":"<p>Trains the model for one epoch.</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.train_epoch--parameters","title":"Parameters","text":"<p>loader : torch_geometric.loader.DataLoader     Data loader</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.train_epoch--returns","title":"Returns","text":"<p>tuple     (loss, accuracy)</p> Source code in <code>imgraph/training/trainer.py</code> <pre><code>def train_epoch(self, loader):\n    \"\"\"\n    Trains the model for one epoch.\n\n    Parameters\n    ----------\n    loader : torch_geometric.loader.DataLoader\n        Data loader\n\n    Returns\n    -------\n    tuple\n        (loss, accuracy)\n    \"\"\"\n    self.model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    # Progress bar\n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n\n    # Iterate over batches\n    for data in pbar:\n        # Move data to device\n        data = data.to(self.device)\n\n        # Zero gradients\n        self.optimizer.zero_grad()\n\n        # Forward pass\n        out = self.model(data)\n\n        # Compute loss\n        loss = self.criterion(out, data.y)\n\n        # Backward pass\n        loss.backward()\n\n        # Update weights\n        self.optimizer.step()\n\n        # Update statistics\n        total_loss += loss.item() * data.num_graphs\n\n        # Compute accuracy\n        pred = out.argmax(dim=1)\n        correct += pred.eq(data.y).sum().item()\n        total += data.num_graphs\n\n        # Update progress bar\n        pbar.set_postfix(loss=loss.item(), acc=correct/total)\n\n    # Close progress bar\n    pbar.close()\n\n    # Compute epoch statistics\n    epoch_loss = total_loss / len(loader.dataset)\n    epoch_acc = correct / total\n\n    return epoch_loss, epoch_acc\n</code></pre>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.validate","title":"<code>validate(loader)</code>","text":"<p>Validates the model on the given data loader.</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.validate--parameters","title":"Parameters","text":"<p>loader : torch_geometric.loader.DataLoader     Data loader</p>"},{"location":"imgraph/training/trainer/#imgraph.training.trainer.Trainer.validate--returns","title":"Returns","text":"<p>tuple     (loss, accuracy)</p> Source code in <code>imgraph/training/trainer.py</code> <pre><code>def validate(self, loader):\n    \"\"\"\n    Validates the model on the given data loader.\n\n    Parameters\n    ----------\n    loader : torch_geometric.loader.DataLoader\n        Data loader\n\n    Returns\n    -------\n    tuple\n        (loss, accuracy)\n    \"\"\"\n    self.model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    # Disable gradients\n    with torch.no_grad():\n        # Iterate over batches\n        for data in loader:\n            # Move data to device\n            data = data.to(self.device)\n\n            # Forward pass\n            out = self.model(data)\n\n            # Compute loss\n            loss = self.criterion(out, data.y)\n\n            # Update statistics\n            total_loss += loss.item() * data.num_graphs\n\n            # Compute accuracy\n            pred = out.argmax(dim=1)\n            correct += pred.eq(data.y).sum().item()\n            total += data.num_graphs\n\n    # Compute epoch statistics\n    epoch_loss = total_loss / len(loader.dataset)\n    epoch_acc = correct / total\n\n    return epoch_loss, epoch_acc\n</code></pre>"},{"location":"imgraph/training/utils/","title":"Training Utils","text":""},{"location":"imgraph/training/utils/#imgraphtrainingutils","title":"<code>imgraph.training.utils</code>","text":"<p>Utility functions for training graph neural networks.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>Early stopping to prevent overfitting.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.EarlyStopping--parameters","title":"Parameters","text":"<p>patience : int, optional     Number of epochs with no improvement after which training will be stopped, by default 7 verbose : bool, optional     Whether to print messages, by default False delta : float, optional     Minimum change in the monitored quantity to qualify as an improvement, by default 0.0 path : str, optional     Path for the checkpoint to be saved to, by default 'checkpoint.pt'</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>class EarlyStopping:\n    \"\"\"\n    Early stopping to prevent overfitting.\n\n    Parameters\n    ----------\n    patience : int, optional\n        Number of epochs with no improvement after which training will be stopped, by default 7\n    verbose : bool, optional\n        Whether to print messages, by default False\n    delta : float, optional\n        Minimum change in the monitored quantity to qualify as an improvement, by default 0.0\n    path : str, optional\n        Path for the checkpoint to be saved to, by default 'checkpoint.pt'\n    \"\"\"\n\n    def __init__(self, patience=7, verbose=False, delta=0.0, path='checkpoint.pt'):\n        \"\"\"Initialize EarlyStopping.\"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model=None):\n        \"\"\"\n        Call EarlyStopping to check if training should be stopped.\n\n        Parameters\n        ----------\n        val_loss : float\n            Validation loss\n        model : torch.nn.Module, optional\n            Model to save, by default None\n        \"\"\"\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score &lt; self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model=None):\n        \"\"\"\n        Saves model when validation loss decreases.\n\n        Parameters\n        ----------\n        val_loss : float\n            Validation loss\n        model : torch.nn.Module, optional\n            Model to save, by default None\n        \"\"\"\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --&gt; {val_loss:.6f}). Saving model...')\n        if model is not None:\n            torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.EarlyStopping.__call__","title":"<code>__call__(val_loss, model=None)</code>","text":"<p>Call EarlyStopping to check if training should be stopped.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.EarlyStopping.__call__--parameters","title":"Parameters","text":"<p>val_loss : float     Validation loss model : torch.nn.Module, optional     Model to save, by default None</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def __call__(self, val_loss, model=None):\n    \"\"\"\n    Call EarlyStopping to check if training should be stopped.\n\n    Parameters\n    ----------\n    val_loss : float\n        Validation loss\n    model : torch.nn.Module, optional\n        Model to save, by default None\n    \"\"\"\n    score = -val_loss\n\n    if self.best_score is None:\n        self.best_score = score\n        self.save_checkpoint(val_loss, model)\n    elif score &lt; self.best_score + self.delta:\n        self.counter += 1\n        if self.verbose:\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n        if self.counter &gt;= self.patience:\n            self.early_stop = True\n    else:\n        self.best_score = score\n        self.save_checkpoint(val_loss, model)\n        self.counter = 0\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.EarlyStopping.__init__","title":"<code>__init__(patience=7, verbose=False, delta=0.0, path='checkpoint.pt')</code>","text":"<p>Initialize EarlyStopping.</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def __init__(self, patience=7, verbose=False, delta=0.0, path='checkpoint.pt'):\n    \"\"\"Initialize EarlyStopping.\"\"\"\n    self.patience = patience\n    self.verbose = verbose\n    self.counter = 0\n    self.best_score = None\n    self.early_stop = False\n    self.val_loss_min = np.Inf\n    self.delta = delta\n    self.path = path\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.EarlyStopping.save_checkpoint","title":"<code>save_checkpoint(val_loss, model=None)</code>","text":"<p>Saves model when validation loss decreases.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.EarlyStopping.save_checkpoint--parameters","title":"Parameters","text":"<p>val_loss : float     Validation loss model : torch.nn.Module, optional     Model to save, by default None</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def save_checkpoint(self, val_loss, model=None):\n    \"\"\"\n    Saves model when validation loss decreases.\n\n    Parameters\n    ----------\n    val_loss : float\n        Validation loss\n    model : torch.nn.Module, optional\n        Model to save, by default None\n    \"\"\"\n    if self.verbose:\n        print(f'Validation loss decreased ({self.val_loss_min:.6f} --&gt; {val_loss:.6f}). Saving model...')\n    if model is not None:\n        torch.save(model.state_dict(), self.path)\n    self.val_loss_min = val_loss\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.LRWarmup","title":"<code>LRWarmup</code>","text":"<p>Learning rate warmup.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.LRWarmup--parameters","title":"Parameters","text":"<p>optimizer : torch.optim.Optimizer     Optimizer warmup_epochs : int, optional     Number of epochs for warmup, by default 5 target_lr : float, optional     Target learning rate after warmup, by default 0.01</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>class LRWarmup:\n    \"\"\"\n    Learning rate warmup.\n\n    Parameters\n    ----------\n    optimizer : torch.optim.Optimizer\n        Optimizer\n    warmup_epochs : int, optional\n        Number of epochs for warmup, by default 5\n    target_lr : float, optional\n        Target learning rate after warmup, by default 0.01\n    \"\"\"\n\n    def __init__(self, optimizer, warmup_epochs=5, target_lr=0.01):\n        \"\"\"Initialize LRWarmup.\"\"\"\n        self.optimizer = optimizer\n        self.warmup_epochs = warmup_epochs\n        self.target_lr = target_lr\n        self.initial_lrs = [group['lr'] for group in optimizer.param_groups]\n\n    def step(self, epoch):\n        \"\"\"\n        Adjust learning rate.\n\n        Parameters\n        ----------\n        epoch : int\n            Current epoch\n        \"\"\"\n        if epoch &gt;= self.warmup_epochs:\n            # Warmup complete, set target lr\n            for i, param_group in enumerate(self.optimizer.param_groups):\n                param_group['lr'] = self.target_lr\n        else:\n            # During warmup, linearly increase lr\n            warmup_percent = epoch / self.warmup_epochs\n            for i, param_group in enumerate(self.optimizer.param_groups):\n                param_group['lr'] = self.initial_lrs[i] + (self.target_lr - self.initial_lrs[i]) * warmup_percent\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.LRWarmup.__init__","title":"<code>__init__(optimizer, warmup_epochs=5, target_lr=0.01)</code>","text":"<p>Initialize LRWarmup.</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def __init__(self, optimizer, warmup_epochs=5, target_lr=0.01):\n    \"\"\"Initialize LRWarmup.\"\"\"\n    self.optimizer = optimizer\n    self.warmup_epochs = warmup_epochs\n    self.target_lr = target_lr\n    self.initial_lrs = [group['lr'] for group in optimizer.param_groups]\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.LRWarmup.step","title":"<code>step(epoch)</code>","text":"<p>Adjust learning rate.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.LRWarmup.step--parameters","title":"Parameters","text":"<p>epoch : int     Current epoch</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def step(self, epoch):\n    \"\"\"\n    Adjust learning rate.\n\n    Parameters\n    ----------\n    epoch : int\n        Current epoch\n    \"\"\"\n    if epoch &gt;= self.warmup_epochs:\n        # Warmup complete, set target lr\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            param_group['lr'] = self.target_lr\n    else:\n        # During warmup, linearly increase lr\n        warmup_percent = epoch / self.warmup_epochs\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            param_group['lr'] = self.initial_lrs[i] + (self.target_lr - self.initial_lrs[i]) * warmup_percent\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.count_parameters","title":"<code>count_parameters(model)</code>","text":"<p>Counts the number of trainable parameters in a model.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.count_parameters--parameters","title":"Parameters","text":"<p>model : torch.nn.Module     Model to count parameters</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.count_parameters--returns","title":"Returns","text":"<p>int     Number of trainable parameters</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def count_parameters(model):\n    \"\"\"\n    Counts the number of trainable parameters in a model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to count parameters\n\n    Returns\n    -------\n    int\n        Number of trainable parameters\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.get_data_loaders","title":"<code>get_data_loaders(dataset, batch_size=32, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_seed=42)</code>","text":"<p>Splits a dataset into train, validation, and test sets and creates DataLoader objects.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.get_data_loaders--parameters","title":"Parameters","text":"<p>dataset : torch_geometric.data.Dataset     Dataset to split batch_size : int, optional     Batch size, by default 32 train_ratio : float, optional     Training set ratio, by default 0.7 val_ratio : float, optional     Validation set ratio, by default 0.15 test_ratio : float, optional     Test set ratio, by default 0.15 random_seed : int, optional     Random seed, by default 42</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.get_data_loaders--returns","title":"Returns","text":"<p>tuple     (train_loader, val_loader, test_loader)</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def get_data_loaders(dataset, batch_size=32, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_seed=42):\n    \"\"\"\n    Splits a dataset into train, validation, and test sets and creates DataLoader objects.\n\n    Parameters\n    ----------\n    dataset : torch_geometric.data.Dataset\n        Dataset to split\n    batch_size : int, optional\n        Batch size, by default 32\n    train_ratio : float, optional\n        Training set ratio, by default 0.7\n    val_ratio : float, optional\n        Validation set ratio, by default 0.15\n    test_ratio : float, optional\n        Test set ratio, by default 0.15\n    random_seed : int, optional\n        Random seed, by default 42\n\n    Returns\n    -------\n    tuple\n        (train_loader, val_loader, test_loader)\n    \"\"\"\n    from torch_geometric.loader import DataLoader\n\n    # Set random seed\n    np.random.seed(random_seed)\n\n    # Get number of samples\n    num_samples = len(dataset)\n\n    # Create indices\n    indices = list(range(num_samples))\n    np.random.shuffle(indices)\n\n    # Calculate split sizes\n    train_size = int(train_ratio * num_samples)\n    val_size = int(val_ratio * num_samples)\n\n    # Split indices\n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:train_size + val_size]\n    test_indices = indices[train_size + val_size:]\n\n    # Create subset datasets\n    train_dataset = [dataset[i] for i in train_indices]\n    val_dataset = [dataset[i] for i in val_indices]\n    test_dataset = [dataset[i] for i in test_indices]\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.learning_rate_scheduler","title":"<code>learning_rate_scheduler(optimizer, epoch, initial_lr=0.01, lr_decay_factor=0.1, lr_decay_epochs=[30, 60, 90])</code>","text":"<p>Adjusts the learning rate during training.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.learning_rate_scheduler--parameters","title":"Parameters","text":"<p>optimizer : torch.optim.Optimizer     Optimizer epoch : int     Current epoch initial_lr : float, optional     Initial learning rate, by default 0.01 lr_decay_factor : float, optional     Factor by which to decay the learning rate, by default 0.1 lr_decay_epochs : list, optional     Epochs at which to decay the learning rate, by default [30, 60, 90]</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.learning_rate_scheduler--returns","title":"Returns","text":"<p>float     New learning rate</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def learning_rate_scheduler(optimizer, epoch, initial_lr=0.01, lr_decay_factor=0.1, lr_decay_epochs=[30, 60, 90]):\n    \"\"\"\n    Adjusts the learning rate during training.\n\n    Parameters\n    ----------\n    optimizer : torch.optim.Optimizer\n        Optimizer\n    epoch : int\n        Current epoch\n    initial_lr : float, optional\n        Initial learning rate, by default 0.01\n    lr_decay_factor : float, optional\n        Factor by which to decay the learning rate, by default 0.1\n    lr_decay_epochs : list, optional\n        Epochs at which to decay the learning rate, by default [30, 60, 90]\n\n    Returns\n    -------\n    float\n        New learning rate\n    \"\"\"\n    # Calculate lr_decay_factor^(number of times epoch is in lr_decay_epochs)\n    decay = lr_decay_factor ** sum([epoch &gt;= decay_epoch for decay_epoch in lr_decay_epochs])\n\n    # Set learning rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = initial_lr * decay\n\n    return initial_lr * decay\n</code></pre>"},{"location":"imgraph/training/utils/#imgraph.training.utils.set_seed","title":"<code>set_seed(seed)</code>","text":"<p>Sets random seed for reproducibility.</p>"},{"location":"imgraph/training/utils/#imgraph.training.utils.set_seed--parameters","title":"Parameters","text":"<p>seed : int     Random seed</p> Source code in <code>imgraph/training/utils.py</code> <pre><code>def set_seed(seed):\n    \"\"\"\n    Sets random seed for reproducibility.\n\n    Parameters\n    ----------\n    seed : int\n        Random seed\n    \"\"\"\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"imgraph/utils/feature_size/","title":"Feature Size","text":""},{"location":"imgraph/utils/feature_size/#imgraphutilsfeature_size","title":"<code>imgraph.utils.feature_size</code>","text":""},{"location":"imgraph/visualization/graph_plots/","title":"Graph Plots","text":""},{"location":"imgraph/visualization/graph_plots/#imgraphvisualizationgraph_plots","title":"<code>imgraph.visualization.graph_plots</code>","text":"<p>Plot functions for visualizing image graphs.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.compare_graphs","title":"<code>compare_graphs(image, graphs, names, figsize=(15, 10))</code>","text":"<p>Compares multiple graphs side by side.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.compare_graphs--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image graphs : list     List of graph data objects names : list     List of graph names figsize : tuple, optional     Figure size, by default (15, 10)</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.compare_graphs--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def compare_graphs(image, graphs, names, figsize=(15, 10)):\n    \"\"\"\n    Compares multiple graphs side by side.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    graphs : list\n        List of graph data objects\n    names : list\n        List of graph names\n    figsize : tuple, optional\n        Figure size, by default (15, 10)\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    n_graphs = len(graphs)\n\n    # Create figure with subplots\n    fig, axs = plt.subplots(1, n_graphs + 1, figsize=figsize)\n\n    # Display original image\n    axs[0].imshow(image)\n    axs[0].set_title(\"Original Image\")\n    axs[0].axis('off')\n\n    # Display each graph\n    for i, (graph, name) in enumerate(zip(graphs, names)):\n        visualize_graph(image, graph, ax=axs[i+1])\n        axs[i+1].set_title(f\"{name}\\n{graph.num_nodes} nodes, {graph.edge_index.shape[1]} edges\")\n\n    plt.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_edge_feature_distribution","title":"<code>plot_edge_feature_distribution(graph, feature_indices=None, num_features=3, figsize=(12, 8))</code>","text":"<p>Plots the distribution of edge features.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_edge_feature_distribution--parameters","title":"Parameters","text":"<p>graph : torch_geometric.data.Data     Graph data feature_indices : list, optional     Indices of features to plot, by default None (auto-select) num_features : int, optional     Number of features to plot if feature_indices is None, by default 3 figsize : tuple, optional     Figure size, by default (12, 8)</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_edge_feature_distribution--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def plot_edge_feature_distribution(graph, feature_indices=None, num_features=3, figsize=(12, 8)):\n    \"\"\"\n    Plots the distribution of edge features.\n\n    Parameters\n    ----------\n    graph : torch_geometric.data.Data\n        Graph data\n    feature_indices : list, optional\n        Indices of features to plot, by default None (auto-select)\n    num_features : int, optional\n        Number of features to plot if feature_indices is None, by default 3\n    figsize : tuple, optional\n        Figure size, by default (12, 8)\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    if not hasattr(graph, 'edge_attr') or graph.edge_attr is None or graph.edge_attr.shape[1] == 0:\n        raise ValueError(\"Graph does not contain edge features\")\n\n    # Select features to plot\n    if feature_indices is None:\n        feature_dim = graph.edge_attr.shape[1]\n        feature_indices = list(range(min(num_features, feature_dim)))\n\n    # Create figure\n    fig, axs = plt.subplots(len(feature_indices), 1, figsize=figsize, sharex=True)\n\n    # Handle single feature case\n    if len(feature_indices) == 1:\n        axs = [axs]\n\n    # Plot each feature distribution\n    for i, feat_idx in enumerate(feature_indices):\n        feature_values = graph.edge_attr[:, feat_idx].cpu().numpy()\n        axs[i].hist(feature_values, bins=30, alpha=0.7)\n        axs[i].set_title(f\"Edge Feature {feat_idx} Distribution\")\n        axs[i].set_ylabel(\"Count\")\n\n    axs[-1].set_xlabel(\"Feature Value\")\n    plt.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_graph_spectral_clustering","title":"<code>plot_graph_spectral_clustering(image, graph, n_clusters=5, cmap='tab10', ax=None)</code>","text":"<p>Visualizes spectral clustering of the graph.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_graph_spectral_clustering--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image graph : torch_geometric.data.Data     Graph data n_clusters : int, optional     Number of clusters, by default 5 cmap : str, optional     Colormap, by default 'tab10' ax : matplotlib.axes.Axes, optional     Axes to plot on, by default None</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_graph_spectral_clustering--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def plot_graph_spectral_clustering(image, graph, n_clusters=5, cmap='tab10', ax=None):\n    \"\"\"\n    Visualizes spectral clustering of the graph.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    graph : torch_geometric.data.Data\n        Graph data\n    n_clusters : int, optional\n        Number of clusters, by default 5\n    cmap : str, optional\n        Colormap, by default 'tab10'\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on, by default None\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Create figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(12, 10))\n    else:\n        fig = ax.figure\n\n    # Display image\n    ax.imshow(image)\n\n    # Get node positions\n    if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n        node_positions = graph.node_info['centroids']\n    else:\n        raise ValueError(\"Graph does not contain node positions\")\n\n    # Get edge indices and convert to adjacency matrix\n    edge_index = graph.edge_index.cpu().numpy()\n    num_nodes = graph.num_nodes\n\n    # Create sparse adjacency matrix\n    rows = edge_index[0]\n    cols = edge_index[1]\n    data = np.ones(len(rows))\n    adj_matrix = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n\n    # Spectral clustering\n    sc = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', \n                            assign_labels='discretize', random_state=42)\n    cluster_labels = sc.fit_predict(adj_matrix)\n\n    # Plot nodes with cluster colors\n    cmap_obj = cm.get_cmap(cmap, n_clusters)\n    sc = ax.scatter(node_positions[:, 1], node_positions[:, 0], \n                     c=cluster_labels, cmap=cmap_obj, s=30, alpha=0.8)\n\n    # Add colorbar\n    cbar = plt.colorbar(sc, ax=ax, ticks=range(n_clusters), shrink=0.8)\n    cbar.set_label('Cluster')\n\n    # Plot edges\n    for i in range(edge_index.shape[1]):\n        src_idx = edge_index[0, i]\n        dst_idx = edge_index[1, i]\n\n        src_pos = node_positions[src_idx]\n        dst_pos = node_positions[dst_idx]\n\n        # Only draw edges between nodes in the same cluster\n        if cluster_labels[src_idx] == cluster_labels[dst_idx]:\n            ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], \n                     color=cmap_obj(cluster_labels[src_idx]), alpha=0.3, linewidth=0.7)\n        else:\n            ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], \n                     'gray', alpha=0.1, linewidth=0.3)\n\n    # Set title and turn off axis\n    ax.set_title(f\"Spectral Clustering (k={n_clusters})\")\n    ax.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_node_feature_distribution","title":"<code>plot_node_feature_distribution(graph, feature_indices=None, num_features=5, figsize=(12, 8))</code>","text":"<p>Plots the distribution of node features.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_node_feature_distribution--parameters","title":"Parameters","text":"<p>graph : torch_geometric.data.Data     Graph data feature_indices : list, optional     Indices of features to plot, by default None (auto-select) num_features : int, optional     Number of features to plot if feature_indices is None, by default 5 figsize : tuple, optional     Figure size, by default (12, 8)</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.plot_node_feature_distribution--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def plot_node_feature_distribution(graph, feature_indices=None, num_features=5, figsize=(12, 8)):\n    \"\"\"\n    Plots the distribution of node features.\n\n    Parameters\n    ----------\n    graph : torch_geometric.data.Data\n        Graph data\n    feature_indices : list, optional\n        Indices of features to plot, by default None (auto-select)\n    num_features : int, optional\n        Number of features to plot if feature_indices is None, by default 5\n    figsize : tuple, optional\n        Figure size, by default (12, 8)\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    if graph.x is None or graph.x.shape[1] == 0:\n        raise ValueError(\"Graph does not contain node features\")\n\n    # Select features to plot\n    if feature_indices is None:\n        feature_dim = graph.x.shape[1]\n        feature_indices = list(range(min(num_features, feature_dim)))\n\n    # Create figure\n    fig, axs = plt.subplots(len(feature_indices), 1, figsize=figsize, sharex=True)\n\n    # Handle single feature case\n    if len(feature_indices) == 1:\n        axs = [axs]\n\n    # Plot each feature distribution\n    for i, feat_idx in enumerate(feature_indices):\n        feature_values = graph.x[:, feat_idx].cpu().numpy()\n        axs[i].hist(feature_values, bins=30, alpha=0.7)\n        axs[i].set_title(f\"Node Feature {feat_idx} Distribution\")\n        axs[i].set_ylabel(\"Count\")\n\n    axs[-1].set_xlabel(\"Feature Value\")\n    plt.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_adjacency_matrix","title":"<code>visualize_adjacency_matrix(graph, cmap='Blues', figsize=(10, 8))</code>","text":"<p>Visualizes the adjacency matrix of the graph.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_adjacency_matrix--parameters","title":"Parameters","text":"<p>graph : torch_geometric.data.Data     Graph data cmap : str, optional     Colormap, by default 'Blues' figsize : tuple, optional     Figure size, by default (10, 8)</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_adjacency_matrix--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def visualize_adjacency_matrix(graph, cmap='Blues', figsize=(10, 8)):\n    \"\"\"\n    Visualizes the adjacency matrix of the graph.\n\n    Parameters\n    ----------\n    graph : torch_geometric.data.Data\n        Graph data\n    cmap : str, optional\n        Colormap, by default 'Blues'\n    figsize : tuple, optional\n        Figure size, by default (10, 8)\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Create figure\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Get edge indices\n    edge_index = graph.edge_index.cpu().numpy()\n\n    # Create sparse adjacency matrix\n    num_nodes = graph.num_nodes\n    adj_matrix = np.zeros((num_nodes, num_nodes))\n\n    for i in range(edge_index.shape[1]):\n        src, dst = edge_index[0, i], edge_index[1, i]\n        adj_matrix[src, dst] = 1\n\n    # Visualize adjacency matrix\n    im = ax.imshow(adj_matrix, cmap=cmap)\n\n    # Add colorbar\n    plt.colorbar(im, ax=ax)\n\n    # Set title and labels\n    ax.set_title(\"Adjacency Matrix\")\n    ax.set_xlabel(\"Node Index\")\n    ax.set_ylabel(\"Node Index\")\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_graph","title":"<code>visualize_graph(image, graph, ax=None, node_size=20, edge_width=0.5, node_color='red', edge_color='blue', alpha=0.6)</code>","text":"<p>Visualizes a graph overlaid on an image.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_graph--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image graph : torch_geometric.data.Data     Graph data ax : matplotlib.axes.Axes, optional     Axes to plot on, by default None node_size : int, optional     Size of nodes, by default 20 edge_width : float, optional     Width of edges, by default 0.5 node_color : str, optional     Color of nodes, by default 'red' edge_color : str, optional     Color of edges, by default 'blue' alpha : float, optional     Transparency, by default 0.6</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_graph--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def visualize_graph(image, graph, ax=None, node_size=20, edge_width=0.5, node_color='red', edge_color='blue', alpha=0.6):\n    \"\"\"\n    Visualizes a graph overlaid on an image.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    graph : torch_geometric.data.Data\n        Graph data\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on, by default None\n    node_size : int, optional\n        Size of nodes, by default 20\n    edge_width : float, optional\n        Width of edges, by default 0.5\n    node_color : str, optional\n        Color of nodes, by default 'red'\n    edge_color : str, optional\n        Color of edges, by default 'blue'\n    alpha : float, optional\n        Transparency, by default 0.6\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Create figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 8))\n    else:\n        fig = ax.figure\n\n    # Display image\n    ax.imshow(image)\n\n    # Get node positions\n    if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n        node_positions = graph.node_info['centroids']\n    else:\n        raise ValueError(\"Graph does not contain node positions\")\n\n    # Get edge indices\n    edge_index = graph.edge_index.cpu().numpy()\n\n    # Plot nodes\n    ax.scatter(node_positions[:, 1], node_positions[:, 0], c=node_color, s=node_size, alpha=alpha)\n\n    # Plot edges\n    for i in range(edge_index.shape[1]):\n        src_idx = edge_index[0, i]\n        dst_idx = edge_index[1, i]\n\n        src_pos = node_positions[src_idx]\n        dst_pos = node_positions[dst_idx]\n\n        ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], color=edge_color, alpha=alpha*0.5, linewidth=edge_width)\n\n    # Set title and turn off axis\n    ax.set_title(f\"Graph: {graph.num_nodes} nodes, {edge_index.shape[1]} edges\")\n    ax.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_graph_with_features","title":"<code>visualize_graph_with_features(image, graph, node_feature_idx=None, edge_feature_idx=None, cmap='viridis', ax=None)</code>","text":"<p>Visualizes a graph with node or edge features as colors.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_graph_with_features--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image graph : torch_geometric.data.Data     Graph data node_feature_idx : int, optional     Index of node feature to visualize, by default None edge_feature_idx : int, optional     Index of edge feature to visualize, by default None cmap : str, optional     Colormap, by default 'viridis' ax : matplotlib.axes.Axes, optional     Axes to plot on, by default None</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_graph_with_features--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def visualize_graph_with_features(image, graph, node_feature_idx=None, edge_feature_idx=None, cmap='viridis', ax=None):\n    \"\"\"\n    Visualizes a graph with node or edge features as colors.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    graph : torch_geometric.data.Data\n        Graph data\n    node_feature_idx : int, optional\n        Index of node feature to visualize, by default None\n    edge_feature_idx : int, optional\n        Index of edge feature to visualize, by default None\n    cmap : str, optional\n        Colormap, by default 'viridis'\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on, by default None\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Create figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(12, 10))\n    else:\n        fig = ax.figure\n\n    # Display image\n    ax.imshow(image)\n\n    # Get node positions\n    if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n        node_positions = graph.node_info['centroids']\n    else:\n        raise ValueError(\"Graph does not contain node positions\")\n\n    # Get edge indices\n    edge_index = graph.edge_index.cpu().numpy()\n\n    # Visualize node features\n    if node_feature_idx is not None and graph.x is not None:\n        # Extract feature values\n        node_features = graph.x[:, node_feature_idx].cpu().numpy()\n\n        # Normalize feature values\n        norm = Normalize(vmin=node_features.min(), vmax=node_features.max())\n\n        # Create a colormap\n        cmap_obj = cm.get_cmap(cmap)\n\n        # Plot nodes with feature colors\n        sc = ax.scatter(node_positions[:, 1], node_positions[:, 0], \n                         c=node_features, cmap=cmap_obj, norm=norm, s=30)\n\n        # Add colorbar\n        cbar = plt.colorbar(sc, ax=ax, shrink=0.8)\n        cbar.set_label(f'Node Feature {node_feature_idx}')\n\n        # Plot edges\n        for i in range(edge_index.shape[1]):\n            src_idx = edge_index[0, i]\n            dst_idx = edge_index[1, i]\n\n            src_pos = node_positions[src_idx]\n            dst_pos = node_positions[dst_idx]\n\n            ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.2, linewidth=0.5)\n\n    # Visualize edge features\n    elif edge_feature_idx is not None and hasattr(graph, 'edge_attr') and graph.edge_attr is not None:\n        # Extract feature values\n        edge_features = graph.edge_attr[:, edge_feature_idx].cpu().numpy()\n\n        # Normalize feature values\n        norm = Normalize(vmin=edge_features.min(), vmax=edge_features.max())\n\n        # Create a colormap\n        cmap_obj = cm.get_cmap(cmap)\n\n        # Plot nodes\n        ax.scatter(node_positions[:, 1], node_positions[:, 0], c='gray', s=20, alpha=0.5)\n\n        # Plot edges with feature colors\n        lines = []\n        colors = []\n\n        for i in range(edge_index.shape[1]):\n            src_idx = edge_index[0, i]\n            dst_idx = edge_index[1, i]\n\n            src_pos = node_positions[src_idx]\n            dst_pos = node_positions[dst_idx]\n\n            # Plot edge with color\n            line = ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], \n                            color=cmap_obj(norm(edge_features[i])), linewidth=2, alpha=0.7)[0]\n\n            lines.append(line)\n            colors.append(edge_features[i])\n\n        # Add colorbar\n        sm = cm.ScalarMappable(cmap=cmap_obj, norm=norm)\n        sm.set_array([])\n        cbar = plt.colorbar(sm, ax=ax, shrink=0.8)\n        cbar.set_label(f'Edge Feature {edge_feature_idx}')\n\n    else:\n        # Default visualization\n        ax.scatter(node_positions[:, 1], node_positions[:, 0], c='red', s=20, alpha=0.6)\n\n        # Plot edges\n        for i in range(edge_index.shape[1]):\n            src_idx = edge_index[0, i]\n            dst_idx = edge_index[1, i]\n\n            src_pos = node_positions[src_idx]\n            dst_pos = node_positions[dst_idx]\n\n            ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'b-', alpha=0.3, linewidth=0.5)\n\n    # Set title and turn off axis\n    if node_feature_idx is not None:\n        ax.set_title(f\"Graph with Node Feature {node_feature_idx}\")\n    elif edge_feature_idx is not None:\n        ax.set_title(f\"Graph with Edge Feature {edge_feature_idx}\")\n    else:\n        ax.set_title(f\"Graph: {graph.num_nodes} nodes, {edge_index.shape[1]} edges\")\n\n    ax.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_node_embeddings","title":"<code>visualize_node_embeddings(graph, image=None, method='tsne', figsize=(12, 10))</code>","text":"<p>Visualizes node embeddings in 2D space.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_node_embeddings--parameters","title":"Parameters","text":"<p>graph : torch_geometric.data.Data     Graph data image : numpy.ndarray, optional     Input image for reference, by default None method : str, optional     Dimensionality reduction method, by default 'tsne'     Options: 'tsne', 'pca' figsize : tuple, optional     Figure size, by default (12, 10)</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_node_embeddings--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def visualize_node_embeddings(graph, image=None, method='tsne', figsize=(12, 10)):\n    \"\"\"\n    Visualizes node embeddings in 2D space.\n\n    Parameters\n    ----------\n    graph : torch_geometric.data.Data\n        Graph data\n    image : numpy.ndarray, optional\n        Input image for reference, by default None\n    method : str, optional\n        Dimensionality reduction method, by default 'tsne'\n        Options: 'tsne', 'pca'\n    figsize : tuple, optional\n        Figure size, by default (12, 10)\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    if graph.x is None or graph.x.shape[1] == 0:\n        raise ValueError(\"Graph does not contain node features\")\n\n    # Create figure\n    fig = plt.figure(figsize=figsize)\n\n    # If image is provided, create a side-by-side layout\n    if image is not None:\n        gs = fig.add_gridspec(1, 2, width_ratios=[1, 1])\n        ax1 = fig.add_subplot(gs[0])\n        ax2 = fig.add_subplot(gs[1])\n\n        # Display image with graph\n        if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n            visualize_graph(image, graph, ax=ax1)\n        else:\n            ax1.imshow(image)\n            ax1.set_title(\"Original Image\")\n            ax1.axis('off')\n    else:\n        ax2 = fig.add_subplot(111)\n\n    # Get node features\n    node_features = graph.x.cpu().numpy()\n\n    # Reduce dimensionality to 2D\n    if method.lower() == 'tsne':\n        embeddings = TSNE(n_components=2, random_state=42).fit_transform(node_features)\n    elif method.lower() == 'pca':\n        embeddings = PCA(n_components=2, random_state=42).fit_transform(node_features)\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    # Get edge indices\n    edge_index = graph.edge_index.cpu().numpy()\n\n    # Plot embeddings\n    ax2.scatter(embeddings[:, 0], embeddings[:, 1], c='blue', s=30, alpha=0.7)\n\n    # Plot edges in embedding space\n    for i in range(edge_index.shape[1]):\n        src_idx = edge_index[0, i]\n        dst_idx = edge_index[1, i]\n\n        src_pos = embeddings[src_idx]\n        dst_pos = embeddings[dst_idx]\n\n        ax2.plot([src_pos[0], dst_pos[0]], [src_pos[1], dst_pos[1]], 'gray', alpha=0.2, linewidth=0.5)\n\n    # Set title and labels\n    ax2.set_title(f\"Node Embeddings ({method.upper()})\")\n    ax2.set_xlabel(\"Dimension 1\")\n    ax2.set_ylabel(\"Dimension 2\")\n\n    plt.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_node_importance","title":"<code>visualize_node_importance(image, graph, importance, cmap='plasma', ax=None, alpha=0.7)</code>","text":"<p>Visualizes node importance on the graph.</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_node_importance--parameters","title":"Parameters","text":"<p>image : numpy.ndarray     Input image graph : torch_geometric.data.Data     Graph data importance : torch.Tensor or numpy.ndarray     Node importance scores cmap : str, optional     Colormap, by default 'plasma' ax : matplotlib.axes.Axes, optional     Axes to plot on, by default None alpha : float, optional     Transparency, by default 0.7</p>"},{"location":"imgraph/visualization/graph_plots/#imgraph.visualization.graph_plots.visualize_node_importance--returns","title":"Returns","text":"<p>matplotlib.figure.Figure     Figure with the visualization</p> Source code in <code>imgraph/visualization/graph_plots.py</code> <pre><code>def visualize_node_importance(image, graph, importance, cmap='plasma', ax=None, alpha=0.7):\n    \"\"\"\n    Visualizes node importance on the graph.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Input image\n    graph : torch_geometric.data.Data\n        Graph data\n    importance : torch.Tensor or numpy.ndarray\n        Node importance scores\n    cmap : str, optional\n        Colormap, by default 'plasma'\n    ax : matplotlib.axes.Axes, optional\n        Axes to plot on, by default None\n    alpha : float, optional\n        Transparency, by default 0.7\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure with the visualization\n    \"\"\"\n    # Create figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(12, 10))\n    else:\n        fig = ax.figure\n\n    # Display image\n    ax.imshow(image)\n\n    # Get node positions\n    if hasattr(graph, 'node_info') and 'centroids' in graph.node_info:\n        node_positions = graph.node_info['centroids']\n    else:\n        raise ValueError(\"Graph does not contain node positions\")\n\n    # Convert importance to numpy if it's a tensor\n    if isinstance(importance, torch.Tensor):\n        importance = importance.cpu().numpy()\n\n    # Normalize importance scores\n    norm = Normalize(vmin=importance.min(), vmax=importance.max())\n\n    # Create a colormap\n    cmap_obj = cm.get_cmap(cmap)\n\n    # Plot nodes with importance scores as colors and sizes\n    sizes = 20 + 100 * norm(importance)  # Scale sizes\n    sc = ax.scatter(node_positions[:, 1], node_positions[:, 0], \n                     c=importance, cmap=cmap_obj, norm=norm, s=sizes, alpha=alpha)\n\n    # Add colorbar\n    cbar = plt.colorbar(sc, ax=ax, shrink=0.8)\n    cbar.set_label('Node Importance')\n\n    # Get edge indices\n    edge_index = graph.edge_index.cpu().numpy()\n\n    # Plot edges\n    for i in range(edge_index.shape[1]):\n        src_idx = edge_index[0, i]\n        dst_idx = edge_index[1, i]\n\n        src_pos = node_positions[src_idx]\n        dst_pos = node_positions[dst_idx]\n\n        ax.plot([src_pos[1], dst_pos[1]], [src_pos[0], dst_pos[0]], 'gray', alpha=0.2, linewidth=0.5)\n\n    # Set title and turn off axis\n    ax.set_title(\"Node Importance Visualization\")\n    ax.axis('off')\n\n    return fig\n</code></pre>"},{"location":"imgraph/writer/makedirs/","title":"Make Dirs","text":""},{"location":"imgraph/writer/makedirs/#imgraphwritermakedirs","title":"<code>imgraph.writer.makedirs</code>","text":""},{"location":"imgraph/writer/makedirs/#imgraph.writer.makedirs.makedirs","title":"<code>makedirs(path)</code>","text":"<p>Recursively creates a directory. Args:     path (str): The path to create.</p> Source code in <code>imgraph/writer/makedirs.py</code> <pre><code>def makedirs(path: str):\n    r\"\"\"Recursively creates a directory.\n    Args:\n        path (str): The path to create.\n    \"\"\"\n    try:\n        os.makedirs(osp.expanduser(osp.normpath(path)))\n    except OSError as e:\n        if e.errno != errno.EEXIST and osp.isdir(path):\n            raise e\n</code></pre>"},{"location":"imgraph/writer/write_files/","title":"Write Files","text":""},{"location":"imgraph/writer/write_files/#imgraphwriterwrite_files","title":"<code>imgraph.writer.write_files</code>","text":""},{"location":"imgraph/writer/write_files/#imgraph.writer.write_files.download_from_url","title":"<code>download_from_url(url, path, filename=None)</code>","text":"<p>Downloads a file from a URL. Args:     url (str): The URL to download from.     path (str): The path to save the file to.</p> Source code in <code>imgraph/writer/write_files.py</code> <pre><code>def download_from_url(url : str, path : str, filename : Optional[str] = None):\n    \"\"\"Downloads a file from a URL.\n    Args:\n        url (str): The URL to download from.\n        path (str): The path to save the file to.\n    \"\"\"\n    if filename is None:\n        filename = url.rpartition('/')[2]\n        filename = filename if filename[0] == '?' else filename.split('?')[0]\n\n    filepath = osp.join(path, filename)\n\n    if osp.exists(filepath):  \n        return filepath\n\n    makedirs(path)\n\n    context = ssl._create_unverified_context()\n    data = urllib.request.urlopen(url, context=context)\n\n    with open(filepath, 'wb') as f:\n        # workaround for https://bugs.python.org/issue42853\n        while True:\n            chunk = data.read(10 * 1024 * 1024)\n            if not chunk:\n                break\n            f.write(chunk)\n\n    return filepath\n</code></pre>"},{"location":"imgraph/writer/write_files/#imgraph.writer.write_files.write_dataloader","title":"<code>write_dataloader(dataloader, path)</code>","text":"<p>Writes a dataloader to a file. Args:     dataloader (torch dataloader): The dataloader to write.     path (str): The path to a local dataloader file.</p> Source code in <code>imgraph/writer/write_files.py</code> <pre><code>def write_dataloader(dataloader, path):\n    \"\"\"Writes a dataloader to a file.\n    Args:\n        dataloader (torch dataloader): The dataloader to write.\n        path (str): The path to a local dataloader file.\n    \"\"\"\n    with open(path, 'wb') as f:\n        pickle.dump(dataloader, f)\n</code></pre>"},{"location":"imgraph/writer/write_files/#imgraph.writer.write_files.write_graph","title":"<code>write_graph(graph, path)</code>","text":"<p>Writes a graph to a file. Args:     graph (networkx graph): The graph to write.     path (str): The path to a local graph file.</p> Source code in <code>imgraph/writer/write_files.py</code> <pre><code>def write_graph(graph, path):\n    \"\"\"Writes a graph to a file.\n    Args:\n        graph (networkx graph): The graph to write.\n        path (str): The path to a local graph file.\n    \"\"\"\n    print(\"writing graph to file\", path)\n    nx.write_gpickle(graph, osp.expanduser(path))\n</code></pre>"},{"location":"imgraph/writer/write_files/#imgraph.writer.write_files.write_image","title":"<code>write_image(img, path)</code>","text":"<p>Writes an image to a file. Args:     img (PIL image): The image to write.     path (str): The path to a local image file. Todo: support other image formats.</p> Source code in <code>imgraph/writer/write_files.py</code> <pre><code>def write_image(img, path):\n    \"\"\"Writes an image to a file.\n    Args:\n        img (PIL image): The image to write.\n        path (str): The path to a local image file.\n    Todo: support other image formats.\n    \"\"\"\n    img.save(path)\n</code></pre>"},{"location":"imgraph/writer/write_files/#imgraph.writer.write_files.write_pickle_file","title":"<code>write_pickle_file(obj, path)</code>","text":"<p>Writes a pickle file to a file. Args:     obj (object): The object to write.     path (str): The path to a local pickle file.</p> Source code in <code>imgraph/writer/write_files.py</code> <pre><code>def write_pickle_file(obj, path):\n    \"\"\"Writes a pickle file to a file.\n    Args:\n        obj (object): The object to write.\n        path (str): The path to a local pickle file.\n    \"\"\"\n    with open(path, 'wb') as f:\n        pickle.dump(obj, f)\n</code></pre>"},{"location":"imgraph/writer/write_files/#imgraph.writer.write_files.write_pyg_data","title":"<code>write_pyg_data(data, path)</code>","text":"<p>Writes a PyG data object to a file. Args:     data (PyG data object): The data object to write.     path (str): The path to a local PyG data file.</p> Source code in <code>imgraph/writer/write_files.py</code> <pre><code>def write_pyg_data(data, path):\n    \"\"\"Writes a PyG data object to a file.\n    Args:\n        data (PyG data object): The data object to write.\n        path (str): The path to a local PyG data file.\n    \"\"\"\n    torch.save(data, osp.expanduser(path))\n</code></pre>"}]}